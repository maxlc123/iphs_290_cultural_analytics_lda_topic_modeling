# -*- coding: utf-8 -*-
"""Copy of ph200_miniproj2_Twitter-Pulse-Checker_class_20201022.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1005rur32cVg4slGU6Y8FBwB9Dbp7P0Fj

# **Twitter Pulse Checker : Mini-Project #2**

References:
* https://colab.research.google.com/drive/1WIcVZgbrU0DYOQqaxuaCLKY6CoLBV18O
* https://towardsdatascience.com/twitter-pulse-checker-an-interactive-colab-notebook-for-data-sciencing-on-twitter-76a27ec8526f

Instructions:
1. Answer all the questions marked "QUESTION:" below with at least 1-2 complete sentences
2. In code blocks with "CUSTOMIZE:", modify the code (or enter different search terms) to perform your own customized search
3. In code blocks with "CLEAN:", insert code to clean your tweets
4. In code blocks with "EXPLAIN:", give your interpretations on the question presented


## **IPHS 200: Programming Humanity**
### **Prof Chun and Prof Elkins**
### **Fall 2020**

![preview](https://cdn.pixabay.com/photo/2013/06/07/09/53/twitter-117595_960_720.png)

This is a quick and dirty way to get a sense of what's trending on Twitter related to a particular Topic. For my use case, I am focusing on the city of Seattle but you can easily apply this to any topic.

**Use the GPU for this notebook to speed things up:** select the menu option "Runtime" -> "Change runtime type", select "Hardware Accelerator" -> "GPU" and click "SAVE".

The code in this notebook does the following things:


*   Scrapes Tweets related to the Topic you are interested in.
*   Extracts relevant Tags from the text (NER: Named Entity Recognition).
*   Does Sentiment Analysis on those Tweets.
*   Provides some visualizations in an interactive format to get a 'pulse' of what's happening.

We use Tweepy to scrape Twitter data and Flair to do NER / Sentiment Analysis. We use Seaborn for visualizations and all of this is possible because of the wonderful, free and fast (with GPU) Google Colab.

**A bit about NER (Named Entity Recognition)** 

This is the process of extracting labels form text. 

So, take an example sentence: 'George Washington went to Washington'. NER will allow us to extract labels such as Person for 'George Washington' and Location for 'Washington (state)'. It is one of the most common and useful applications in NLP and, using it, we can extract labels from Tweets and do analysis on them.

**A bit about Sentiment Analysis** 

Most commonly, this is the process of getting a sense of whether some text is Positive or Negative. More generally, you can apply it to any label of your choosing (Spam/No Spam etc.).

So, 'I hated this movie' would be classified as a negative statement but 'I loved this movie' would be classified as positive. Again - it is a very useful application as it allows us to get a sense of people's opinions about something (Twitter topics, Movie reviews etc). 

To learn more about these applications, check out the Flair Github homepage and Tutorials: https://github.com/zalandoresearch/flair


Note: You will need Twitter API keys (and of course a Twitter account) to make this work. You can get those by signing up here: https://developer.twitter.com/en/apps

To get up and running, we need to import a bunch of stuff and install Flair. Run through the next 3 cells.
"""

# Commented out IPython magic to ensure Python compatibility.
# Connect your Google Colab Virtual Machine (VM) to your Google gdrive
#   so all your work is saved permanently to your gdrive

from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

!pwd

# Commented out IPython magic to ensure Python compatibility.
# change directory (cd) into your root folder that
# you have write permissions on 

# %cd ./My\ Drive

# Commented out IPython magic to ensure Python compatibility.
# import lots of stuff
import sys
import os
import re
import tweepy
from tweepy import OAuthHandler
from textblob import TextBlob

# QUESTION: What does the Python library TextBlob do? Google it and explain in a few sentences based on what you find on Github or Pypi
#
# Your Answer: It provides a simple API for common natural language processing tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, and translation.

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import string
import time
from IPython.display import clear_output
from tqdm import tqdm

# QUESTION: What does the Python library datetime do? How is it different from the 'time' library imported beneat it?
#
# Your Answer: the time module is principally for working with unix time stamps. The datetime module provides a more object oriented set of types and has limited support for time zones or leap years.

# QUESTION: What does the Python library tqdm do?
#
# Your Answer:  It provides more time-related functions and is a comprehensive package within python used for progress bars. 

import matplotlib.pyplot as plt
import seaborn as sns
# % matplotlib inline

# QUESTION: What are the advantages of seaborn library over matplotlib for visualizations?
#
# Your Answer: Matplotlib provides basic visualization deafults like scatter plots and bar and line graphs, while sesaborn provides more interesting defaults and can summarize data better more automatedly.

from os import path
from PIL import Image
from wordcloud import WordCloud, STOPWORDS

# QUESTION: What does the Python library PIL do?
#
# Your Answer: (Python imaging library) allows for image processing in your python library.

# QUESTION: What does the Python library wordcloud do?
#
# Your Answer: Wordcloud is a data visualization technique that representes text data where the size of a given word correlates to its apparent frequency throughout a text.

# install Flair
!pip install --upgrade git+https://github.com/flairNLP/flair.git

clear_output()

# QUESTION: What does the Python library flair do?
#
# Your Answer: Flair is a NPL library that supports a large amount of language and has the edge over others NPLs because its word embeddings and ability to combine word embeddings.

# import Flair stuff
from flair.data import Sentence
from flair.models import SequenceTagger

# QUESTION: What does the SequenceTagger and Sentence objects do?
#
# Your Answer: A sentence is essentially a list of objects. You can use the SequenceTagger to recognize names in the sentence or other entities.

tagger = SequenceTagger.load('ner')

clear_output()

#import Flair Classifier
from flair.models import TextClassifier

# QUESTION: What does the TextClassifier object do?
#
# Your Answer: TextClassifier allows you to interpret meaning of text and generate predicted actions based on the text.

classifier = TextClassifier.load('en-sentiment')

clear_output()

"""### Authenticate with Twitter API"""

#@title Enter Twitter Credentials
TWITTER_KEY = 'kC16SbbeBdLOOQXTpUY2oGlb6' #@param {type:"string"}
TWITTER_SECRET_KEY = 'WiJeohW9LAaApCkb55zMlhOm0PNk91RcIF2K0DuYeDbZtCOKgL' #@param {type:"string"}

# Authenticate
auth = tweepy.AppAuthHandler(TWITTER_KEY, TWITTER_SECRET_KEY)

api = tweepy.API(auth, wait_on_rate_limit=True,
				   wait_on_rate_limit_notify=True)

# QUESTION: What are the 4 steps to authenticate yourself to Twitter using Tweepy?
# Hint: http://docs.tweepy.org/en/latest/auth_tutorial.html
# 
# Your Answer:

if (not api):
    print ("Can't Authenticate")
    sys.exit(-1)

"""###Lets start scraping!

The Twitter scrape code here was taken from: https://bhaskarvk.github.io/2015/01/how-to-use-twitters-search-rest-api-most-effectively.

My thanks to the author.

We need to provide a Search term and a Max Tweet count. Twitter lets you to request 45,000 tweets every 15 minutes  so setting something below that works.
"""

# CUSTOMIZE: Enter your own search term in place of 'Seattle'
#            If you enter more than one term like 'Biden Kamala' you will only get tweets with BOTH 'Biden' AND 'Kamala'
#            You should re-run this with at least 3 different but related search terms
#            (e.g. "Antifa", "inequality" and "Bernie")
#
#            Try experimenting with maxTweets = 1000 to 10000
#
#            Write your tweets to a *.csv file, download and combine them
#            (see video)

#@title Twitter Search API Inputs
#@markdown ### Enter Search Query:
searchQuery = 'election fraud' #@param {type:"string"}
#@markdown ### Enter Max Tweets To Scrape:
#@markdown #### The Twitter API Rate Limit (currently) is 45,000 tweets every 15 minutes.
maxTweets = 2500 #@param {type:"slider", min:0, max:45000, step:100}
Filter_Retweets = True #@param {type:"boolean"}

tweetsPerQry = 100  # this is the max the API permits
tweet_lst = []

if Filter_Retweets:
  searchQuery = searchQuery + ' -filter:retweets'  # to exclude retweets

# If results from a specific ID onwards are reqd, set since_id to that ID.
# else default to no lower limit, go as far back as API allows
sinceId = None

# If results only below a specific ID are, set max_id to that ID.
# else default to no upper limit, start from the most recent tweet matching the search query.
max_id = -10000000000

tweetCount = 0
print("Downloading max {0} tweets".format(maxTweets))
while tweetCount < maxTweets:
    try:
        if (max_id <= 0):
            if (not sinceId):
                new_tweets = api.search(q=searchQuery, count=tweetsPerQry, lang="en")
            else:
                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,
                                        lang="en", since_id=sinceId)
        else:
            if (not sinceId):
                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,
                                        lang="en", max_id=str(max_id - 1))
            else:
                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,
                                        lang="en", max_id=str(max_id - 1),
                                        since_id=sinceId)
        if not new_tweets:
            print("No more tweets found")
            break
        for tweet in new_tweets:
          if hasattr(tweet, 'reply_count'):
            reply_count = tweet.reply_count
          else:
            reply_count = 0
          if hasattr(tweet, 'retweeted'):
            retweeted = tweet.retweeted
          else:
            retweeted = "NA"
            
          # fixup search query to get topic
          topic = searchQuery[:searchQuery.find('-')].capitalize().strip()
          
          # fixup date
          tweetDate = tweet.created_at.date()
          
          tweet_lst.append([tweetDate, topic, 
                      tweet.id, tweet.user.screen_name, tweet.user.name, tweet.text, tweet.favorite_count, 
                      reply_count, tweet.retweet_count, retweeted])

        tweetCount += len(new_tweets)
        print("Downloaded {0} tweets".format(tweetCount))
        max_id = new_tweets[-1].id
    except tweepy.TweepError as e:
        # Just exit if any error
        print("some error : " + str(e))
        break

clear_output()
print("Downloaded {0} tweets".format(tweetCount))

"""##Data Sciencing

Let's load the tweet data into a Pandas Dataframe so we can do Data Science to it. 

The data is also saved down in a tweets.csv file in case you want to download it.
"""

pd.set_option('display.max_colwidth', -1)

# load it into a pandas dataframe
tweet_df = pd.DataFrame(tweet_lst, columns=['tweet_dt', 'topic', 'id', 'username', 'name', 'tweet', 'like_count', 'reply_count', 'retweet_count', 'retweeted'])

tweet_df.head()

# CLEAN: Insert code here to clean the text of the tweets
#        At minimum, perform 3 distinct cleaning operations (e.g. remove punctuation, lowercase, translate emojis to text, etc.)
#
# Hint: You can find, copy and modify code to clean up tweets from these sour
#       
#       https://github.com/Deffro/text-preprocessing-techniques/blob/master/techniques.py
#       https://github.com/jfogerty18/CleanTweets/blob/master/CleanTweets/preprocessing.py
#       https://www.kaggle.com/ragnisah/text-data-cleaning-tweets-analysis 
#       https://github.com/kevalmorabia97/pyTweetCleaner/blob/master/pyTweetCleaner.py
#       https://github.com/cbaziotis/ekphrasis
#       https://github.com/aritter/twitter_nlp
#       https://gist.github.com/search?q=text+preprocessing
#       just copy and edit code from one of these notebooks or tutorials to clean 
#       the text of your tweets which are stored in the Pandas DataFrame[column]/Series
#       tweet_df['tweet']
#
#       Start with: 
#                    tweet_df['tweet'] = ['tweet']
# 


# First two cleaning operations are done for you, 
# implement 3 more cleaning operations

# Lowercase the tweet
#   create a new column 'tweet_clean' in the tweet_df that we'll 
#   serially operate on to clean up the messy text
def remove_pattern(df):
    """This function removes the user handle, URL, emoticons, and stop words."""
    

# Remove numbers from tweet
#   from this point forward, always read/operate on the 'tweet_clean' column
#   and save the cleaned up text back into this same column


# Remove URLs from tweet
tweet_df['tweet_clean'] = tweet_df['tweet_clean'].str.replace(r)

# Remove #hashtags from tweet
# see https://stackoverflow.com/questions/8376691/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression
# DO THIS: tweet_df['tweet_clean'] = tweet_df['tweet_clean'].str.replace('<insert regex here>', '')

# Remove @userhandles from tweet
# see https://stackoverflow.com/questions/8376691/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression
# DO THIS: tweet_df['tweet_clean'] = tweet_df['tweet_clean'].str.replace('<insert regex here', '')

# Remove punctuation from tweet
# see https://stackoverflow.com/questions/18429143/strip-punctuation-with-regex-python/50985687
#     https://stackoverflow.com/questions/21209024/python-regex-remove-all-punctuation-except-hyphen-for-unicode-string
tweet_df['tweet_clean'] = tweet_df['tweet_clean'].str.replace(r'[^\w\s]', '')

# Remove stopwords from tweet
# see https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe 

# Translate emoji unicode to plain text description
# https://github.com/carpedm20/emoji or demohji

# Expand contractions
# see https://pypi.org/project/pycontractions/ and https://github.com/ian-beaver/pycontractions
# tweet_df['tweet_clean'] = tweet_df['tweet_clean'].apply(lambda s: contractions.fix(str(s)))

# Expand standard English abbreviations
# https://github.com/estnltk/abbreviation-resolver

#   translate common twitter abbr/slang: https://medium.com/coinmonks/remaking-of-shortened-sms-tweet-pos
#     (WARNING: NSFW/profanity and potentionally worse)
#   identify hate speech:
#   identify profanity: 
#   identify potential troll/bot:
#   identify language: https://github.com/Mimino666/langdetect/stargazers
#   translate into english: https://github.com/ssut/py-googletrans/stargazers
#   autospell correct: https://github.com/fsondej/autocorrect
#     (note the accuracy rate, in a real-word production system there would be some intelligence
#      built-in to govern, quantify and decide when to correct or not unknown (OOV) words)
#   build a psychological profile: and translate if necessary into english

# Or use a general NLP library to do these things
# https://github.com/s/preprocessor
# https://github.com/Ankur3107/nlp_preprocessing

#Remove emojis
tweet_df['tweet_clean'] = tweet_df['tweet_clean'].str.replace("[^a-zA-Z#]","") 
#Remove twitter handle
tweet_df['tweet_clean'] = tweet_df['tweet_clean'].str.replace(r'@[\w]*','')
#Remove stop words
tweet_df['tweet_clean'] = tweet_df['tweet_clean'].apply(lambda x: ''.join([w for w in x.split() if len(w)>3]))

tweet_df.head()

# Check that we cleaned the tweets correctly, view the first 50 tweets
#   this should also bring your attention to potential other problems with dirty text
#   as well as new ideas for sub-stories hidden in the data 
#   and the search terms/RegEx's needed to filter the dataset to focus on them

tweet_df['tweet_clean'][:50]

# CHECKPOINT: Save work:  Always do this periodically through your DataSciencing process
#                         so you don't loose valuable work thru crashes, disconnects, errors, etc.

# Save search results to a time stamped file named after the search term and number of tweets

timestr = time.strftime("%Y%m%d-%H%M%S")

# Parse out only the search term from searchQuery (e.g. 'Seattle -filter:retweets')
searchQuery_ls = str(searchQuery).split()
searchTerm = searchQuery_ls[0]
                            
file_name = f"tweets_{maxTweets}_{searchTerm}_on-{timestr}.csv"

tweet_df.to_csv(file_name, encoding='utf-8')

"""Unfortunately Twitter does not let you filter by date when you request tweets. However, we can do this at this stage. I have set it up to pull yesterday + todays Tweets by default."""

# OPTIONAL: Only restrict the date range if you have way too many tweets
#           or you want to talk about events within a narrow date range

#@title Filter By Date Range
today = datetime.now().date()
yesterday = today - timedelta(1)

start_dt = '' #@param {type:"date"}
end_dt = '' #@param {type:"date"}

if start_dt == '':
  start_dt = yesterday
else:
  start_dt = datetime.strptime(start_dt, '%Y-%m-%d').date()

if end_dt == '':
  end_dt = today
else:
  end_dt = datetime.strptime(end_dt, '%Y-%m-%d').date()


tweet_df = tweet_df[(tweet_df['tweet_dt'] >= start_dt) 
                    & (tweet_df['tweet_dt'] <= end_dt)]
tweet_df.shape

# CHECKPOINT: Save work, produces nonsense filename if no date range filter given above

# Save time restricted search results to a time stamped file named after the search term and number of tweets

timestr = time.strftime("%Y%m%d-%H%M%S")

# Uncomment the next 2 line if you filtered out by date range
# file_name = f"tweets_{maxTweets}_{searchTerm}-thru-{start_dt}_on-{end_dt}_{timestr}.csv"
# tweet_df.to_csv(file_name, encoding='utf-8')

"""# **Mini-Project #2 ENDS HERE**

### You're just to complete the top-half of this Jupyter notebook pertaining to text preprocessing/text wrangling

# **Mini-Project #3 BEGINS HERE**

Mini-Project #3 will involve analysis/modeling. This rest of this notebook gives an example of just one of the many ways you can complete MP#3.

## More details will be forthcoming in the weeks ahead of the due-date

## NER and Sentiment Analysis

Now let's do some NER / Sentiment Analysis. We will use the Flair library: https://github.com/zalandoresearch/flair

###NER

Previosuly, we extracted, and then appended the Tags as separate rows in our dataframe. This helps us later on to Group by Tags.

We also create a new 'Hashtag' Tag as Flair does not recognize it and it's a big one in this context.

### Sentiment Analysis

We use the Flair Classifier to get Polarity and Result and add those fields to our dataframe.

**Warning:** This can be slow if you have lots of tweets.
"""

# predict NER

# QUESTION: Look up and explain what 'Named Entity Recognition' or NER 
#           means in Natural Language Processing (NLP)
#
# Answer: NER essentially allows the code to identify something like 'New York' as 'Location'.

nerlst = []

for index, row in tqdm(tweet_df.iterrows(), total=tweet_df.shape[0]):
  cleanedTweet = row['tweet'].replace("#", "")
  sentence = Sentence(cleanedTweet, use_tokenizer=True)
  
  # predict NER tags
  tagger.predict(sentence)

  # get ner
  ners = sentence.to_dict(tag_type='ner')['entities']
  
  # predict sentiment
  classifier.predict(sentence)
  
  label = sentence.labels[0]
  response = {'result': label.value, 'polarity':label.score}
  
  # get hashtags
  hashtags = re.findall(r'#\w+', row['tweet'])
  if len(hashtags) >= 1:
    for hashtag in hashtags:
      ners.append({ 'type': 'Hashtag', 'text': hashtag })
  
  for ner in ners:
    adj_polarity = response['polarity']
    if response['result'] == 'NEGATIVE':
      adj_polarity = response['polarity'] * -1
    try:
      ner['type']
    except:
      ner['type'] = ''      
    nerlst.append([ row['tweet_dt'], row['topic'], row['id'], row['username'], 
                   row['name'], row['tweet'], ner['type'], ner['text'], response['result'], 
                   response['polarity'], adj_polarity, row['like_count'], row['reply_count'], 
                  row['retweet_count'] ])

clear_output()

df_ner = pd.DataFrame(nerlst, columns=['tweet_dt', 'topic', 'id', 'username', 'name', 'tweet', 'tag_type', 'tag', 'sentiment', 'polarity', 
                                       'adj_polarity','like_count', 'reply_count', 'retweet_count'])
df_ner.head()

"""Let's filter out obvious tags like #Seattle that would show up for this search. You can comment this portion out or use different Tags for your list."""

# CUSTOMIZE: After initial Exploratory Data Analysis (EDA), identify the 'stopwords' you need to filter out
#            of your results because they are too frequent, don't add much meaning and drown out more important
#            but less frequent terms.

# filter out obvious tags - change this to filter out terms relevant to your own search results

banned_words = ['Election', 'Fraud', 'Trump', 'Biden', 'Republicans']

df_ner = df_ner[~df_ner['tag'].isin(banned_words)]

"""Calculate Frequency, Likes, Replies, Retweets and Average Polarity per Tag."""

ner_groups = df_ner.groupby(['tag', 'tag_type']).agg({'tag': "count", 'adj_polarity': "mean",
                                                     'like_count': 'sum', 'reply_count': 'sum',
                                                     'retweet_count': 'sum'})
ner_groups = ner_groups.rename(columns={
    "tag": "Frequency",
    "adj_polarity": "Avg_Polarity",
    "like_count": "Total_Likes",
    "reply_count": "Total_Replies",
    "retweet_count": "Total_Retweets"
})
ner_groups = ner_groups.sort_values(['Frequency'], ascending=False)
ner_groups = ner_groups.reset_index()
ner_groups.head()

"""Create an overall Sentiment column based on the Average Polarity of the Tag."""

ner_groups['Sentiment'] = np.where(ner_groups['Avg_Polarity']>=0, 'POSITIVE', 'NEGATIVE')
ner_groups.head()

# CHECKPOINT: Save work

# Save time restricted Named Entity Relationship-Sentiment Analysis filtered 
# search results to a time stamped file named after the search term and number of tweets

timestr = time.strftime("%Y%m%d-%H%M%S")

# Uncomment if you filtered out by date range
# file_name = f"tweets_ner_sa_{maxTweets}_{searchTerm}_{start_dt}-thru-{end_dt}_on-{timestr}.csv"

# Uncomment if you did not filter out by date range
file_name = f"tweets_ner_sa_{maxTweets}_{searchTerm}_on-{timestr}.csv"


tweet_df.to_csv(file_name, encoding='utf-8')

"""## Visualize!

We can get some bar plots for the Tags based on the following metrics:



*   Most Popular Tweets
*   Most Liked Tweets
*   Most Replied Tweets
*   Most Retweeted Tweets

By default, we do the analysis on all the Tags but we can also filter by Tag by checking the Filter_TAG box. 
This way we can further drill down into the metrics for Hashtags, Persons, Locations & Organizations.

We cut the plots by Sentiment i.e. the color of the bars tells us if the overall Sentiment was Positive or Negative.
"""

# CUSTOMIZE:
# In the code cell below, do some Exploratory Data Analysis (EDA) by trying to filter
# by various TAGs, see if you can find TAGs that help clarify, focus and reveal interesting
# relationships and trends

#@title Visualize Top TAGs
Filter_TAG = False #@param {type:"boolean"}
TAG = 'Person' #@param ["Hashtag", "Person", "Location", "Organization"]
#@markdown ###Pick how many tags to display per chart:
Top_N = 10 #@param {type:"integer"}

# get TAG value
if TAG != 'Hashtag':
  TAG = TAG[:3].upper()

if Filter_TAG:
  filtered_group = ner_groups[(ner_groups['tag_type'] == TAG)]
else:
  filtered_group = ner_groups

# plot the figures
fig = plt.figure(figsize=(20, 16))
fig.subplots_adjust(hspace=0.2, wspace=0.5)

ax1 = fig.add_subplot(321)
sns.barplot(x="Frequency", y="tag", data=filtered_group[:Top_N], hue="Sentiment")
ax2 = fig.add_subplot(322)
filtered_group = filtered_group.sort_values(['Total_Likes'], ascending=False)
sns.barplot(x="Total_Likes", y="tag", data=filtered_group[:Top_N], hue="Sentiment")
ax3 = fig.add_subplot(323)
filtered_group = filtered_group.sort_values(['Total_Replies'], ascending=False)
sns.barplot(x="Total_Replies", y="tag", data=filtered_group[:Top_N], hue="Sentiment")
ax4 = fig.add_subplot(324)
filtered_group = filtered_group.sort_values(['Total_Retweets'], ascending=False)
sns.barplot(x="Total_Retweets", y="tag", data=filtered_group[:Top_N], hue="Sentiment")

ax1.title.set_text('Most Popular')
ax2.title.set_text('Most Liked')
ax3.title.set_text('Most Replied')
ax4.title.set_text('Most Retweeted')

ax1.set_ylabel('')    
ax1.set_xlabel('')
ax2.set_ylabel('')    
ax2.set_xlabel('')
ax3.set_ylabel('')    
ax3.set_xlabel('')
ax4.set_ylabel('')    
ax4.set_xlabel('')

"""###Get the Average Polarity Distribution."""

# EXPLAIN: What does the shape of this polarity distribution tell you
#
# Be specific and explain the features (peaks/valleys/sharpness) seen
#
# Answer: The sentiment has been very negative as can be seen by the first large peak. There is a small positive peak but that is negligible in comparison.
#

fig = plt.figure(figsize=(12, 6))
sns.distplot(filtered_group['Avg_Polarity'], hist=False, kde_kws={"shade": True})

"""## Word Cloud

Let's build a Word Cloud based on these metrics. 

Since I am interested in Seattle, I am going to use overlay the Seattle city skyline view over my Word Cloud. 
You can change this by selecting a different Mask option from the drop down.

Images for Masks can be found at:

http://clipart-library.com/clipart/2099977.htm

https://needpix.com
"""

# CUSTOMIZE: Find a *.jpg image that would make an appropriate silhouette for your wordcloud of tweets
#            and !wget it below

# download mask images
!wget http://clipart-library.com/img/2099977.jpg -O seattle.jpg
!wget https://storage.needpix.com/rsynced_images/trotting-horse-silhouette.jpg -O horse.jpg
!wget https://storage.needpix.com/rsynced_images/black-balloon.jpg -O balloon.jpg
  
clear_output()

#@title Build Word Cloud For Top TAGs
Metric = 'Most Popular' #@param ["Most Popular", "Most Liked", "Most Replied", "Most Retweeted"]
#@markdown
Filter_TAG = False #@param {type:"boolean"}
##@markdown
TAG = 'Location' #@param ["Hashtag", "Person", "Location", "Organization"]
Mask = 'Seattle' #@param ["Rectangle", "Seattle", "Balloon", "Horse"]

# CUSTOMIZE: edit Mask and 'if MASK' lines in this code block to 
#            use your more appropriate silhouette image for your wordcloud

# get correct Metric value
if Metric == 'Most Popular':
   Metric = 'Frequency'
elif Metric == 'Most Liked':
   Metric = 'Total_Likes'
elif Metric == 'Most Replied':
   Metric = 'Total_Replies'
elif Metric == 'Most Retweeted':
   Metric = 'Total_Retweets'    

# get TAG value
if TAG != 'Hashtag':
  TAG = TAG[:3].upper()

if Filter_TAG:
  filtered_group = ner_groups[(ner_groups['tag_type'] == TAG)]
else:
  filtered_group = ner_groups

countDict = {}

for index, row in filtered_group.iterrows():
  if row[Metric] == 0:
    row[Metric] = 1
  countDict.update( {row['tag'] : row[Metric]} )
  
if Mask == 'Seattle':
  Mask = np.array(Image.open("seattle.jpg"))
elif Mask == 'Rectangle':
  Mask = np.array(Image.new('RGB', (800,600), (0, 0, 0)))
elif Mask == 'Horse':
  Mask = np.array(Image.open("horse.png"))
elif Mask == 'Balloon':
  Mask = np.array(Image.open("balloon.jpg"))

clear_output()

# Generate Word Cloud
wordcloud = WordCloud(
    max_words=100,
#     max_font_size=50,
    height=300,
    width=800,
    background_color = 'white',
    mask=Mask,
    contour_width=1,
    contour_color='steelblue',
    stopwords = STOPWORDS).generate_from_frequencies(countDict)
fig = plt.figure(
    figsize = (18, 18),
    )
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()

import random 

coin = random.randint(1, 2)

print("Congradulations!, you're ")
for i in range(10):
  coin = random.randint(1, 2)
  if coin == 1:
    print("-- muy caliente --")
  else:
    print("---- en fuego ----")

