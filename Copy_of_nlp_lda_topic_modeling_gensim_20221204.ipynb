{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxlc123/iphs_290_cultural_analytics_lda_topic_modeling/blob/main/Copy_of_nlp_lda_topic_modeling_gensim_20221204.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA Topic Modeling with Gensim\n",
        "\n",
        "Heavily modified and expanded based upon Gensim tutorial\n",
        "\n",
        "By Jon Chun\n",
        "28 Mar 2022 Updated"
      ],
      "metadata": {
        "id": "r4WqLPGjiQvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Configuration"
      ],
      "metadata": {
        "id": "mgacJy4F2oPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-slugify"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzuqzcKenhRB",
        "outputId": "1ab6f7da-9030-460c-dfbc-1a3a7abb7424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (7.0.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify) (1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyldavis"
      ],
      "metadata": {
        "id": "VNlvDgGYwXl7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d29fc601-f19e-4831-b96a-8d54525b7939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyldavis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 3.9 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from pyldavis) (1.7.3)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from pyldavis) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from pyldavis) (57.4.0)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from pyldavis) (1.3.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (from pyldavis) (3.6.0)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from pyldavis) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.8/dist-packages (from pyldavis) (1.21.6)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.8/dist-packages (from pyldavis) (2.8.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from pyldavis) (1.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from pyldavis) (2.11.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.0->pyldavis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.0->pyldavis) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyldavis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim->pyldavis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->pyldavis) (2.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->pyldavis) (3.1.0)\n",
            "Building wheels for collected packages: pyldavis, sklearn\n",
            "  Building wheel for pyldavis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyldavis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136898 sha256=9e0cc711c04f1d2d3eaaacbcad2eef99bd774f7038f74fb24cc742c2b7313c82\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/61/ec/9dbe9efc3acf9c4e37ba70fbbcc3f3a0ebd121060aa593181a\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=e85a8ae641e97434dbd6f9d8928648eb7756b656195bc864c541ee27789994ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
            "Successfully built pyldavis sklearn\n",
            "Installing collected packages: sklearn, funcy, pyldavis\n",
            "Successfully installed funcy-1.17 pyldavis-3.3.1 sklearn-0.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "_vga1QAOwbzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22777922-60a0-46fd-9ec7-79a081409977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (110 kB)\n",
            "\u001b[K     |████████████████████████████████| 110 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 31.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.73 pyahocorasick-1.4.4 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure Jupyter Notebook"
      ],
      "metadata": {
        "id": "JhMpuuFmVsJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Configure Jupyter Notebook\n",
        "\n",
        "# Ignore warnings\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Enable multiple outputs from one code cell\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Image\n",
        "from ipywidgets import widgets, interactive\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "metadata": {
        "id": "FsUfs2XNVqiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "8qCH0NT521pv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7MRHAaZgftt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from slugify import slugify"
      ],
      "metadata": {
        "id": "TVpojXuroPXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Text Data\n",
        "\n",
        "You have (2) ways to get data in this tutorial, but if you're following\n",
        "this tutorial just to learn about LDA I encourage you to consider picking a\n",
        "corpus on a subject that you are familiar with. Qualitatively evaluating the\n",
        "output of an LDA model is challenging and can require you to understand the\n",
        "subject matter of your corpus (depending on your goal with the model).\n",
        "\n",
        "Reference to Compare::\n",
        "\n",
        "    The NeurIPS corpus contains 1740 documents, and not particularly long ones.\n",
        "\n",
        "    `website <http://www.cs.nyu.edu/~roweis/data.html>`\n",
        "    \n",
        "    So keep in mind that this tutorial is not geared towards efficiency, and be\n",
        "    careful before applying the code to a large dataset.\n"
      ],
      "metadata": {
        "id": "T_hXCqqYl5H1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f18Z61fYtvpN"
      },
      "source": [
        "## Upload Raw Textfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "TEXT_ENCODING = 'utf-8'"
      ],
      "metadata": {
        "id": "ew2AfqTXk5Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OPTION (i): Novels"
      ],
      "metadata": {
        "id": "g_RueeussaRO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "7sKJqMys8gUR",
        "outputId": "e565cdae-1b05-48c7-db5f-9009292357b4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3878d953-3c81-499a-a620-bb38f0437561\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3878d953-3c81-499a-a620-bb38f0437561\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TheGreatGatsby_FScottFitzgerald.txt to TheGreatGatsby_FScottFitzgerald.txt\n",
            "User uploaded file \"TheGreatGatsby_FScottFitzgerald.txt\" with length 274704 bytes\n",
            "CPU times: user 745 ms, sys: 77.5 ms, total: 822 ms\n",
            "Wall time: 1min 5s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 1m07s\n",
        "\n",
        "# Upload Plain Text File\n",
        "uploaded = files.upload()\n",
        "\n",
        "# NOTE: Allows for multiple file uploads, will only process the last\n",
        "#       Left in for future feature addition (processing multiple files at once)\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  text_filename_str = fn\n",
        "\n",
        "# Extract from Dict and decode binary into char string\n",
        "text_raw_str = uploaded[text_filename_str].decode(TEXT_ENCODING)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_raw_str[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "cXsAVFL1WiTL",
        "outputId": "85cd47e7-40e6-4790-ff2c-fcb4beddc04e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1\\r\\n\\r\\n\\r\\n\\r\\nIn my younger and more vulnerable years my father gave me some advice\\r\\nthat I\\'ve been turning over in my mind ever since.\\r\\n\\r\\n\"Whenever you feel like criticizing any one,\" he told me, \"just\\r\\nremember that all the people in this world haven\\'t had the advantages\\r\\nthat you\\'ve had.\"\\r\\n\\r\\nHe didn\\'t say any more but we\\'ve always been unusually communicative\\r\\nin a reserved way, and I understood that he meant a great deal more\\r\\nthan that. In consequence I\\'m inclined to reserve all judgment'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OPTION (ii): Tweets"
      ],
      "metadata": {
        "id": "457wC7i-sfuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 1m07s\n",
        "\n",
        "# Upload Plain Text File\n",
        "uploaded = files.upload()\n",
        "\n",
        "# NOTE: Allows for multiple file uploads, will only process the last\n",
        "#       Left in for future feature addition (processing multiple files at once)\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  text_filename_str = fn\n",
        "\n",
        "# Extract from Dict and decode binary into char string\n",
        "text_raw_str = uploaded[text_filename_str].decode(TEXT_ENCODING)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "kI8vrvN8Ws1T",
        "outputId": "2927874f-959c-4e96-93be-987618736fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a8da2d3c-587e-4163-8c52-0b1d768212f2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a8da2d3c-587e-4163-8c52-0b1d768212f2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'tweets_raw_twitter-acquisition_2022-11-07_12-52-15_2022-12-06_07-15-49.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recover parameters of original search from filename components\n",
        "word1, word2, search_str, start_date, start_time, end_date, end_time = text_filename_str.split('_')\n",
        "\n",
        "print(f'word1: {word1}')\n",
        "print(f'word2: {word2}')\n",
        "print(f'search_str: {search_str}')\n",
        "print(f'start_date: {start_date}')\n",
        "print(f'end_date: {end_date}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKTb2IjuZ2Qz",
        "outputId": "83c7fa7d-7ab1-43d5-dfbc-bf038ce1c06a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word1: tweets\n",
            "word2: raw\n",
            "search_str: twitter-acquisition\n",
            "start_date: 2022-11-07\n",
            "end_date: 2022-12-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TWITTER\n",
        "\n",
        "# Read raw tweet datafile into DataFrame 'data'\n",
        "\n",
        "try:\n",
        "  headers = ['datetime','id','tweet','username','reply_ct','retweet_ct','like_ct','quote_ct','retweeted_id','quoted_id',\n",
        "             'text_raw','text_noemoji','text_clean','vader','textblob','distilbert'] # ,'roberta']\n",
        "  dtypes = {'datetime':'str',\n",
        "            'id':'int',\n",
        "            'tweet':'string',\n",
        "            'username':'string',\n",
        "            'reply_ct':'int',\n",
        "            'retweet_ct':'int',\n",
        "            'like_ct':'int',\n",
        "            'quote_ct':'int',\n",
        "            'retweeted_id':'string',\n",
        "            'quoted_id':'string',\n",
        "            'text_raw':'string',\n",
        "            'text_noemoji':'string',\n",
        "            'text_clean':'string',\n",
        "            'vader':'float',\n",
        "            'textblob':'float',\n",
        "            'distilbert':'float'}\n",
        "            # 'roberta':'float'}\n",
        "  parse_dates = ['datetime']\n",
        "  text_df = pd.read_csv(text_filename_str, skiprows=[0], names=headers, dtype=dtypes, parse_dates=parse_dates, index_col=None)\n",
        "  text_df.head()\n",
        "  text_df.info()\n",
        "except:\n",
        "  print(f'ERROR: Tweets datafile cannot be read by pd.read_csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "PljN8v0dl_xj",
        "outputId": "3739f75b-48f2-40e1-f600-194af89b3ab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   datetime                   id  \\\n",
              "0 2022-11-07 12:52:15+00:00  1589601624531423233   \n",
              "1 2022-11-07 12:53:27+00:00  1589601928870514690   \n",
              "2 2022-11-07 12:53:28+00:00  1589601932364361728   \n",
              "3 2022-11-07 12:54:35+00:00  1589602211533066246   \n",
              "4 2022-11-07 12:56:12+00:00  1589602618015449090   \n",
              "\n",
              "                                               tweet         username  \\\n",
              "0  I love how we add \"gate\" to every scandal out ...      DirtyDryIce   \n",
              "1  the thing I find most hilarious about a migrat...  elfexgirlfriend   \n",
              "2  While elon musk said 'comedy is now legal' fol...        psalm7115   \n",
              "3  Musk’s $44 billion acquisition deal piled Twit...      AnneHandler   \n",
              "4  EsensConsulting: #Decentralized : #Mastodon re...    Guillaume_GCH   \n",
              "\n",
              "   reply_ct  retweet_ct  like_ct  quote_ct retweeted_id quoted_id  \\\n",
              "0         0           0        0         0         <NA>      <NA>   \n",
              "1         1           0        4         0         <NA>      <NA>   \n",
              "2         0           0        1         0         <NA>      <NA>   \n",
              "3         0           0        0         0         <NA>      <NA>   \n",
              "4         0           0        0         1         <NA>      <NA>   \n",
              "\n",
              "                                            text_raw  \\\n",
              "0  I love how we add \"gate\" to every scandal out ...   \n",
              "1  the thing I find most hilarious about a migrat...   \n",
              "2  While elon musk said 'comedy is now legal' fol...   \n",
              "3  Musk’s $44 billion acquisition deal piled Twit...   \n",
              "4  EsensConsulting: #Decentralized : #Mastodon re...   \n",
              "\n",
              "                                        text_noemoji  \\\n",
              "0  I love how we add \" gate \" to every scandal ou...   \n",
              "1  the thing I find most hilarious about a migrat...   \n",
              "2  While elon musk said ' comedy is now legal ' f...   \n",
              "3  Musk ' s $ 44 billion acquisition deal piled T...   \n",
              "4  EsensConsulting #Decentralized #Mastodon recor...   \n",
              "\n",
              "                                          text_clean  vader  textblob  \\\n",
              "0  love add gate scandal matter insignificant wan...    NaN       NaN   \n",
              "1  thing find hilarious migration tumblr half fri...    NaN       NaN   \n",
              "2  elon musk say comedy legal follow acquisition ...    NaN       NaN   \n",
              "3  musk s billion acquisition deal pile twitter b...    NaN       NaN   \n",
              "4  esensconsulting decentralize mastodon record k...    NaN       NaN   \n",
              "\n",
              "   distilbert  \n",
              "0         NaN  \n",
              "1         NaN  \n",
              "2         NaN  \n",
              "3         NaN  \n",
              "4         NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a62376e0-0874-4eef-849a-ee28be11087b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datetime</th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>username</th>\n",
              "      <th>reply_ct</th>\n",
              "      <th>retweet_ct</th>\n",
              "      <th>like_ct</th>\n",
              "      <th>quote_ct</th>\n",
              "      <th>retweeted_id</th>\n",
              "      <th>quoted_id</th>\n",
              "      <th>text_raw</th>\n",
              "      <th>text_noemoji</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>vader</th>\n",
              "      <th>textblob</th>\n",
              "      <th>distilbert</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-11-07 12:52:15+00:00</td>\n",
              "      <td>1589601624531423233</td>\n",
              "      <td>I love how we add \"gate\" to every scandal out ...</td>\n",
              "      <td>DirtyDryIce</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>I love how we add \"gate\" to every scandal out ...</td>\n",
              "      <td>I love how we add \" gate \" to every scandal ou...</td>\n",
              "      <td>love add gate scandal matter insignificant wan...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-11-07 12:53:27+00:00</td>\n",
              "      <td>1589601928870514690</td>\n",
              "      <td>the thing I find most hilarious about a migrat...</td>\n",
              "      <td>elfexgirlfriend</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>the thing I find most hilarious about a migrat...</td>\n",
              "      <td>the thing I find most hilarious about a migrat...</td>\n",
              "      <td>thing find hilarious migration tumblr half fri...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-11-07 12:53:28+00:00</td>\n",
              "      <td>1589601932364361728</td>\n",
              "      <td>While elon musk said 'comedy is now legal' fol...</td>\n",
              "      <td>psalm7115</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>While elon musk said 'comedy is now legal' fol...</td>\n",
              "      <td>While elon musk said ' comedy is now legal ' f...</td>\n",
              "      <td>elon musk say comedy legal follow acquisition ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-11-07 12:54:35+00:00</td>\n",
              "      <td>1589602211533066246</td>\n",
              "      <td>Musk’s $44 billion acquisition deal piled Twit...</td>\n",
              "      <td>AnneHandler</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>Musk’s $44 billion acquisition deal piled Twit...</td>\n",
              "      <td>Musk ' s $ 44 billion acquisition deal piled T...</td>\n",
              "      <td>musk s billion acquisition deal pile twitter b...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022-11-07 12:56:12+00:00</td>\n",
              "      <td>1589602618015449090</td>\n",
              "      <td>EsensConsulting: #Decentralized : #Mastodon re...</td>\n",
              "      <td>Guillaume_GCH</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>EsensConsulting: #Decentralized : #Mastodon re...</td>\n",
              "      <td>EsensConsulting #Decentralized #Mastodon recor...</td>\n",
              "      <td>esensconsulting decentralize mastodon record k...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a62376e0-0874-4eef-849a-ee28be11087b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a62376e0-0874-4eef-849a-ee28be11087b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a62376e0-0874-4eef-849a-ee28be11087b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 16 columns):\n",
            " #   Column        Non-Null Count  Dtype              \n",
            "---  ------        --------------  -----              \n",
            " 0   datetime      10000 non-null  datetime64[ns, UTC]\n",
            " 1   id            10000 non-null  int64              \n",
            " 2   tweet         10000 non-null  string             \n",
            " 3   username      10000 non-null  string             \n",
            " 4   reply_ct      10000 non-null  int64              \n",
            " 5   retweet_ct    10000 non-null  int64              \n",
            " 6   like_ct       10000 non-null  int64              \n",
            " 7   quote_ct      10000 non-null  int64              \n",
            " 8   retweeted_id  0 non-null      string             \n",
            " 9   quoted_id     701 non-null    string             \n",
            " 10  text_raw      10000 non-null  string             \n",
            " 11  text_noemoji  10000 non-null  string             \n",
            " 12  text_clean    10000 non-null  string             \n",
            " 13  vader         0 non-null      float64            \n",
            " 14  textblob      0 non-null      float64            \n",
            " 15  distilbert    0 non-null      float64            \n",
            "dtypes: datetime64[ns, UTC](1), float64(3), int64(5), string(7)\n",
            "memory usage: 1.2 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reform Tweets to assume the form of a Novel\n",
        "\n",
        "line_ls = []\n",
        "\n",
        "for i, aline in enumerate(text_df.text_clean.to_list()):\n",
        "  if (i % 100) == 0:\n",
        "    print(f'Line #{i}: {aline}')\n",
        "  line_ls.append(aline)\n",
        "\n",
        "print('\\n\\n')\n",
        "\n",
        "text_raw_str = \"\\n\\n\".join(line_ls)\n",
        "text_raw_str[:500]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ea8MsCi_XwDo",
        "outputId": "265f6bdf-37df-4599-fe5c-5e9f4f24f993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Line #0: love add gate scandal matter insignificant want twitter acquisition elongate come dangerously close give compliment\n",
            "Line #100: musk twitter acquisition mean social medium crypto adoption url finance fintech cryptocurrencie\n",
            "Line #200: don t disclose actual acquisition\n",
            "Line #300: support russian invasion way japan timeline calm acquisition japanese talk change issue war originally twitter japanese\n",
            "Line #400: face tear joy face tear joy face tear joy face tear joy go track twitter acquisition arrogance overconfidence fine line\n",
            "Line #500: real agenda musk s acquisition twitter shill trump\n",
            "Line #600: elon musk news acquisition twitter likely save democracy damage war elite win t reply text message url\n",
            "Line #700: elon musk s twitter acquisition affect midterm url\n",
            "Line #800: joke elon musk acquisition twitter fresh wind sail dogecoin meme cryptocurrency catapult fame url\n",
            "Line #900: elon musk twitter acquisition affect midterm url url\n",
            "Line #1000: want delete tweet remove bunch tweet permanently close twitter account wake elon musk acquisition farewell url\n",
            "Line #1100: prince alwaleed shareholder decade elon acquisition notice elon prince alwaleed necessarily friendly term b c elon hard saudi govt source twitter dissident datum\n",
            "Line #1200: continue astound impacting powerful communication mobile phone tool spend hour day twitter man office worker acquisition twitter url\n",
            "Line #1300: mastodon usage spike musk s twitter acquisition url\n",
            "Line #1400: search mastodon spike google follow twitter acquisition especially europe social network base url\n",
            "Line #1500: think people claim elon acquisition twitter affect election grin squint face grin squint face grin squint face\n",
            "Line #1600: musky b net worth morning lose wealth past week entire twitter acquisition cost url\n",
            "Line #1700: don t understand people arm think lot potential platform s kind dull prior acquisition\n",
            "Line #1800: what think question reporter bloomberg ask biden press conference think threat s national security w tool investigate joint acquisition twitter w foreign govt include saudis\n",
            "Line #1900: cnbc jim cramer collapse ftx musk acquisition twitter layoff meta url\n",
            "Line #2000: musk seek reassure uneasy advertiser twitter reality setting turbulent acquisition bloom social medium twitter chaos facebook contraction prov url\n",
            "Line #2100: tesla stockholder watch tesla stock tank twitter acquisition role don t let lead twitter focus company especially tesla try deliberately tank tesla stock value\n",
            "Line #2200: wonder mank feel musk s acquisition running twitter\n",
            "Line #2300: twitter s security privacy compliance leader leave company hour week acquisition billionaire elon musk url\n",
            "Line #2400: wish business school case study twitter s acquisition transition pure clown face shit\n",
            "Line #2500: elon musk twitter deal receive medium coverage past month collision project crypto base solution actively work laptop crypto twitter acquisition blockchain web binance virtualreality url\n",
            "Line #2600: twitter mfs treat elon acquisition like end summer camp skull skull url\n",
            "Line #2700: tesla ceo elon musk share tweet recently take dig criticism twitter receive follow acquisition micro blogge platform read url\n",
            "Line #2800: new twitter billion user platform pay sign new platform billion user billion spend user acquisition instead billion happen support let free bird\n",
            "Line #2900: elon live silicon valley sort twitter acquisition\n",
            "Line #3000: look twitter s acquisition message platform tape acquire year ago plan use tech improve dm actually come\n",
            "Line #3100: opinion people concern elon musk s acquisition twitter clearly good step implement freedom speech twitter read article url\n",
            "Line #3200: week acquisition twitter elon mush consider bankruptcy genius\n",
            "Line #3300: um inform twitter blue exist musk acquisition continue exist twitter blue premium subscription service include ability edit tweet musk go bundle tie blue check verification blue near future\n",
            "Line #3400: translation omnicom media group advise customer temporarily stop advertise twitter follow acquisition platform ilon musk reason recommendatio source url media url\n",
            "Line #3500: understanding debt twitter book problem acquisition interest twitter debt grow b year forget think like m\n",
            "Line #3600: post lie twitter billion annual debt service debt take result acquisition bold face lie twitter debt result acquisition take musk personally\n",
            "Line #3700: elon musk favour big eyes coin dogecoin twitter acquisition url\n",
            "Line #3800: twitter acquisition war room idea come remind people prime ripe innovate position power generally ve elon fan don t think s embarrass right cmon\n",
            "Line #3900: itsagundam hour long video roasting detractor acquisition later note eli lilly cost company twitter go anytime soon idea have laugh big pharma expense great company url\n",
            "Line #4000: cnn elon musk say work absolute morning today learn night seven day week ask recent acquisition twitter leadership automaker tesla url\n",
            "Line #4100: acquisition price social medium company twitter billion linkedin billion youtube billion tumblr billion instagram billion myspace million vine million good deal\n",
            "Line #4200: brazilian tired twitter continue censor comment favor country freedom expression acquisition\n",
            "Line #4300: bad investigate twitter acquisition ground national security new ownership friendly politically\n",
            "Line #4400: talk rock social medium street news outlet elon musk acquisition twitter begin april year finalise buyout october lot change san francisco base micro blogge platform url\n",
            "Line #4500: feel world cut little slack guy good businessman hiccup acquisition think twitter well place long run open mind ceo day chance\n",
            "Line #4600: yellen basis investigate musk s twitter acquisition url url\n",
            "Line #4700: think musk acquisition twitter rational business point view twtr tsla url\n",
            "Line #4800: promise screw yahoo ceo marissa mayer immediately screw lose bil acquisition tumblr hold beer elon old musky musk launch twitter\n",
            "Line #4900: s initial burst activity need post acquisition reorganise company musk say testimony expect reduce time twitter url\n",
            "Line #5000: surprise hillary clinton link dark money group set sight twitter advertiser amid elon musk s acquisition social medium platform fox business learn url\n",
            "Line #5100: twitter acquisition go real smooth url\n",
            "Line #5200: nannimus post acquisition twitter feel like night diner minute bar close like endless september s usenet aol user get access like go need find someplace hang embrace incivility chaos\n",
            "Line #5300: eye m elon tesla stock collateral twitter acquisition bk s twitter lose manage control tesla s need know future twitter\n",
            "Line #5400: tell x holdings iii llc margin loan borrower collateralized outstanding tesla shares jpm margin finance debt twitter acquisition skull p s x holdings ii llc official entity consummate acquisition\n",
            "Line #5500: wow twitter go bonker say second bad acquisition history url\n",
            "Line #5600: remember couple day twitter acquisition complete musk fly engineer china help fremont tesla bullshit tsla tslaq twtr url\n",
            "Line #5700: elon s acquisition twitter people consider place mean life doubt downside s bring positive people variety way friendship career opportunity kind crazy think\n",
            "Line #5800: think beginning end twitter acquisition deal\n",
            "Line #5900: maybe twitter acquisition elon elaborate plot commoner billionaire mean smart well homie show roulette table billion bet white\n",
            "Line #6000: musk play pride twitter employee start acquisition soon realise cost end soon twitter version downgrade version time good luck decision\n",
            "Line #6100: thing journalism lose way change elon musk s acquisition twitter lead shutdown\n",
            "Line #6200: immediately acquisition s close wave racist antisemitic trolling emerge twitter wary marketer slow pause ad spending platform kick crisis company protect precious ad revenue url\n",
            "Line #6300: don t ask advertiser instead major brand don t come twitter acquisition dead water\n",
            "Line #6400: know saudi arabian prince help musk finance acquisition twitter\n",
            "Line #6500: late s networth b elon air qatar world cup live twitter somewhat way please qia profit wait raise m qia twitter acquisition investment fishy\n",
            "Line #6600: like twitter acquisition backfire time trump creature sprout shyte hundred reply see people point lie influence ex supporter fed\n",
            "Line #6700: stat twitter usage acquisition\n",
            "Line #6800: think beginning end twitter acquisition deal\n",
            "Line #6900: digital world acquisition corp share trade low twitter ceo elon musk reinstate donald trumps twitter account dwac\n",
            "Line #7000: twitofshit elon like lot thing space x tesla etc head twitter acquisition need know intellectual limit b\n",
            "Line #7100: try snarky try understand build koo post twitter customer acquisition\n",
            "Line #7200: compare ftx catastrophe s twitter acquisition url\n",
            "Line #7300: normal person hype twitter acquisition extraordinary need medium hue cry witness good thing die bad survives long run\n",
            "Line #7400: like twitter complete failure company financially acquisition lineup people want work musk long position fill twitter\n",
            "Line #7500: think s significant contribution society isn t s acquisition free speech expose elite s power grab ideation url\n",
            "Line #7600: check shocking datum crypto twitter spam wake elon acquisition url\n",
            "Line #7700: recall criticism receive acquisition management twitter versus tesla spacex ironically tesla spacex herald genius twitter destroy democracy cheeky erson facepalme person shrug\n",
            "Line #7800: engineer dev job tesla spacex good job engineering cs grad talent acquisition unparalleled industry function s try create twitter rebuild destroy\n",
            "Line #7900: twitter acquisition hurt tesla stock noise slowly die recent lose memory\n",
            "Line #8000: le pdg de spacex et de tesla elon musk promis de creer son propre smartphone si apple et google retirent lapplication twitter sa recente acquisition bien qu il espere ne pas en arriver la\n",
            "Line #8100: fortune company pause ad twitter elon musk s acquisition url\n",
            "Line #8200: famous equal egomania successful financially entitle twitter acquisition vanity wrest control medium poison welcome\n",
            "Line #8300: feel different acquisition twitter elite t control narrative time truly think beginning wild\n",
            "Line #8400: yes like foam mouth acquisition twitter intolerance way like s ok censor disagree agenda freedom speech mean apple show true color url\n",
            "Line #8500: interesting recall m annual amt twitter pay interest debt musk acquisition url\n",
            "Line #8600: ren nochang ha ninarunogahutsuunanokana elon musk acquisition twitter mitaina\n",
            "Line #8700: s acquisition historic focus controversy see miss innovation play mind wealthy person world think thing watch twitter musk era url url\n",
            "Line #8800: try install android musk acquisition android snanana let\n",
            "Line #8900: wonder twitter acquisition loan tie fed interest rate url\n",
            "Line #9000: cook yeah s go happen twitter terribly manage company rarely turn profit elon upside acquisition twitter twitter owe b yr interest payment s buy apple buy tesla\n",
            "Line #9100: good point executive trust give publicly report stonewalling elon musk s valid inquiry acquisition\n",
            "Line #9200: run rampant difference report anon report elon nearly non stop acquisition kanye trend day awful remark change twitter ya think\n",
            "Line #9300: twitter s ui ux bad acquisition unsure say s well probably bunch hyper stan m sure look lot bug albeit tiny one\n",
            "Line #9400: right reason investigate reality twitter get close cesspool hate speech bigotry follow musk s acquisition\n",
            "Line #9500: twitter acquisition impact global south opinion hatice nur keskin url\n",
            "Line #9600: get spam dm include explicit one post acquisition twitter experience\n",
            "Line #9700: guy openly conduct political operation right twitter acquisition subsequent tweet url\n",
            "Line #9800: twitter acquisition elonmusk impact global south opinion hatice nur keskin url\n",
            "Line #9900: understand saxon musk want expose democrat corruption midterm election dilly dally month twitter acquisition saxondrama aryas understand elon cia url\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'love add gate scandal matter insignificant want twitter acquisition elongate come dangerously close give compliment\\n\\nthing find hilarious migration tumblr half friend leave tumblr twitter unfortunate acquisition subsequent purging nsfw content appealing advertiser\\n\\nelon musk say comedy legal follow acquisition twitter joke new owner criticism takeover get user block suspend url\\n\\nmusk s billion acquisition deal pile twitter billion debt result company have pay billion year interest expense despit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enter Title"
      ],
      "metadata": {
        "id": "lGy4--UQUbwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If this is a Twitter textfile, the search terms are recovered from the filename\n",
        "\n",
        "# Recover parameters of original search from filename components\n",
        "try:\n",
        "  word1, word2, search_str, start_date, start_time, end_date, end_time = text_filename_str.split('_')\n",
        "\n",
        "  print(f'word1: {word1}')\n",
        "  print(f'word2: {word2}')\n",
        "  print(f'search_str: {search_str}')\n",
        "  print(f'start_date: {start_date}')\n",
        "  print(f'end_date: {end_date}')\n",
        "  print('\\n')\n",
        "  \n",
        "  text_title_str = f'[{search_str}] from [{start_date}] to [{end_date}]'\n",
        "  print(f'\\nTwitter Search String:\\n\\n  {text_title_str}\\n')\n",
        "\n",
        "except:\n",
        "  print('This is not a Twitter datafile named in standard format\\n\\nEnter the Novel information below.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPN5ZpDJacFZ",
        "outputId": "037017d9-29aa-403a-fe51-49950698b2e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is not a Twitter datafile named in standard format\n",
            "\n",
            "Enter the Novel information below.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-JpWesZI3nUr",
        "outputId": "686ebfe6-e219-4bc8-8697-36c0df39e20a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Great Gatsby By F Scott Fitzgerald'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#@title Enter Title \"[Title] by [Author]\"\"\n",
        "\n",
        "Text_Title = \"The Great Gatsby by F. Scott Fitzgerald\" #@param {type:\"string\"}\n",
        "\n",
        "# Remove illegal filename punctuation\n",
        "Text_Title = Text_Title.replace(\"'\",\"\")\n",
        "Text_Title = Text_Title.replace('\"','')\n",
        "Text_Title = Text_Title.replace('.',' ')\n",
        "\n",
        "text_title_str = ' '.join([x.capitalize() for x in slugify(Text_Title).replace('-',' ').split()])\n",
        "text_title_str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS_9h1COgftx"
      },
      "source": [
        "\n",
        "# LDA Model\n",
        "\n",
        "Introduces Gensim's LDA model and demonstrates its use on the NIPS corpus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt_T3h4Sgft0"
      },
      "source": [
        "The purpose of this tutorial is to demonstrate how to train and tune an LDA model.\n",
        "\n",
        "In this tutorial we will:\n",
        "\n",
        "* Load input data.\n",
        "* Pre-process that data.\n",
        "* Transform documents into bag-of-words vectors.\n",
        "* Train an LDA model.\n",
        "\n",
        "This tutorial will **not**:\n",
        "\n",
        "* Explain how Latent Dirichlet Allocation works\n",
        "* Explain how the LDA model performs inference\n",
        "* Teach you all the parameters and options for Gensim's LDA implementation\n",
        "\n",
        "If you are not familiar with the LDA model or how to use it in Gensim, I (Olavur Mortensen)\n",
        "suggest you read up on that before continuing with this tutorial. Basic\n",
        "understanding of the LDA model should suffice. Examples:\n",
        "\n",
        "* `Introduction to Latent Dirichlet Allocation <http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation>`_\n",
        "* Gensim tutorial: `sphx_glr_auto_examples_core_run_topics_and_transformations.py`\n",
        "* Gensim's LDA model API docs: :py:class:`gensim.models.LdaModel`\n",
        "\n",
        "I would also encourage you to consider each step when applying the model to\n",
        "your data, instead of just blindly applying my solution. The different steps\n",
        "will depend on your data and possibly your goal with the model.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segment Text"
      ],
      "metadata": {
        "id": "JHz5Sx3FtNf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_raw_str[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "dRKz2bqYSlWv",
        "outputId": "23a50d3c-9968-4aec-9b7a-6d584b8135eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1\\r\\n\\r\\n\\r\\n\\r\\nIn my younger and more vulnerable years my father gave me some advice\\r\\nthat I\\'ve been turning over in my mind ever since.\\r\\n\\r\\n\"Whenever you feel like criticizing any one,\" he told me, \"just\\r\\nremember that all the people in this world haven\\'t had the advantages\\r\\nthat you\\'ve had.\"\\r\\n\\r\\nHe didn\\'t say any more but we\\'ve always been unusually communicative\\r\\nin a reserved way, and I understood that he meant a great deal more\\r\\nthan that. In consequence I\\'m inclined to reserve all judgment'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into paragraphs\n",
        "text_parags_ls = text_raw_str.split('\\n')\n",
        "print(f'There are {len(text_parags_ls)} raw paragraphs\\n')\n",
        "\n",
        "# Delete empty paragraphs\n",
        "text_parags_ls = [x for x in text_parags_ls if len(x.strip()) > 0]\n",
        "print(f'There are {len(text_parags_ls)} non-empty paragraphs\\n\\n')\n",
        "\n",
        "# Replace stray/embedded /n with a space\n",
        "text_parags_clean_ls = []\n",
        "for aparag in text_parags_ls:\n",
        "  text_parags_clean_ls.append(aparag.replace('\\n',' ').strip())\n",
        "\n",
        "text_parags_ls = text_parags_clean_ls\n",
        "text_parags_ls[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3FxV2LKKQrB",
        "outputId": "66ec536c-7b67-439e-96a7-aab6a9a6c79d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 6319 raw paragraphs\n",
            "\n",
            "There are 4628 non-empty paragraphs\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Chapter 1',\n",
              " 'In my younger and more vulnerable years my father gave me some advice',\n",
              " \"that I've been turning over in my mind ever since.\",\n",
              " '\"Whenever you feel like criticizing any one,\" he told me, \"just',\n",
              " \"remember that all the people in this world haven't had the advantages\"]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify first paragraph (after title and chapter headings)\n",
        "\n",
        "text_parags_ls[3]\n",
        "print('\\n')\n",
        "# Paragraph char count\n",
        "len(text_parags_ls[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "KCXF860_MIpI",
        "outputId": "171b3c66-e8d4-4b5d-e720-86e03b446dbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Whenever you feel like criticizing any one,\" he told me, \"just'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete any paragraphs shorter than MIN_LEN_PARAG\n",
        "\n",
        "MIN_LEN_PARAG = 5\n",
        "MIN_LEN_DOC = 1000\n",
        "\n",
        "# Delete any paragraphs shorter than MIN_LEN_PARAG\n",
        "text_parags_ls = [x for x in text_parags_ls if len(x) > MIN_LEN_PARAG]\n",
        "\n",
        "# Trim any leading/trailing/multiple embedded whitespaces\n",
        "text_parags_ls = [' '.join(x.split()) for x in text_parags_ls]\n",
        "\n",
        "len(text_parags_ls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AN7YcYK3KurS",
        "outputId": "8161bf39-aca1-4150-b2cb-d0e1e04c1317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4603"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Agglomerate paragraphs into Documents of MIN_LEN_DOC=1000 chars\n",
        "\n",
        "parag_ct = len(text_parags_ls)\n",
        "\n",
        "doc_now_str = ''\n",
        "doc_now_len = 0\n",
        "docs = []\n",
        "\n",
        "for i in range(parag_ct):\n",
        "  # print(f'Processing Paragraph #{i}')\n",
        "  parag_now_str = text_parags_ls[i]\n",
        "  doc_now_str += parag_now_str\n",
        "  doc_now_len += len(parag_now_str)\n",
        "  if doc_now_len > MIN_LEN_DOC:\n",
        "    docs.append(doc_now_str)\n",
        "    doc_now_str = ''\n",
        "    doc_now_len = 0\n",
        "\n",
        "docs[-1] += doc_now_str\n",
        "\n",
        "print(f'There are now {len(docs)} Documents of {MIN_LEN_DOC} chars or more')"
      ],
      "metadata": {
        "id": "eeIM8FkjKupE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a5c6c54-74ea-48e7-a799-1b5ec4e1d1b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are now 253 Documents of 1000 chars or more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first 5 docs\n",
        "\n",
        "docs[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gTq_SyWKunI",
        "outputId": "b1e98acc-97dc-47d9-8966-5ea6e8678e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Chapter 1In my younger and more vulnerable years my father gave me some advicethat I\\'ve been turning over in my mind ever since.\"Whenever you feel like criticizing any one,\" he told me, \"justremember that all the people in this world haven\\'t had the advantagesthat you\\'ve had.\"He didn\\'t say any more but we\\'ve always been unusually communicativein a reserved way, and I understood that he meant a great deal morethan that. In consequence I\\'m inclined to reserve all judgments,a habit that has opened up many curious natures to me and alsomade me the victim of not a few veteran bores. The abnormal mindis quick to detect and attach itself to this quality when itappears in a normal person, and so it came about that in college Iwas unjustly accused of being a politician, because I was privy to thesecret griefs of wild, unknown men. Most of the confidences wereunsought--frequently I have feigned sleep, preoccupation, or a hostilelevity when I realized by some unmistakable sign that an intimaterevelation was quivering on the horizon--for the intimate revelations',\n",
              " \"of young men or at least the terms in which they express them areusually plagiaristic and marred by obvious suppressions. Reservingjudgments is a matter of infinite hope. I am still a little afraid ofmissing something if I forget that, as my father snobbishly suggested,and I snobbishly repeat, a sense of the fundamental decencies isparcelled out unequally at birth.And, after boasting this way of my tolerance, I come to the admissionthat it has a limit. Conduct may be founded on the hard rock or the wetmarshes but after a certain point I don't care what it's founded on.When I came back from the East last autumn I felt that I wanted theworld to be in uniform and at a sort of moral attention forever; Iwanted no more riotous excursions with privileged glimpses into thehuman heart. Only Gatsby, the man who gives his name to this book, wasexempt from my reaction--Gatsby who represented everything for which Ihave an unaffected scorn. If personality is an unbroken series ofsuccessful gestures, then there was something gorgeous about him, some\",\n",
              " 'heightened sensitivity to the promises of life, as if he were relatedto one of those intricate machines that register earthquakes tenthousand miles away. This responsiveness had nothing to do with thatflabby impressionability which is dignified under the name of the\"creative temperament\"--it was an extraordinary gift for hope, a romanticreadiness such as I have never found in any other person and which itis not likely I shall ever find again. No--Gatsby turned out all rightat the end; it is what preyed on Gatsby, what foul dust floated in thewake of his dreams that temporarily closed out my interest in theabortive sorrows and short-winded elations of men.My family have been prominent, well-to-do people in this middle-westerncity for three generations. The Carraways are something of a clan and wehave a tradition that we\\'re descended from the Dukes of Buccleuch, but theactual founder of my line was my grandfather\\'s brother who came here infifty-one, sent a substitute to the Civil War and started the wholesale',\n",
              " 'hardware business that my father carries on today.I never saw this great-uncle but I\\'m supposed to look like him--withspecial reference to the rather hard-boiled painting that hangs inFather\\'s office. I graduated from New Haven in 1915, just a quarter of acentury after my father, and a little later I participated in thatdelayed Teutonic migration known as the Great War. I enjoyed thecounter-raid so thoroughly that I came back restless. Instead of beingthe warm center of the world the middle-west now seemed like theragged edge of the universe--so I decided to go east and learn the bondbusiness. Everybody I knew was in the bond business so I supposed itcould support one more single man. All my aunts and uncles talked itover as if they were choosing a prep-school for me and finally said,\"Why--ye-es\" with very grave, hesitant faces. Father agreed to financeme for a year and after various delays I came east, permanently, Ithought, in the spring of twenty-two.The practical thing was to find rooms in the city but it was a warm',\n",
              " 'season and I had just left a country of wide lawns and friendly trees,so when a young man at the office suggested that we take a housetogether in a commuting town it sounded like a great idea. He foundthe house, a weather beaten cardboard bungalow at eighty a month, butat the last minute the firm ordered him to Washington and I went outto the country alone. I had a dog, at least I had him for a few daysuntil he ran away, and an old Dodge and a Finnish woman who made my bedand cooked breakfast and muttered Finnish wisdom to herself over theelectric stove.It was lonely for a day or so until one morning some man, more recentlyarrived than I, stopped me on the road.\"How do you get to West Egg village?\" he asked helplessly.I told him. And as I walked on I was lonely no longer. I was a guide, apathfinder, an original settler. He had casually conferred on me thefreedom of the neighborhood.And so with the sunshine and the great bursts of leaves growing on thetrees--just as things grow in fast movies--I had that familiar']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first 500 chars in the 50th doc\n",
        "\n",
        "docs[49][:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "pK59gJoVKuRk",
        "outputId": "f8ed6909-b493-4df9-d12a-7bacc496e51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'even told me about it, and the man came after it one day when he was out.She looked around to see who was listening: \" \\'Oh, is that your suit?\\' I\\'This is the first I ever heard about it.\\' But I gave it to him and then Ilay downand cried to beat the band all afternoon.\"\"She really ought to get away from him,\" resumed Catherine to me.\"They\\'ve been living over that garage for eleven years. And Tom\\'s thefirst sweetie she ever had.\"The bottle of whiskey--a second one--was now in constant demand by al'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4jA-R3YCgft3",
        "outputId": "52be5df1-9478-4be0-e67b-33e8b1e34463"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nd one finemorning----So we beat on, boats against the current, borne back ceaselessly intothe past.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# View the last 100 chars in the last doc\n",
        "\n",
        "docs[-1][-100:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOzKW9vZgft4"
      },
      "source": [
        "## Pre-process and vectorize the documents\n",
        "\n",
        "As part of preprocessing, we will:\n",
        "\n",
        "* Expand Contractions\n",
        "* Tokenize (split the documents into tokens)\n",
        "* Define stopwords and filter out\n",
        "* Lemmatize the tokens.\n",
        "* Compute bigrams.\n",
        "* Compute a bag-of-words representation of the data.\n",
        "\n",
        "First we tokenize the text using a regular expression tokenizer from NLTK. We\n",
        "remove numeric tokens and tokens that are only a single character, as they\n",
        "don't tend to be useful, and the dataset contains a lot of them.\n",
        "\n",
        ".. Important::\n",
        "\n",
        "   This tutorial uses the nltk library for preprocessing, although you can\n",
        "   replace it with something else if you want.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Steps"
      ],
      "metadata": {
        "id": "fdb8MYBsukpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Expand Contractions"
      ],
      "metadata": {
        "id": "65I_UBaumu8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contractions (e.g. can't -> can not)\n",
        "\n",
        "import contractions\n",
        "\n",
        "# Test\n",
        "contractions.fix(\"yall're happy now it isn't a don't tell problem\", slang=False) # default: true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8mtC2FHRj9nJ",
        "outputId": "fe80761e-e174-4921-efc3-037a9a94273b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"yall're happy now it is not a do not tell problem\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand all Contractions paragraph by paragraph\n",
        "docs_clean_ls = []\n",
        "for adoc in docs:\n",
        "  docs_clean_ls.append(contractions.fix(adoc))\n",
        "\n",
        "docs = docs_clean_ls\n",
        "docs[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-FDOfPGl-W2",
        "outputId": "8d3795a9-9b09-441c-d2de-6841323bf184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Chapter 1In my younger and more vulnerable years my father gave me some advicethat I have been turning over in my mind ever since.\"Whenever you feel like criticizing any one,\" he told me, \"justremember that all the people in this world have not had the advantagesthat you have had.\"He did not say any more but we have always been unusually communicativein a reserved way, and I understood that he meant a great deal morethan that. In consequence I am inclined to reserve all judgments,a habit that has opened up many curious natures to me and alsomade me the victim of not a few veteran bores. The abnormal mindis quick to detect and attach itself to this quality when itappears in a normal person, and so it came about that in college Iwas unjustly accused of being a politician, because I was privy to thesecret griefs of wild, unknown men. Most of the confidences wereunsought--frequently I have feigned sleep, preoccupation, or a hostilelevity when I realized by some unmistakable sign that an intimaterevelation was quivering on the horizon--for the intimate revelations',\n",
              " 'of young men or at least the terms in which they express them areusually plagiaristic and marred by obvious suppressions. Reservingjudgments is a matter of infinite hope. I am still a little afraid ofmissing something if I forget that, as my father snobbishly suggested,and I snobbishly repeat, a sense of the fundamental decencies isparcelled out unequally at birth.And, after boasting this way of my tolerance, I come to the admissionthat it has a limit. Conduct may be founded on the hard rock or the wetmarshes but after a certain point I do not care what it is founded on.When I came back from the East last autumn I felt that I wanted theworld to be in uniform and at a sort of moral attention forever; Iwanted no more riotous excursions with privileged glimpses into thehuman heart. Only Gatsby, the man who gives his name to this book, wasexempt from my reaction--Gatsby who represented everything for which Ihave an unaffected scorn. If personality is an unbroken series ofsuccessful gestures, then there was something gorgeous about him, some',\n",
              " 'heightened sensitivity to the promises of life, as if he were relatedto one of those intricate machines that register earthquakes tenthousand miles away. This responsiveness had nothing to do with thatflabby impressionability which is dignified under the name of the\"creative temperament\"--it was an extraordinary gift for hope, a romanticreadiness such as I have never found in any other person and which itis not likely I shall ever find again. No--Gatsby turned out all rightat the end; it is what preyed on Gatsby, what foul dust floated in thewake of his dreams that temporarily closed out my interest in theabortive sorrows and short-winded elations of men.My family have been prominent, well-to-do people in this middle-westerncity for three generations. The Carraways are something of a clan and wehave a tradition that we are descended from the Dukes of Buccleuch, but theactual founder of my line was my grandfather\\'s brother who came here infifty-one, sent a substitute to the Civil War and started the wholesale',\n",
              " 'hardware business that my father carries on today.I never saw this great-uncle but I am supposed to look like him--withspecial reference to the rather hard-boiled painting that hangs inFather\\'s office. I graduated from New Haven in 1915, just a quarter of acentury after my father, and a little later I participated in thatdelayed Teutonic migration known as the Great War. I enjoyed thecounter-raid so thoroughly that I came back restless. Instead of beingthe warm center of the world the middle-west now seemed like theragged edge of the universe--so I decided to go east and learn the bondbusiness. Everybody I knew was in the bond business so I supposed itcould support one more single man. All my aunts and uncles talked itover as if they were choosing a prep-school for me and finally said,\"Why--ye-es\" with very grave, hesitant faces. Father agreed to financeme for a year and after various delays I came east, permanently, Ithought, in the spring of twenty-two.The practical thing was to find rooms in the city but it was a warm',\n",
              " 'season and I had just left a country of wide lawns and friendly trees,so when a young man at the office suggested that we take a housetogether in a commuting town it sounded like a great idea. He foundthe house, a weather beaten cardboard bungalow at eighty a month, butat the last minute the firm ordered him to Washington and I went outto the country alone. I had a dog, at least I had him for a few daysuntil he ran away, and an old Dodge and a Finnish woman who made my bedand cooked breakfast and muttered Finnish wisdom to herself over theelectric stove.It was lonely for a day or so until one morning some man, more recentlyarrived than I, stopped me on the road.\"How do you get to West Egg village?\" he asked helplessly.I told him. And as I walked on I was lonely no longer. I was a guide, apathfinder, an original settler. He had casually conferred on me thefreedom of the neighborhood.And so with the sunshine and the great bursts of leaves growing on thetrees--just as things grow in fast movies--I had that familiar']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenize Text"
      ],
      "metadata": {
        "id": "0APjvgrZm0hC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pplVNGTUgft4"
      },
      "outputs": [],
      "source": [
        "# Tokenize the documents.\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Split the documents into tokens.\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for idx in range(len(docs)):\n",
        "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
        "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
        "\n",
        "# Remove numbers, but not words that contain numbers.\n",
        "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
        "\n",
        "# Remove words that are only one character.\n",
        "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify docs\n",
        "\n",
        "# content and token count of first doc\n",
        "docs[0]\n",
        "print(f'\\n\\nThere are {len(docs[0])} tokens in the first document')"
      ],
      "metadata": {
        "id": "1dY809w2jeCF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e1a78e9-0ed8-436f-a546-86dd423a8bc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chapter',\n",
              " '1in',\n",
              " 'my',\n",
              " 'younger',\n",
              " 'and',\n",
              " 'more',\n",
              " 'vulnerable',\n",
              " 'years',\n",
              " 'my',\n",
              " 'father',\n",
              " 'gave',\n",
              " 'me',\n",
              " 'some',\n",
              " 'advicethat',\n",
              " 'have',\n",
              " 'been',\n",
              " 'turning',\n",
              " 'over',\n",
              " 'in',\n",
              " 'my',\n",
              " 'mind',\n",
              " 'ever',\n",
              " 'since',\n",
              " 'whenever',\n",
              " 'you',\n",
              " 'feel',\n",
              " 'like',\n",
              " 'criticizing',\n",
              " 'any',\n",
              " 'one',\n",
              " 'he',\n",
              " 'told',\n",
              " 'me',\n",
              " 'justremember',\n",
              " 'that',\n",
              " 'all',\n",
              " 'the',\n",
              " 'people',\n",
              " 'in',\n",
              " 'this',\n",
              " 'world',\n",
              " 'have',\n",
              " 'not',\n",
              " 'had',\n",
              " 'the',\n",
              " 'advantagesthat',\n",
              " 'you',\n",
              " 'have',\n",
              " 'had',\n",
              " 'he',\n",
              " 'did',\n",
              " 'not',\n",
              " 'say',\n",
              " 'any',\n",
              " 'more',\n",
              " 'but',\n",
              " 'we',\n",
              " 'have',\n",
              " 'always',\n",
              " 'been',\n",
              " 'unusually',\n",
              " 'communicativein',\n",
              " 'reserved',\n",
              " 'way',\n",
              " 'and',\n",
              " 'understood',\n",
              " 'that',\n",
              " 'he',\n",
              " 'meant',\n",
              " 'great',\n",
              " 'deal',\n",
              " 'morethan',\n",
              " 'that',\n",
              " 'in',\n",
              " 'consequence',\n",
              " 'am',\n",
              " 'inclined',\n",
              " 'to',\n",
              " 'reserve',\n",
              " 'all',\n",
              " 'judgments',\n",
              " 'habit',\n",
              " 'that',\n",
              " 'has',\n",
              " 'opened',\n",
              " 'up',\n",
              " 'many',\n",
              " 'curious',\n",
              " 'natures',\n",
              " 'to',\n",
              " 'me',\n",
              " 'and',\n",
              " 'alsomade',\n",
              " 'me',\n",
              " 'the',\n",
              " 'victim',\n",
              " 'of',\n",
              " 'not',\n",
              " 'few',\n",
              " 'veteran',\n",
              " 'bores',\n",
              " 'the',\n",
              " 'abnormal',\n",
              " 'mindis',\n",
              " 'quick',\n",
              " 'to',\n",
              " 'detect',\n",
              " 'and',\n",
              " 'attach',\n",
              " 'itself',\n",
              " 'to',\n",
              " 'this',\n",
              " 'quality',\n",
              " 'when',\n",
              " 'itappears',\n",
              " 'in',\n",
              " 'normal',\n",
              " 'person',\n",
              " 'and',\n",
              " 'so',\n",
              " 'it',\n",
              " 'came',\n",
              " 'about',\n",
              " 'that',\n",
              " 'in',\n",
              " 'college',\n",
              " 'iwas',\n",
              " 'unjustly',\n",
              " 'accused',\n",
              " 'of',\n",
              " 'being',\n",
              " 'politician',\n",
              " 'because',\n",
              " 'was',\n",
              " 'privy',\n",
              " 'to',\n",
              " 'thesecret',\n",
              " 'griefs',\n",
              " 'of',\n",
              " 'wild',\n",
              " 'unknown',\n",
              " 'men',\n",
              " 'most',\n",
              " 'of',\n",
              " 'the',\n",
              " 'confidences',\n",
              " 'wereunsought',\n",
              " 'frequently',\n",
              " 'have',\n",
              " 'feigned',\n",
              " 'sleep',\n",
              " 'preoccupation',\n",
              " 'or',\n",
              " 'hostilelevity',\n",
              " 'when',\n",
              " 'realized',\n",
              " 'by',\n",
              " 'some',\n",
              " 'unmistakable',\n",
              " 'sign',\n",
              " 'that',\n",
              " 'an',\n",
              " 'intimaterevelation',\n",
              " 'was',\n",
              " 'quivering',\n",
              " 'on',\n",
              " 'the',\n",
              " 'horizon',\n",
              " 'for',\n",
              " 'the',\n",
              " 'intimate',\n",
              " 'revelations']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "There are 172 tokens in the first document\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Customize Stopwords"
      ],
      "metadata": {
        "id": "JH33oN6vmx4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords_ls = stopwords.words('english')\n",
        "\n",
        "print(f'\\nThe first ten stopwords:')\n",
        "stopwords_ls[20:]\n",
        "print(f'\\n\\nThere are [{len(stopwords_ls)}] English stopwords imported from NLTK')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiuFCv0Ef07n",
        "outputId": "67a01027-765e-46e4-e4e7-29d8d1eebfa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The first ten stopwords:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "There are [179] English stopwords imported from NLTK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all stopwords with contractions (optional - could also remove fragments like 'll', 're', and 've' but not necessary)\n",
        "\n",
        "stopwords_ls = [x for x in stopwords_ls if not \"'\" in x]\n",
        "stopwords_ls[20:]\n",
        "print(f'\\n\\nThere are now {len(stopwords_ls)} stopwords after removing words with contractions')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4glO1gNtngd4",
        "outputId": "ebc24712-7308-4e31-93d9-d4804d09158a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['herself',\n",
              " 'it',\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " 'should',\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " 'couldn',\n",
              " 'didn',\n",
              " 'doesn',\n",
              " 'hadn',\n",
              " 'hasn',\n",
              " 'haven',\n",
              " 'isn',\n",
              " 'ma',\n",
              " 'mightn',\n",
              " 'mustn',\n",
              " 'needn',\n",
              " 'shan',\n",
              " 'shouldn',\n",
              " 'wasn',\n",
              " 'weren',\n",
              " 'won',\n",
              " 'wouldn']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "There are now 153 stopwords after removing words with contractions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[CUSTOMIZE] Stopwords list by adding or deleting tokens**"
      ],
      "metadata": {
        "id": "7aATg-NyvVX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [CUSTOMIZE] Stopwords to ADD or DELETE from default NLTK English stopword list\n",
        "\n",
        "STOPWORDS_ADD_SET = set(['bazinga', 'woo-hoo']) # Edit this list to add new words to the stopwords list\n",
        "STOPWORDS_DEL_SET = set(['the']) # Edit this list to remove exising words from the stopwords list\n",
        "\n",
        "stopwords_en_ls = list(set(stopwords_ls).difference(set(STOPWORDS_DEL_SET)).union(set(STOPWORDS_ADD_SET)))\n",
        "print(f'Final Count after Customized Add/Del: {len(stopwords_en_ls)} Stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Glbh2ePNfdEy",
        "outputId": "6283d8ef-ce7d-46ef-8490-c1c28594d99d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Count after Customized Add/Del: 154 Stopwords\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lemmatize"
      ],
      "metadata": {
        "id": "Yq08gxuCm45Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5F_B5V0gft5"
      },
      "source": [
        "We use the WordNet lemmatizer from NLTK. A lemmatizer is preferred over a\n",
        "stemmer in this case because it produces more readable words. Output that is\n",
        "easy to read is very desirable in topic modelling.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9XiwmYDg5Zh",
        "outputId": "4d16b285-b874-4019-d401-7eb0cbecd644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oF_I-F9Rgft5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67e89817-aa9a-4a6e-9632-8a86ceefd722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.63 s, sys: 71.8 ms, total: 1.7 s\n",
            "Wall time: 1.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 0m24s @03:48 on 20220228 Colab Pro \n",
        "\n",
        "# Lemmatize the documents.\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeHP0Dougft6"
      },
      "source": [
        "We find bigrams in the documents. Bigrams are sets of two adjacent words.\n",
        "Using bigrams we can get phrases like \"machine_learning\" in our output\n",
        "(spaces are replaced with underscores); without bigrams we would only get\n",
        "\"machine\" and \"learning\".\n",
        "\n",
        "Note that in the code below, we find bigrams and then add them to the\n",
        "original data, because we would like to keep the words \"machine\" and\n",
        "\"learning\" as well as the bigram \"machine_learning\".\n",
        "\n",
        ".. Important::\n",
        "    Computing n-grams of large dataset can be very computationally\n",
        "    and memory intensive.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identify Bi- and Tri-Grams"
      ],
      "metadata": {
        "id": "od5nsgvem8E_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk5VQcqSgft6"
      },
      "outputs": [],
      "source": [
        "# Compute bigrams.\n",
        "from gensim.models import Phrases\n",
        "\n",
        "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
        "bigram = Phrases(docs, min_count=20)\n",
        "for idx in range(len(docs)):\n",
        "    for token in bigram[docs[idx]]:\n",
        "        if '_' in token:\n",
        "            # Token is a bigram, add to document.\n",
        "            docs[idx].append(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5o8CBSMgft6"
      },
      "source": [
        "We remove rare words and common words based on their *document frequency*.\n",
        "Below we remove words that appear in less than 20 documents or in more than\n",
        "50% of the documents. Consider trying to remove words only based on their\n",
        "frequency, or maybe combining that with this approach.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Minimum number of documents a word must appear in: (default 20)\n",
        "\n",
        "Min_No_Documents = 5 #@param {type:\"slider\", min:5, max:100, step:1}\n",
        "\n",
        "#@markdown Max percent of documents a word can appear in: (default 0.50 or 50%)\n",
        "\n",
        "Max_Percent_Documents = 0.49 #@param {type:\"slider\", min:0.05, max:0.9, step:0.01}\n"
      ],
      "metadata": {
        "id": "nFiU_87MpQJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# docs_ls_ls = [astring.split() for astring in docs]\n",
        "# docs = docs_ls_ls"
      ],
      "metadata": {
        "id": "x9qdQ2lQvKZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Aa1mH88gft7"
      },
      "outputs": [],
      "source": [
        "# Remove rare and common tokens.\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Create a dictionary representation of the documents.\n",
        "dictionary = Dictionary(docs)\n",
        "\n",
        "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
        "dictionary.filter_extremes(no_below=Min_No_Documents, no_above=Max_Percent_Documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0YX0ewEgft7"
      },
      "source": [
        "Finally, we transform the documents to a vectorized form. We simply compute\n",
        "the frequency of each word, including the bigrams.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7WBqZ5Mgft7"
      },
      "outputs": [],
      "source": [
        "# Bag-of-words representation of the documents.\n",
        "corpus = [dictionary.doc2bow(doc) for doc in docs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eAsWXotgft8"
      },
      "source": [
        "Let's see how many tokens and documents we have to train on.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Corpus as a list of documents, each a list of tokens identifyed by dictionary tuples\n",
        "\n",
        "len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh9LzoeXhfip",
        "outputId": "b3ec172e-70d2-4999-a4d5-c186db25352f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "253"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(corpus[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpRsFMgAhgzn",
        "outputId": "9532f39b-47f5-452e-9038-83f80fa2fdfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42KjXcKPhtJF",
        "outputId": "61cf8c37-f6c5-4fd3-9a3d-821e742ed772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 1),\n",
              " (1, 1),\n",
              " (2, 1),\n",
              " (3, 1),\n",
              " (4, 2),\n",
              " (5, 1),\n",
              " (6, 2),\n",
              " (7, 1),\n",
              " (8, 1),\n",
              " (9, 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oZGyeXzgft8",
        "outputId": "b1c24094-323b-4183-ea16-d805b09bca70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tokens: 964\n",
            "Number of documents: 253\n"
          ]
        }
      ],
      "source": [
        "print('Number of unique tokens: %d' % len(dictionary))  # Orig 1864\n",
        "print('Number of documents: %d' % len(corpus))          # Orig 1740"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqaaUBeagft8"
      },
      "source": [
        "## Training\n",
        "\n",
        "We are ready to train the LDA model. We will first discuss how to set some of\n",
        "the training parameters.\n",
        "\n",
        "First of all, the elephant in the room: how many topics do I need? There is\n",
        "really no easy answer for this, it will depend on both your data and your\n",
        "application. I have used 10 topics here because I wanted to have a few topics\n",
        "that I could interpret and \"label\", and because that turned out to give me\n",
        "reasonably good results. You might not need to interpret all your topics, so\n",
        "you could use a large number of topics, for example 100.\n",
        "\n",
        "``chunksize`` controls how many documents are processed at a time in the\n",
        "training algorithm. Increasing chunksize will speed up training, at least as\n",
        "long as the chunk of documents easily fit into memory. I've set ``chunksize =\n",
        "2000``, which is more than the amount of documents, so I process all the\n",
        "data in one go. Chunksize can however influence the quality of the model, as\n",
        "discussed in Hoffman and co-authors [2], but the difference was not\n",
        "substantial in this case.\n",
        "\n",
        "``passes`` controls how often we train the model on the entire corpus.\n",
        "Another word for passes might be \"epochs\". ``iterations`` is somewhat\n",
        "technical, but essentially it controls how often we repeat a particular loop\n",
        "over each document. It is important to set the number of \"passes\" and\n",
        "\"iterations\" high enough.\n",
        "\n",
        "I suggest the following way to choose iterations and passes. First, enable\n",
        "logging (as described in many Gensim tutorials), and set ``eval_every = 1``\n",
        "in ``LdaModel``. When training the model look for a line in the log that\n",
        "looks something like this::\n",
        "\n",
        "   2016-06-21 15:40:06,753 - gensim.models.ldamodel - DEBUG - 68/1566 documents converged within 400 iterations\n",
        "\n",
        "If you set ``passes = 20`` you will see this line 20 times. Make sure that by\n",
        "the final passes, most of the documents have converged. So you want to choose\n",
        "both passes and iterations to be high enough for this to happen.\n",
        "\n",
        "We set ``alpha = 'auto'`` and ``eta = 'auto'``. Again this is somewhat\n",
        "technical, but essentially we are automatically learning two parameters in\n",
        "the model that we usually would have to specify explicitly.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown How many Topics do you want to find?\n",
        "\n",
        "No_of_Topics = 10 #@param {type:\"slider\", min:2, max:200, step:1}\n",
        "\n",
        "#@markdown Default 10-50 depending upon how large the text and diverse the vocabulary"
      ],
      "metadata": {
        "id": "evkoGUByoQFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0wOHHJCgft9",
        "outputId": "5bcd9d14-b030-4535-be58-43273b8112cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.39 s, sys: 46.2 ms, total: 3.44 s\n",
            "Wall time: 3.46 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 2m01s @20220328 with Google Colab/CPU on Harry Potter\n",
        "\n",
        "# Train LDA model.\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Set training parameters.\n",
        "num_topics = No_of_Topics\n",
        "chunksize = 2000\n",
        "passes = 20\n",
        "iterations = 400\n",
        "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
        "\n",
        "# Make a index to word dictionary.\n",
        "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
        "id2word = dictionary.id2token\n",
        "\n",
        "model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    chunksize=chunksize,\n",
        "    alpha='auto',\n",
        "    eta='auto',\n",
        "    iterations=iterations,\n",
        "    num_topics=num_topics,\n",
        "    passes=passes,\n",
        "    eval_every=eval_every\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4IDUbAygft9"
      },
      "source": [
        "We can compute the topic coherence of each topic. Below we display the\n",
        "average topic coherence and print the topics in order of topic coherence.\n",
        "\n",
        "Note that we use the \"Umass\" topic coherence measure here (see\n",
        ":py:func:`gensim.models.ldamodel.LdaModel.top_topics`), Gensim has recently\n",
        "obtained an implementation of the \"AKSW\" topic coherence measure (see\n",
        "accompanying blog post, http://rare-technologies.com/what-is-topic-coherence/).\n",
        "\n",
        "If you are familiar with the subject of the articles in this dataset, you can\n",
        "see that the topics below make a lot of sense. However, they are not without\n",
        "flaws. We can see that there is substantial overlap between some topics,\n",
        "others are hard to interpret, and most of them have at least some terms that\n",
        "seem out of place. If you were able to do better, feel free to share your\n",
        "methods on the blog at http://rare-technologies.com/lda-training-tips/ !\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Xa3_6XDbgft-",
        "outputId": "525029b6-5789-4d3f-b715-02c920875a2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Great Gatsby By F Scott Fitzgerald\n",
            "\n",
            "\n",
            "Topic #0: -0.9222677762928143 (coherence)\n",
            "  Word #0: (0.011974162, 'tom')\n",
            "  Word #1: (0.01158095, 'do')\n",
            "  Word #2: (0.010578074, 'we')\n",
            "  Word #3: (0.010524663, 'what')\n",
            "  Word #4: (0.0094726, 'did')\n",
            "  Word #5: (0.009149128, 'have')\n",
            "  Word #6: (0.008833581, 'if')\n",
            "  Word #7: (0.00844679, 'daisy')\n",
            "  Word #8: (0.0082881, 'this')\n",
            "  Word #9: (0.007726801, 'would')\n",
            "  Word #10: (0.0077033187, 'an')\n",
            "  Word #11: (0.007571487, 'one')\n",
            "  Word #12: (0.007053175, 'so')\n",
            "  Word #13: (0.006837032, 'are')\n",
            "  Word #14: (0.0067437934, 'know')\n",
            "  Word #15: (0.006615188, 'about')\n",
            "  Word #16: (0.0064966637, 'like')\n",
            "  Word #17: (0.006307618, 'were')\n",
            "  Word #18: (0.0060513145, 'man')\n",
            "  Word #19: (0.006042431, 'then')\n",
            "\n",
            "\n",
            "Topic #1: -0.972677527839028 (coherence)\n",
            "  Word #0: (0.011273947, 'daisy')\n",
            "  Word #1: (0.011114904, 'tom')\n",
            "  Word #2: (0.009186996, 'over')\n",
            "  Word #3: (0.008894915, 'were')\n",
            "  Word #4: (0.0085534705, 'an')\n",
            "  Word #5: (0.008040821, 'we')\n",
            "  Word #6: (0.0080245575, 'do')\n",
            "  Word #7: (0.007961585, 'this')\n",
            "  Word #8: (0.00786964, 'came')\n",
            "  Word #9: (0.0078311525, 'have')\n",
            "  Word #10: (0.007564133, 'then')\n",
            "  Word #11: (0.0073932973, 'into')\n",
            "  Word #12: (0.007164911, 'back')\n",
            "  Word #13: (0.0069737183, 'house')\n",
            "  Word #14: (0.0066047683, 'they')\n",
            "  Word #15: (0.0065572164, 'if')\n",
            "  Word #16: (0.0064509674, 'when')\n",
            "  Word #17: (0.006360126, 'so')\n",
            "  Word #18: (0.00593347, 'down')\n",
            "  Word #19: (0.005788296, 'looked')\n",
            "\n",
            "\n",
            "Topic #2: -0.9962172913640029 (coherence)\n",
            "  Word #0: (0.010971979, 'do')\n",
            "  Word #1: (0.010635709, 'have')\n",
            "  Word #2: (0.010055293, 'we')\n",
            "  Word #3: (0.009781625, 'then')\n",
            "  Word #4: (0.009302534, 'daisy')\n",
            "  Word #5: (0.009153076, 'they')\n",
            "  Word #6: (0.008103396, 'one')\n",
            "  Word #7: (0.008061894, 'about')\n",
            "  Word #8: (0.006898805, 'an')\n",
            "  Word #9: (0.006793351, 'into')\n",
            "  Word #10: (0.0067618317, 'down')\n",
            "  Word #11: (0.00644013, 'them')\n",
            "  Word #12: (0.006334503, 'over')\n",
            "  Word #13: (0.006332603, 'do_not')\n",
            "  Word #14: (0.0062169116, 'would')\n",
            "  Word #15: (0.00608247, 'house')\n",
            "  Word #16: (0.005982864, 'tom')\n",
            "  Word #17: (0.0058853556, 'if')\n",
            "  Word #18: (0.005884992, 'mr')\n",
            "  Word #19: (0.005703762, 'were')\n",
            "\n",
            "\n",
            "Topic #3: -1.0090073827651298 (coherence)\n",
            "  Word #0: (0.012912054, 'have')\n",
            "  Word #1: (0.010856331, 'over')\n",
            "  Word #2: (0.009730042, 'this')\n",
            "  Word #3: (0.009279749, 'we')\n",
            "  Word #4: (0.009185027, 'tom')\n",
            "  Word #5: (0.009095958, 'would')\n",
            "  Word #6: (0.00887147, 'daisy')\n",
            "  Word #7: (0.008571523, 'they')\n",
            "  Word #8: (0.00819187, 'an')\n",
            "  Word #9: (0.007887647, 'do')\n",
            "  Word #10: (0.0075293323, 'into')\n",
            "  Word #11: (0.0075082188, 'one')\n",
            "  Word #12: (0.0071323463, 'be')\n",
            "  Word #13: (0.006988716, 'then')\n",
            "  Word #14: (0.0069349376, 'about')\n",
            "  Word #15: (0.00684904, 'mr')\n",
            "  Word #16: (0.006573375, 'so')\n",
            "  Word #17: (0.0065266583, 'are')\n",
            "  Word #18: (0.006364663, 'some')\n",
            "  Word #19: (0.0062686256, 'by')\n",
            "\n",
            "\n",
            "Topic #4: -1.0405110926832957 (coherence)\n",
            "  Word #0: (0.010959179, 'have')\n",
            "  Word #1: (0.010686952, 'we')\n",
            "  Word #2: (0.01002902, 'one')\n",
            "  Word #3: (0.010008858, 'would')\n",
            "  Word #4: (0.008906542, 'about')\n",
            "  Word #5: (0.008533563, 'they')\n",
            "  Word #6: (0.008211216, 'daisy')\n",
            "  Word #7: (0.00810448, 'when')\n",
            "  Word #8: (0.007837167, 'them')\n",
            "  Word #9: (0.0077766413, 'into')\n",
            "  Word #10: (0.0074106, 'then')\n",
            "  Word #11: (0.007131722, 'this')\n",
            "  Word #12: (0.0069894777, 'do')\n",
            "  Word #13: (0.0069156415, 'by')\n",
            "  Word #14: (0.0067760353, 'mr')\n",
            "  Word #15: (0.0067202505, 'if')\n",
            "  Word #16: (0.0066515873, 'did')\n",
            "  Word #17: (0.006519588, 'will')\n",
            "  Word #18: (0.0064542275, 'no')\n",
            "  Word #19: (0.00638478, 'back')\n",
            "\n",
            "\n",
            "Topic #5: -1.0459167400088012 (coherence)\n",
            "  Word #0: (0.0118022375, 'were')\n",
            "  Word #1: (0.010143287, 'we')\n",
            "  Word #2: (0.00891389, 'an')\n",
            "  Word #3: (0.008202907, 'into')\n",
            "  Word #4: (0.007938317, 'house')\n",
            "  Word #5: (0.007652498, 'hand')\n",
            "  Word #6: (0.0075975363, 'when')\n",
            "  Word #7: (0.007224903, 'they')\n",
            "  Word #8: (0.00679407, 'daisy')\n",
            "  Word #9: (0.006390884, 'one')\n",
            "  Word #10: (0.006385402, 'have')\n",
            "  Word #11: (0.0062508294, 'or')\n",
            "  Word #12: (0.006133932, 'so')\n",
            "  Word #13: (0.005834696, 'who')\n",
            "  Word #14: (0.005642269, 'down')\n",
            "  Word #15: (0.0055979025, 'went')\n",
            "  Word #16: (0.005515855, 'over')\n",
            "  Word #17: (0.0055093695, 'by')\n",
            "  Word #18: (0.0054279314, 'then')\n",
            "  Word #19: (0.005137121, 'them')\n",
            "\n",
            "\n",
            "Topic #6: -1.049019729522785 (coherence)\n",
            "  Word #0: (0.010960807, 'over')\n",
            "  Word #1: (0.010941651, 'this')\n",
            "  Word #2: (0.009124218, 'into')\n",
            "  Word #3: (0.0088608125, 'so')\n",
            "  Word #4: (0.008781626, 'we')\n",
            "  Word #5: (0.008590994, 'an')\n",
            "  Word #6: (0.00834553, 'when')\n",
            "  Word #7: (0.007538229, 'about')\n",
            "  Word #8: (0.0072892243, 'have')\n",
            "  Word #9: (0.0068390113, 'house')\n",
            "  Word #10: (0.006640964, 'one')\n",
            "  Word #11: (0.0062535303, 'or')\n",
            "  Word #12: (0.005955441, 'daisy')\n",
            "  Word #13: (0.0059535243, 'were')\n",
            "  Word #14: (0.005864747, 'if')\n",
            "  Word #15: (0.005831878, 'like')\n",
            "  Word #16: (0.005725103, 'time')\n",
            "  Word #17: (0.0056250966, 'too')\n",
            "  Word #18: (0.005472187, 'what')\n",
            "  Word #19: (0.0054073627, 'be')\n",
            "\n",
            "\n",
            "Topic #7: -1.0491890142746783 (coherence)\n",
            "  Word #0: (0.012935473, 'have')\n",
            "  Word #1: (0.011237028, 'one')\n",
            "  Word #2: (0.009977082, 'an')\n",
            "  Word #3: (0.0091157025, 'we')\n",
            "  Word #4: (0.008918277, 'daisy')\n",
            "  Word #5: (0.008867941, 'they')\n",
            "  Word #6: (0.008314847, 'this')\n",
            "  Word #7: (0.008161144, 'be')\n",
            "  Word #8: (0.007824049, 'into')\n",
            "  Word #9: (0.0076927003, 'about')\n",
            "  Word #10: (0.007501728, 'by')\n",
            "  Word #11: (0.0074430024, 'so')\n",
            "  Word #12: (0.007383864, 'them')\n",
            "  Word #13: (0.0069855577, 'when')\n",
            "  Word #14: (0.0067244256, 'over')\n",
            "  Word #15: (0.006299121, 'like')\n",
            "  Word #16: (0.00557628, 'who')\n",
            "  Word #17: (0.0055648354, 'are')\n",
            "  Word #18: (0.005466922, 'will')\n",
            "  Word #19: (0.0054102163, 'what')\n",
            "\n",
            "\n",
            "Topic #8: -1.0735467702562649 (coherence)\n",
            "  Word #0: (0.013810688, 'they')\n",
            "  Word #1: (0.011574927, 'were')\n",
            "  Word #2: (0.011398477, 'would')\n",
            "  Word #3: (0.009877144, 'we')\n",
            "  Word #4: (0.009862897, 'one')\n",
            "  Word #5: (0.009357402, 'who')\n",
            "  Word #6: (0.008483425, 'have')\n",
            "  Word #7: (0.0076461015, 'will')\n",
            "  Word #8: (0.007399881, 'this')\n",
            "  Word #9: (0.0073382366, 'man')\n",
            "  Word #10: (0.007269899, 'into')\n",
            "  Word #11: (0.007213714, 'daisy')\n",
            "  Word #12: (0.0071947384, 'so')\n",
            "  Word #13: (0.006811897, 'been')\n",
            "  Word #14: (0.006807691, 'time')\n",
            "  Word #15: (0.006329872, 'night')\n",
            "  Word #16: (0.006312701, 'came')\n",
            "  Word #17: (0.006027503, 'if')\n",
            "  Word #18: (0.0058361106, 'them')\n",
            "  Word #19: (0.005673676, 'over')\n",
            "\n",
            "\n",
            "Topic #9: -1.0862637585264865 (coherence)\n",
            "  Word #0: (0.015102727, 'we')\n",
            "  Word #1: (0.010312789, 'daisy')\n",
            "  Word #2: (0.009699045, 'by')\n",
            "  Word #3: (0.008999923, 'then')\n",
            "  Word #4: (0.008017405, 'into')\n",
            "  Word #5: (0.007386738, 'room')\n",
            "  Word #6: (0.0069463965, 'over')\n",
            "  Word #7: (0.0066715656, 'one')\n",
            "  Word #8: (0.006570996, 'this')\n",
            "  Word #9: (0.0065349243, 'when')\n",
            "  Word #10: (0.006441818, 'an')\n",
            "  Word #11: (0.0063539785, 'have')\n",
            "  Word #12: (0.006314015, 'through')\n",
            "  Word #13: (0.0061942926, 'tom')\n",
            "  Word #14: (0.005886671, 'before')\n",
            "  Word #15: (0.005780227, 'would')\n",
            "  Word #16: (0.0057716556, 'house')\n",
            "  Word #17: (0.00565505, 'like')\n",
            "  Word #18: (0.0055955397, 'back')\n",
            "  Word #19: (0.0055805733, 'now')\n",
            "\n",
            "\n",
            "Average topic coherence: -1.0245\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7159"
            ]
          },
          "metadata": {},
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5e466aab-ae38-4aa7-bd11-7d797cb5a3da\", \"lda_topic_word_report.txt\", 7159)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "top_topics = model.top_topics(corpus) #, num_words=20)\n",
        "\n",
        "\n",
        "# from pprint import pprint\n",
        "# pprint(top_topics)\n",
        "\n",
        "# print('\\n\\n')\n",
        "# for i,atopic in enumerate(top_topics):\n",
        "#   atopic_coherence_str = f'Topic #{i}: coherence = {atopic[1]}'\n",
        "#   print(atopic_coherence_str)\n",
        "#   report_line_ls.append(atopic_coherence_str)\n",
        "\n",
        "\n",
        "topic_word_ls = []\n",
        "\n",
        "topic_word_ls.append(text_title_str)\n",
        "\n",
        "# print(text_title_str)\n",
        "\n",
        "for i, atopic in enumerate(top_topics):\n",
        "  topic_str = f'\\n\\nTopic #{i}: {atopic[1]} (coherence)'\n",
        "  # print(topic_str)\n",
        "  topic_word_ls.append(topic_str)\n",
        "  for j, aword in enumerate(atopic[0]):\n",
        "    word_str = f'  Word #{j}: {aword}'\n",
        "    # print(word_str)\n",
        "    topic_word_ls.append(word_str)\n",
        "\n",
        "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
        "avg_topic_coherence_str = f'Average topic coherence: {avg_topic_coherence:.4f}'\n",
        "# print(avg_topic_coherence_str)\n",
        "topic_word_ls.append(f'\\n\\n{avg_topic_coherence_str}')\n",
        "\n",
        "topic_word_str = \"\\n\".join(topic_word_ls)\n",
        "print(topic_word_str)\n",
        "\n",
        "# Save to file\n",
        "filename_topic_word_str = f'lda_topic_word_report.txt'\n",
        "with open(filename_topic_word_str, 'w') as fp:\n",
        "  fp.write(topic_word_str)\n",
        "\n",
        "files.download(filename_topic_word_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize"
      ],
      "metadata": {
        "id": "1TXD97_dQTnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis"
      ],
      "metadata": {
        "id": "GHAcDkpbQdJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d9471d-744f-4592-a36c-760c37c3840c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
            "  from collections import Iterable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vis_data = gensimvis.prepare(lda, corpus, dictionary)\n",
        "vis_data = gensimvis.prepare(model, corpus, dictionary)\n",
        "pyLDAvis.display(vis_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        },
        "id": "GgLhaMRiQdIH",
        "outputId": "5cae3627-a8d2-47f4-bea4-314b36985a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el681396381909907201207654260\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el681396381909907201207654260_data = {\"mdsDat\": {\"x\": [0.03868886390489097, -0.04298824105849848, 0.023131999134889034, -0.0022078963871209143, -0.005998741834520036, 0.008288410989076435, -0.002720926775944462, -0.010112144237278032, 0.002217714636785111, -0.008299038372279525], \"y\": [-0.002634122895301342, -0.00029792344310638157, 0.012852510422032193, -0.027117842849995952, 0.039449691201395326, -0.010345194128429155, -0.004910242859762418, -0.00948290284251081, -0.0032615724606365314, 0.0057475998563150344], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [39.6133575903163, 14.884191381251382, 7.772455290527929, 6.493680609443615, 6.24145024750374, 6.232193224010946, 5.621693354596166, 4.869377112674556, 4.4707490566951735, 3.8008521329801965]}, \"tinfo\": {\"Term\": [\"have\", \"we\", \"this\", \"they\", \"daisy\", \"over\", \"one\", \"an\", \"into\", \"about\", \"would\", \"then\", \"by\", \"were\", \"do\", \"so\", \"when\", \"tom\", \"be\", \"house\", \"them\", \"who\", \"will\", \"back\", \"came\", \"room\", \"like\", \"man\", \"or\", \"mr\", \"alive\", \"absolutely\", \"listening\", \"show\", \"help\", \"garage\", \"politely\", \"answered\", \"enough\", \"shaking\", \"george\", \"fellow\", \"stay\", \"office\", \"stand\", \"sick\", \"object\", \"control\", \"nick\", \"startled\", \"wrong\", \"stared\", \"subject\", \"included\", \"france\", \"michaelis\", \"appeared\", \"effort\", \"ask\", \"gathered\", \"wolfshiem\", \"wife\", \"wilson\", \"friend\", \"what\", \"why\", \"did\", \"tom\", \"know\", \"do\", \"want\", \"are\", \"if\", \"get\", \"your\", \"you_are\", \"here\", \"any\", \"want_to\", \"look\", \"way\", \"do_not\", \"this\", \"did_not\", \"we\", \"like\", \"just\", \"have\", \"would\", \"so\", \"daisy\", \"man\", \"an\", \"one\", \"about\", \"some\", \"been\", \"then\", \"mr\", \"will\", \"were\", \"they\", \"into\", \"when\", \"be\", \"gatz\", \"box\", \"blew\", \"pool\", \"stiff\", \"wind\", \"beauty\", \"water\", \"caught\", \"stretched\", \"beyond\", \"number\", \"forty\", \"song\", \"thin\", \"muttered\", \"soon\", \"continually\", \"touched\", \"shadow\", \"summer\", \"pair\", \"husky\", \"moonlight\", \"rich\", \"deep\", \"waiting\", \"champagne\", \"hall\", \"watched\", \"dark\", \"perhaps\", \"egg\", \"white\", \"own\", \"were\", \"house\", \"hand\", \"west\", \"woman\", \"most\", \"lawn\", \"there_wa\", \"or\", \"an\", \"toward\", \"when\", \"into\", \"we\", \"went\", \"light\", \"while\", \"them\", \"they\", \"who\", \"day\", \"night\", \"young\", \"down\", \"so\", \"daisy\", \"little\", \"one\", \"by\", \"have\", \"over\", \"made\", \"then\", \"old\", \"just\", \"like\", \"would\", \"came\", \"tom\", \"newspaper\", \"myrtle\", \"play\", \"grew\", \"crazy\", \"klipspringer\", \"letter\", \"stranger\", \"should\", \"twelve\", \"interrupted\", \"somebody\", \"murmured\", \"information\", \"married\", \"changing\", \"distance\", \"catherine\", \"drew\", \"shock\", \"number\", \"beneath\", \"wire\", \"afraid\", \"hardly\", \"corner\", \"coming\", \"laughed\", \"backward\", \"phone\", \"try\", \"god\", \"them\", \"believe\", \"question\", \"girl\", \"would\", \"alone\", \"about\", \"one\", \"more\", \"get\", \"have\", \"no\", \"long\", \"when\", \"they\", \"we\", \"by\", \"back\", \"mr\", \"will\", \"then\", \"looked\", \"into\", \"room\", \"know\", \"daisy\", \"went\", \"did\", \"this\", \"if\", \"do\", \"got\", \"an\", \"over\", \"were\", \"be\", \"singing\", \"lap\", \"buy\", \"fresh\", \"romantic\", \"glowing\", \"sad\", \"servant\", \"dog\", \"song\", \"future\", \"innocently\", \"thrilling\", \"movie\", \"wonder\", \"suggestion\", \"butler\", \"accident\", \"miss_baker\", \"rose\", \"afterward\", \"drug\", \"store\", \"beneath\", \"worried\", \"silver\", \"gay\", \"star\", \"park\", \"flower\", \"hear\", \"baker\", \"already\", \"miss\", \"almost\", \"voice\", \"white\", \"came\", \"tree\", \"daisy\", \"over\", \"tom\", \"back\", \"house\", \"were\", \"looked\", \"light\", \"tell\", \"an\", \"then\", \"face\", \"this\", \"into\", \"do\", \"do_not\", \"down\", \"have\", \"when\", \"who\", \"we\", \"if\", \"will\", \"so\", \"they\", \"mr\", \"about\", \"what\", \"be\", \"one\", \"come\", \"man\", \"drank\", \"cody\", \"american\", \"dan\", \"blind\", \"invited\", \"during\", \"bell\", \"goodbye\", \"promise\", \"early\", \"soon\", \"guessed\", \"photograph\", \"spot\", \"trust\", \"veranda\", \"ride\", \"drawn\", \"gorgeous\", \"bed\", \"month\", \"three\", \"nine\", \"killed\", \"turn\", \"somewhere\", \"automobile\", \"second\", \"brown\", \"read\", \"lawn\", \"they\", \"who\", \"were\", \"would\", \"found\", \"night\", \"will\", \"one\", \"time\", \"been\", \"man\", \"those\", \"them\", \"first\", \"came\", \"we\", \"so\", \"have\", \"or\", \"into\", \"this\", \"around\", \"before\", \"daisy\", \"by\", \"back\", \"if\", \"be\", \"room\", \"over\", \"mr\", \"about\", \"an\", \"am\", \"tom\", \"apartment\", \"terrible\", \"solemn\", \"glowing\", \"copy\", \"continually\", \"straw\", \"second\", \"stranger\", \"magazine\", \"living\", \"newspaper\", \"distance\", \"tea\", \"scene\", \"given\", \"wheel\", \"eat\", \"request\", \"herself\", \"happened\", \"sharply\", \"nineteen\", \"bottle\", \"park\", \"dog\", \"station\", \"line\", \"small\", \"urged\", \"buchanan\", \"afternoon\", \"anyhow\", \"train\", \"sister\", \"over\", \"have\", \"room\", \"lawn\", \"this\", \"would\", \"rather\", \"be\", \"some\", \"they\", \"no\", \"mr\", \"tom\", \"are\", \"daisy\", \"an\", \"toward\", \"very\", \"into\", \"we\", \"then\", \"about\", \"by\", \"do\", \"one\", \"so\", \"man\", \"back\", \"just\", \"house\", \"old\", \"been\", \"do_not\", \"will\", \"were\", \"rain\", \"watch\", \"interrupted\", \"gate\", \"owl\", \"attention\", \"thick\", \"fine\", \"servant\", \"nobody\", \"live\", \"curiosity\", \"sunlight\", \"murmur\", \"strange\", \"use\", \"chauffeur\", \"throwing\", \"scene\", \"mckee\", \"wet\", \"grass\", \"beginning\", \"kitchen\", \"whenever\", \"drop\", \"scarcely\", \"copy\", \"damp\", \"glass\", \"story\", \"dozen\", \"good\", \"coming\", \"then\", \"sound\", \"them\", \"do\", \"have\", \"they\", \"their\", \"took\", \"night\", \"about\", \"daisy\", \"down\", \"do_not\", \"we\", \"voice\", \"one\", \"house\", \"began\", \"before\", \"go\", \"could\", \"into\", \"mr\", \"over\", \"an\", \"or\", \"looked\", \"would\", \"like\", \"two\", \"if\", \"u\", \"tom\", \"were\", \"what\", \"by\", \"fresh\", \"funny\", \"gay\", \"beach\", \"dance\", \"horn\", \"cheerful\", \"shining\", \"itwas\", \"ten\", \"pocket\", \"murmur\", \"suggestion\", \"silent\", \"drawn\", \"part\", \"given\", \"picture\", \"able\", \"fromthe\", \"bridge\", \"itself\", \"autumn\", \"bottle\", \"clothes\", \"bright\", \"bedroom\", \"living\", \"feel\", \"june\", \"aware\", \"run\", \"opened\", \"room\", \"through\", \"by\", \"we\", \"then\", \"daisy\", \"before\", \"light\", \"into\", \"year\", \"until\", \"over\", \"house\", \"now\", \"window\", \"when\", \"only\", \"back\", \"very\", \"this\", \"one\", \"like\", \"came\", \"an\", \"see\", \"have\", \"tom\", \"would\", \"door\", \"man\", \"about\", \"did\", \"were\", \"do\", \"down\", \"grotesque\", \"ash\", \"vanished\", \"bell\", \"weather\", \"ihad\", \"form\", \"whose\", \"jumped\", \"sad\", \"hot\", \"memory\", \"holding\", \"silence\", \"dust\", \"host\", \"noon\", \"understood\", \"horn\", \"filled\", \"point\", \"sleep\", \"ghostly\", \"kissed\", \"moonlight\", \"receiver\", \"set\", \"suggestion\", \"whole\", \"casual\", \"chapter\", \"far\", \"taxi\", \"every\", \"keep\", \"being\", \"over\", \"too\", \"first\", \"this\", \"against\", \"house\", \"so\", \"when\", \"into\", \"or\", \"an\", \"about\", \"time\", \"we\", \"am\", \"jordan\", \"see\", \"your\", \"like\", \"have\", \"one\", \"voice\", \"came\", \"be\", \"if\", \"looked\", \"could\", \"just\", \"were\", \"man\", \"daisy\", \"what\", \"then\", \"would\", \"did\", \"they\", \"stable\", \"shadow\", \"understood\", \"pair\", \"blind\", \"gentleman\", \"various\", \"shoe\", \"ash\", \"vanished\", \"certain\", \"kitchen\", \"physical\", \"bay\", \"fact\", \"smile\", \"itself\", \"best\", \"many\", \"however\", \"yard\", \"drank\", \"object\", \"shining\", \"college\", \"itwas\", \"thirty\", \"overhead\", \"known\", \"should\", \"quality\", \"life\", \"among\", \"them\", \"their\", \"have\", \"one\", \"year\", \"be\", \"an\", \"against\", \"by\", \"they\", \"two\", \"about\", \"this\", \"daisy\", \"into\", \"so\", \"when\", \"we\", \"over\", \"like\", \"again\", \"who\", \"are\", \"will\", \"now\", \"u\", \"eye\", \"what\", \"down\", \"do\", \"am\", \"tom\", \"back\", \"man\"], \"Freq\": [185.0, 212.0, 151.0, 148.0, 172.0, 129.0, 157.0, 159.0, 145.0, 130.0, 147.0, 131.0, 109.0, 150.0, 171.0, 133.0, 127.0, 173.0, 107.0, 93.0, 77.0, 100.0, 101.0, 96.0, 97.0, 73.0, 114.0, 107.0, 88.0, 107.0, 4.544374750360455, 6.399509884277845, 4.254955261132381, 6.403140132265519, 5.531665813274503, 14.242477462197359, 5.488075749565981, 12.687402592830994, 7.737707494396822, 4.591065185566762, 9.540111125897626, 5.397126608300236, 4.606896849588411, 11.557741083080016, 9.167888811807092, 7.382577142490433, 5.516701911109838, 3.698066713285823, 17.551545211361905, 3.7134048555722705, 5.424944411181042, 9.102927379823447, 3.709425792029818, 4.646991629365876, 3.711248044406927, 13.656307654384245, 3.727918243914143, 8.883171679410808, 10.286402480571704, 3.728665773706604, 20.192824114699587, 16.540730958875677, 43.486977781896634, 15.409653276981322, 85.92664208805199, 26.824714541390403, 77.33727100222825, 97.76080761765515, 55.058438626647565, 94.55049553989033, 39.48137004104133, 55.81966632040432, 72.1201156368483, 41.48958578004911, 35.60757846275038, 21.986454607294714, 39.16853476321571, 37.738155276750476, 23.48462026642153, 33.18263553675696, 40.43634909303432, 46.792407204239126, 67.6666439308751, 40.00727744712373, 86.36270145047139, 53.04079441269863, 44.36062483210897, 74.69633996399425, 63.08402112723019, 57.584327063381885, 68.9622380455304, 49.404824709384975, 62.892303734354236, 61.8159900945601, 54.00846388781807, 42.335943478193876, 42.71117397002521, 49.332297686621935, 45.206157538696374, 44.190281183635896, 51.49736827204988, 48.806618388804246, 48.467603885799285, 46.63402023064103, 43.38933223738513, 6.11919060705633, 3.8549436420889602, 3.8740066724538775, 6.862200963635844, 3.8410463429759827, 6.97260726452676, 3.9039575572542686, 3.8689816462218665, 3.906555243332845, 3.0068019943884856, 2.9254894918821535, 3.13559815568857, 3.169328434463332, 2.890935874469206, 3.9425864420147105, 3.1576913436527065, 2.9068166143960985, 2.3851784656040707, 2.41776435545406, 3.175008774684096, 8.323161898474021, 3.2222770905158766, 2.41776399833472, 2.298047774811766, 2.36141199497274, 4.612843375307749, 2.4351083918762737, 3.2061895784911707, 6.378815652939322, 2.2867779812422477, 9.329352781084769, 7.054976840279254, 9.543640098287586, 14.403777188670897, 9.969217072953935, 36.204918670530446, 24.351833512257226, 23.47504553670509, 11.564951275484434, 10.966079998405977, 9.56373663202223, 7.9569267302834605, 15.049093264383279, 19.17524297542949, 27.344533578150962, 12.401680460439092, 23.306443782419525, 25.163497204981596, 31.115868083258565, 17.172303445555475, 11.948285317670353, 12.046436711292582, 15.758795100128477, 22.163341915747697, 17.898698467725882, 13.475519745372162, 14.83344032282461, 10.799888799664874, 17.30840305447626, 18.81664516144245, 20.8417046632311, 15.699301875052562, 19.60487896851337, 16.90071704450121, 19.588061504557864, 16.92061144868946, 12.622743045758869, 16.650894924499017, 13.979762253997357, 13.898154770916252, 14.355906055393332, 15.046432011062198, 13.82859506588668, 13.827663698648168, 1.9802623888634137, 7.245027916023927, 3.9326566749373777, 2.0352451907881592, 3.7695500054226505, 2.11427033412489, 2.622967735465055, 1.3822753720998635, 2.6635620194838947, 1.429160203658778, 1.3930167822670867, 3.371681265385299, 1.4316355262833833, 1.3656149878531845, 5.540476764660852, 1.3160230315057753, 1.430655355588772, 3.4003575987284864, 2.1847868648070654, 1.332516137762906, 1.3508435208374259, 1.3743303202974992, 2.942967172722221, 2.1491516179098213, 2.1561332827623447, 2.098371652161723, 2.1617207778825787, 2.657477352546341, 1.4006915225356156, 2.711837850512198, 2.0758295910461895, 4.553419140111801, 12.554381326028453, 2.9422758689237196, 2.5874262910524743, 8.59780818776487, 16.03321896936073, 5.236551492742734, 14.267416846859192, 16.065516861853336, 8.145332601685638, 9.151848553977677, 17.555541917277818, 10.339046367364157, 7.665594246545021, 12.982590600946038, 13.669940895123911, 17.11945994714644, 11.078186822239275, 10.22779848541418, 10.854551255583335, 10.44374784290933, 11.871062114372407, 8.900461624801473, 12.457424989236818, 8.421712391686011, 9.477177949115873, 13.153571457562986, 8.427362919092163, 10.655197664917496, 11.424326569004768, 10.765189376814408, 11.196465228387662, 8.222475914261462, 9.8939399641556, 9.280830513624528, 8.948143870723051, 8.438555811993718, 2.0420531413607286, 1.8345523822792744, 1.894399408740424, 1.2324563990643937, 2.3222553933058885, 1.278883935337366, 1.8467490435190312, 1.8643200134479398, 5.242967028848885, 1.4190047705361644, 1.323760129590918, 1.2646843337209237, 1.2646370471584154, 1.2670035565420625, 2.0229365941663136, 1.275646480700273, 3.9310255796059783, 1.1854373522986297, 5.2417941351336905, 4.589092181785456, 1.3750061432128509, 2.120082357469797, 2.6889965729616727, 1.275692131319103, 1.17471280666302, 1.9727724846302352, 1.3314418978871414, 1.8007536300833458, 1.3991313267332983, 2.029467281329718, 4.12861703831813, 6.213706093098587, 2.6834521261299877, 5.590250761808548, 3.194046663339521, 7.353639749889952, 5.485678749030961, 10.532313839995755, 2.8743720507182484, 15.08845873931875, 12.295393712624058, 14.875604081893082, 9.589140894136332, 9.333258736715566, 11.90448730777875, 7.7467517184218035, 5.459236913458012, 4.682946426408385, 11.447516199865985, 10.123438853005654, 6.18110999321238, 10.655367965428933, 9.894801234189279, 10.739646797997073, 7.651177707482678, 7.941045114678689, 10.480803738322912, 8.633636360634739, 7.710716085903205, 10.761413262341865, 8.775834454794461, 7.659669657410014, 8.512059258636743, 8.839475468351614, 7.474275171545939, 7.332853605318806, 7.104604250468725, 6.801044773706916, 6.981904735303319, 6.399223352640607, 6.4123065395497925, 1.832061377802168, 4.739427569860254, 1.8164949208362744, 2.446895246059535, 1.8055667433139932, 2.127188061936437, 2.4286507678722513, 1.2668398630963134, 2.0749168554639503, 1.8505448607352626, 1.2550461170083758, 1.4155581701396809, 1.303516758666365, 1.316682985014552, 1.2647590542347567, 1.2742518555110485, 1.2266337539645267, 1.3272516071516876, 1.3147862217874577, 1.2652301751184167, 3.1804178827211746, 3.1640208992388223, 6.824986575863717, 1.3364591302069457, 2.442764627072251, 1.892275448759503, 2.369512068518719, 1.3323613069836542, 1.934611485522662, 1.3121649567055802, 2.0157199024474353, 4.161159181426771, 17.765555805354325, 12.037014582572432, 14.889555991136998, 14.662576957799399, 3.9565225992520934, 8.14251251683953, 9.835661071775155, 12.687264431638116, 8.757160841938964, 8.762571094731358, 9.439635000041367, 3.6232885851292336, 7.50735590732322, 5.5281725824966985, 8.120424045657117, 12.705591722873292, 9.255044267036027, 10.912762614690372, 7.264055665190849, 9.351727976315168, 9.518931862824488, 6.019041008650922, 6.448351636038186, 9.27945389959693, 7.289067901196077, 6.880811255778153, 7.753555953902887, 7.066687462930134, 6.028166317667678, 7.298405858545571, 6.838736836365202, 7.226845801017827, 7.093709974441482, 6.26215675589179, 6.369277005738011, 2.4798018974789384, 2.422147073708276, 1.8503556920413484, 1.256431143101593, 1.876057996646359, 1.2202676280425118, 1.2905662056708453, 2.4718972803273522, 1.2517213890879446, 1.290388264683569, 1.9329255436960044, 1.2461269394010015, 1.301190777913655, 2.649504217044353, 1.8809869619939128, 1.2948937603874535, 1.9717709583998708, 1.9388073157418142, 1.301190777913655, 1.3534270743263914, 3.03011520386065, 1.2764918212284964, 1.2578625961025982, 1.3336198499586758, 1.3848375463669362, 3.8310236998050344, 1.949733789542028, 1.903483487753363, 5.453516836294345, 1.256167222225507, 3.912631324356226, 5.804231643824843, 2.2790095491803175, 2.5227124413257407, 1.9677345390632854, 13.94446897988726, 16.584951738406946, 7.8527612601075045, 3.3778086033332446, 12.497800977758324, 11.683349228789075, 3.3751200495927165, 9.16117778787675, 8.175123339504024, 11.009735827355884, 7.953637949687582, 8.797283384868493, 11.797754218369876, 8.383198857270935, 11.39500348495013, 10.522087804073955, 5.139066701031242, 5.425756846037309, 9.671088471620566, 11.919420396473482, 8.976690964752537, 8.907615170673322, 8.0517674696725, 10.131327003531322, 9.643969068917725, 8.4432041469063, 7.32618931874497, 6.995852492513645, 6.750697661724144, 6.817542464601541, 6.341179787699207, 6.750646821442065, 6.65063022981057, 6.679789225006225, 7.072565693203048, 6.218657984710939, 2.563832204635571, 1.7442810916820402, 2.2849709405158287, 1.7284190566052215, 1.6963433480410093, 1.7365436978254034, 1.9783361767861183, 1.7576826020616134, 2.3061968438984985, 3.2409806867802393, 1.1920985881596886, 1.238503258469457, 1.8311009794355995, 1.1976275525202178, 1.6799775272091084, 1.861129847616044, 1.2171868489979891, 1.7728754855638769, 3.0092060583725937, 1.6807640264695862, 2.3798615165305246, 1.2992660251226662, 1.3352108352623897, 1.297414089803448, 0.9788296223808653, 1.236656583563746, 1.2471960243483824, 1.2117254607186638, 1.9454405181268202, 2.0577560115594693, 2.6482942131964684, 3.976975931120033, 1.6702495389285084, 11.333293833431906, 3.7366408564655202, 7.461734161106649, 12.712474981491956, 12.322862320222278, 10.605037871302315, 5.036892871914732, 4.541931123284289, 6.499954145872381, 9.340760340120301, 10.778204209593815, 7.834467639139114, 7.337149764787178, 11.650374294679912, 5.4539794589892825, 9.388845375339525, 7.047338208099672, 4.813633369268357, 6.002479807939213, 5.9511915855414905, 6.0729073030285425, 7.870986778527871, 6.818533959852245, 7.339351585045802, 7.993169189110661, 6.218950409748963, 5.947132704432539, 7.203106335687945, 6.5561385993034325, 5.408564016183603, 6.818955332462386, 5.394234649790854, 6.931931749643336, 6.608555517604375, 5.784656065916616, 5.538504322747467, 1.5836937874362815, 2.0845745674470137, 1.6699474099922402, 1.6568143332338205, 2.266570901557409, 1.1581762588672002, 1.6958281349864748, 1.1581764925310514, 1.1312917139680787, 1.1722005295542137, 1.7332456621394468, 1.7077431220884292, 1.1911374658814264, 1.6397016105931657, 1.1653582678305516, 2.21267726892646, 1.1779824247209911, 2.575156963710531, 1.118685315530333, 1.2198491452652462, 1.1907793760294154, 2.7560555450478446, 1.1621014610720986, 1.198393312621767, 1.2495850901409356, 2.120579129291594, 1.2757980851316202, 1.248467592772425, 1.705046407581376, 1.166488967206657, 1.360550066947645, 1.7715778664380613, 2.2461201739699983, 7.413172144046267, 6.336610345632692, 9.73375374382409, 15.156773948050253, 9.032130363078355, 10.34969445963435, 5.907737170496087, 4.249861302386125, 8.046096015891674, 4.429146902158394, 3.692183265897126, 6.971254918078561, 5.792310031954351, 5.60054397856015, 3.3824594944523616, 6.558310140352648, 3.985624872860435, 5.615563890917258, 4.110395294157481, 6.594510746168411, 6.695440444724391, 5.67528696930871, 5.2847379370678516, 6.464870770211719, 4.694372197102492, 6.376716876385626, 6.216459458674477, 5.800912132973557, 4.2688992983289955, 5.044907229475503, 5.316186287477285, 5.2712036594768685, 5.203943987227823, 5.296588432946867, 4.294535961430619, 1.5427403414143857, 1.6296199886222216, 1.6213619952051095, 1.07453705437277, 1.0696198019699144, 1.1029175846620374, 1.1568380928774036, 1.7207219016855404, 1.1535298539350494, 1.1535312484133817, 3.8587210635148868, 1.1714398900314515, 1.0644831873986107, 1.630189686654803, 1.1696876743729658, 1.130672315982734, 1.2292947579604352, 0.6297753771696314, 0.6286044444406427, 1.1537531850033669, 1.1503814436631046, 1.2039999936857169, 0.6290279904172287, 0.6286044444406427, 0.6040211322843759, 0.6475365258871554, 1.8629456049510298, 0.650726985044201, 1.6517888688134423, 0.6771755192376627, 1.3026621246621104, 1.9056949472452644, 1.205563632966632, 3.272401063745958, 1.2688638315162075, 1.3015956705404903, 10.099521765693046, 5.183084167253904, 4.33470671959469, 10.081870673496221, 2.79129552683484, 6.301611113298955, 8.164541915326149, 7.689749932698908, 8.407248938270191, 5.762136360489313, 7.915925736814634, 6.945885418028727, 5.275232153601677, 8.091577660130111, 4.8968496831061294, 4.2595490572492976, 4.574764233707107, 3.9479801198281117, 5.373617105192608, 6.716446963685361, 6.119126246154551, 3.716556786479872, 4.773252563264785, 4.982459496890938, 5.4039030292218015, 4.279425736864688, 4.281937085073839, 4.477432220524729, 5.485704415413167, 4.804489736051826, 5.487470468587413, 5.042189938177045, 4.9671485538710645, 4.585462242403287, 4.47983458488882, 4.415117060015671, 1.856140679778752, 1.8541528199970199, 1.000507848197748, 1.4379561915144752, 0.9905158388131796, 0.990046369038138, 1.0086177835307804, 1.0760600167036283, 1.0126904429487258, 1.0303127040966455, 1.7351719614619412, 1.0909002043419918, 1.32412025844531, 1.228683199695054, 2.288889309642338, 1.1010588262516188, 2.1587752325609055, 1.0662980912822357, 1.881308746495056, 1.1519858094652087, 1.0667015361705, 0.5603567250718144, 0.9608800576934057, 0.5769392310448823, 1.2540881899691587, 0.5572371859634742, 1.1087243703232335, 0.771832020763102, 1.4045323693077396, 1.1008323900722428, 1.145619423593913, 2.978756357979411, 1.8623349814492893, 5.784191186935653, 3.7086204437359673, 10.133075020563096, 8.802588923414062, 3.9702982271808955, 6.393078164472659, 7.8156029537894085, 2.5618774115371035, 5.876520607472216, 6.946751192079887, 4.1590050139367225, 6.026119868888155, 6.513482023468061, 6.986182275842823, 6.129012906530724, 5.830517676415458, 5.472175810944597, 7.140836636048603, 5.2676164853513505, 4.934451623667103, 3.3996050051419138, 4.368210156977467, 4.359244998732558, 4.282544143108103, 4.005656560331291, 3.492561114935292, 3.67817786403426, 4.238123255885343, 3.847208504973658, 4.220352712325959, 3.588392043141338, 4.0156835884403375, 3.6144791689860156, 3.6498232757795908], \"Total\": [185.0, 212.0, 151.0, 148.0, 172.0, 129.0, 157.0, 159.0, 145.0, 130.0, 147.0, 131.0, 109.0, 150.0, 171.0, 133.0, 127.0, 173.0, 107.0, 93.0, 77.0, 100.0, 101.0, 96.0, 97.0, 73.0, 114.0, 107.0, 88.0, 107.0, 5.051627916931946, 7.9444285951484686, 5.307373136418939, 7.987568767493013, 7.049559476388943, 18.180652979379644, 7.057534447095778, 16.324317824066632, 9.963587911771842, 5.916900123152459, 12.350636965422135, 7.042913771640843, 6.07529902210689, 15.253301227007746, 12.162461900197105, 9.915568870092887, 7.428020201817226, 4.986656211261958, 23.72843580010698, 5.03994141113442, 7.433824128084529, 12.508171476116658, 5.1422479293553165, 6.448287206227612, 5.151060438068838, 19.016772110375435, 5.192596881442937, 12.474421306290775, 14.446542237298473, 5.250512131547154, 28.442922603409485, 23.33109850939405, 63.86038814673604, 22.337104918353983, 134.82396682354695, 40.5204678323764, 130.46117180543226, 173.9706377472471, 93.29189390963607, 171.33326429730263, 66.74829221576798, 101.0801649002684, 135.70527648543114, 72.19330029477753, 60.94825622012466, 35.015948259392744, 69.69021943437416, 67.27856636355254, 38.61880263277048, 59.02372846555669, 75.68138281016822, 92.39894844766809, 151.1773313203697, 78.26605220913018, 212.02401740147388, 114.08106844441002, 90.06418665139202, 185.36756265810652, 147.92898547113575, 133.14318542778017, 172.3219817038487, 107.8769797238055, 159.38365990496865, 157.8055251504185, 130.93017395768507, 91.3986247341623, 93.43681451267653, 131.1714919259371, 107.27302677896074, 101.6797750591423, 150.7355731657463, 148.14376096336994, 145.45948840136313, 127.83226023648385, 107.52316815225879, 7.579612779704131, 5.198642713150457, 5.272931730175705, 9.76074651440443, 5.5474090990753755, 10.339756488562495, 6.342509266130548, 6.423674428116328, 6.531784877406578, 5.198128496813027, 5.096248344853174, 5.471470772717454, 5.579412494385206, 5.181027357932499, 7.10358941070046, 5.694299590377966, 5.276528284654588, 4.523166708042176, 4.777479006116097, 6.309207913414383, 17.01654643733309, 6.611101392095829, 5.062447247195575, 4.812873526526367, 4.946087465822763, 9.73207463531299, 5.1711026322831675, 6.878919862321281, 13.69395182439278, 4.975905136124929, 20.410096885205625, 16.570224654056304, 24.16894808215563, 39.0910529053427, 26.145402229509884, 150.7355731657463, 93.12855106913456, 90.4633629282096, 36.88686393706295, 34.807128473489726, 28.81115313097811, 22.600318576961012, 60.70849767364402, 88.51999326266139, 159.38365990496865, 46.843626961102636, 127.83226023648385, 145.45948840136313, 212.02401740147388, 80.55481839274374, 45.81938310040669, 46.83771114219279, 77.5008559293364, 148.14376096336994, 100.37130600952021, 58.46140660154816, 71.76435884981878, 39.03114734541796, 103.00470194710718, 133.14318542778017, 172.3219817038487, 89.42707939336077, 157.8055251504185, 109.57381027392891, 185.36756265810652, 129.33851919601798, 54.139433386501224, 131.1714919259371, 78.71685109288093, 90.06418665139202, 114.08106844441002, 147.92898547113575, 97.547409261347, 173.9706377472471, 4.879628020402243, 19.823025210321347, 10.94508331172167, 5.806514106073834, 10.945558087028841, 6.932552441569259, 8.66872509405969, 4.779140820154293, 9.214554285883755, 4.947665113639792, 4.917403664358493, 11.939879020682682, 5.129111307937367, 4.932411131953624, 20.14416000862835, 4.81338668876785, 5.2634726022640725, 12.831248463813157, 8.405420661386898, 5.190132166201378, 5.471470772717454, 5.605421322269264, 12.319313666004147, 9.131791077733391, 9.190325842849763, 8.95355497782531, 9.322206486338743, 11.502564976384436, 6.158937337259828, 11.930761774718595, 9.299306293461926, 22.740602000814366, 77.5008559293364, 13.977458938016754, 12.06901845946776, 59.86175029318792, 147.92898547113575, 31.86068694032696, 130.93017395768507, 157.8055251504185, 60.68770381710911, 72.19330029477753, 185.36756265810652, 90.15317843225013, 58.72718164023399, 127.83226023648385, 148.14376096336994, 212.02401740147388, 109.57381027392891, 96.70890815366874, 107.27302677896074, 101.6797750591423, 131.1714919259371, 80.5907670477991, 145.45948840136313, 73.4805161174645, 93.29189390963607, 172.3219817038487, 80.55481839274374, 130.46117180543226, 151.1773313203697, 135.70527648543114, 171.33326429730263, 79.92385343674275, 159.38365990496865, 129.33851919601798, 150.7355731657463, 107.52316815225879, 4.091366755724344, 4.474766437931072, 5.670985192272891, 3.690580761104904, 7.394579688970872, 4.28813495699164, 6.292109882090946, 6.550663102423074, 19.12940839370445, 5.181027357932499, 4.947291701354406, 4.811103808923059, 4.812384736763482, 4.903217417846987, 8.155820413681255, 5.2409443743373725, 16.280446971168846, 4.9313713049564365, 21.829951642136837, 19.73789373233242, 5.937363985927867, 9.217748302467584, 11.761443730056241, 5.605421322269264, 5.381702082881558, 9.114827054663634, 6.170756815483779, 8.689327580195142, 6.857203090620814, 10.463871131498085, 21.713726587572328, 35.54517402156418, 14.35890504029945, 34.69684095229045, 18.127388276044254, 57.1943939045299, 39.0910529053427, 97.547409261347, 16.418957326906302, 172.3219817038487, 129.33851919601798, 173.9706377472471, 96.70890815366874, 93.12855106913456, 150.7355731657463, 80.5907670477991, 45.81938310040669, 35.90492785961012, 159.38365990496865, 131.1714919259371, 57.1012984210911, 151.1773313203697, 145.45948840136313, 171.33326429730263, 92.39894844766809, 103.00470194710718, 185.36756265810652, 127.83226023648385, 100.37130600952021, 212.02401740147388, 135.70527648543114, 101.6797750591423, 133.14318542778017, 148.14376096336994, 107.27302677896074, 130.93017395768507, 134.82396682354695, 107.52316815225879, 157.8055251504185, 83.89478844403061, 107.8769797238055, 4.305940512620106, 11.420188418656803, 4.516452042678024, 6.260242802517237, 4.884339593237538, 6.435384034294218, 7.395112432896352, 4.12871487170492, 6.849718641301794, 6.708354588031124, 4.552187043958643, 5.276528284654588, 4.97433586623488, 5.045104275771602, 4.9123111116139055, 4.99246017008529, 4.910388054738462, 5.318589539975008, 5.321222838657924, 5.262348165473781, 13.531613458326731, 13.605379043405907, 30.786367702325183, 6.060487221640949, 11.129794468973092, 8.752284948553859, 11.265793146575334, 6.408095508202815, 9.355450504904633, 6.4635053424724775, 9.981019996600121, 22.600318576961012, 148.14376096336994, 100.37130600952021, 150.7355731657463, 147.92898547113575, 24.743020009381453, 71.76435884981878, 101.6797750591423, 157.8055251504185, 90.57973824401714, 93.43681451267653, 107.8769797238055, 24.228724196749212, 77.5008559293364, 50.21883217153855, 97.547409261347, 212.02401740147388, 133.14318542778017, 185.36756265810652, 88.51999326266139, 145.45948840136313, 151.1773313203697, 64.22197558795625, 75.08788628747931, 172.3219817038487, 109.57381027392891, 96.70890815366874, 135.70527648543114, 107.52316815225879, 73.4805161174645, 129.33851919601798, 107.27302677896074, 130.93017395768507, 159.38365990496865, 87.1919222037357, 173.9706377472471, 7.229869547514878, 7.317272252370483, 5.662406806332244, 4.28813495699164, 6.4375492342854015, 4.523166708042176, 4.799887979056109, 9.355450504904633, 4.779140820154293, 4.968690972895297, 7.491154999309776, 4.879628020402243, 5.2634726022640725, 10.910691444709249, 8.269887276287156, 5.704713498793222, 8.865957070961002, 8.749985987237395, 6.085789818942462, 6.364203715038011, 14.290363852926646, 6.040174859985781, 5.95765005010888, 6.555796236001824, 6.857203090620814, 19.12940839370445, 9.934399930959875, 9.735264103385509, 27.902025423184895, 6.544209021151623, 21.499601971366484, 37.26968101546004, 12.368413283798423, 14.218935140427952, 10.478232069797485, 129.33851919601798, 185.36756265810652, 73.4805161174645, 22.600318576961012, 151.1773313203697, 147.92898547113575, 22.748213095701143, 107.52316815225879, 91.3986247341623, 148.14376096336994, 90.15317843225013, 107.27302677896074, 173.9706377472471, 101.0801649002684, 172.3219817038487, 159.38365990496865, 46.843626961102636, 52.112275772829264, 145.45948840136313, 212.02401740147388, 131.1714919259371, 130.93017395768507, 109.57381027392891, 171.33326429730263, 157.8055251504185, 133.14318542778017, 107.8769797238055, 96.70890815366874, 90.06418665139202, 93.12855106913456, 78.71685109288093, 93.43681451267653, 92.39894844766809, 101.6797750591423, 150.7355731657463, 13.620965830016175, 7.15467268240727, 4.917403664358493, 6.689144391546213, 5.31624701636483, 5.3552723959357165, 5.518841302432829, 6.984335793572152, 6.550663102423074, 8.613216321628068, 12.237179083678223, 4.680064633484733, 4.921057843224435, 7.406136649366132, 4.958006880823888, 6.962895271140126, 7.800592599323094, 5.380445264048535, 8.269887276287156, 14.054405041009945, 8.017480591620684, 11.353992524193208, 6.2810053467521785, 6.5696320994476, 6.502639649883545, 4.908136538553035, 6.24759141424314, 6.4375492342854015, 6.270001983004616, 10.325527269972032, 11.251065215921802, 15.569995929028332, 26.48210738298312, 9.322206486338743, 131.1714919259371, 28.476776501343544, 77.5008559293364, 171.33326429730263, 185.36756265810652, 148.14376096336994, 46.92952967736379, 40.07255012167684, 71.76435884981878, 130.93017395768507, 172.3219817038487, 103.00470194710718, 92.39894844766809, 212.02401740147388, 57.1943939045299, 157.8055251504185, 93.12855106913456, 46.9054325792453, 75.08788628747931, 74.95983056840423, 80.75397189830358, 145.45948840136313, 107.27302677896074, 129.33851919601798, 159.38365990496865, 88.51999326266139, 80.5907670477991, 147.92898547113575, 114.08106844441002, 66.3150649637975, 135.70527648543114, 70.24582349853857, 173.9706377472471, 150.7355731657463, 134.82396682354695, 109.57381027392891, 3.690580761104904, 7.52808757199009, 6.170756815483779, 6.216838013335545, 8.730672231582163, 4.495380725516867, 6.793918162863777, 4.643854007177201, 4.574727462071092, 4.958341684943369, 7.41905266018026, 7.406136649366132, 5.2409443743373725, 7.234569652606825, 5.321222838657924, 10.105970996301473, 5.704713498793222, 12.517058380980817, 5.720586109843159, 6.257978900108431, 6.290884884901303, 14.567348609134841, 6.148284836380742, 6.555796236001824, 7.049570044961421, 12.104177267642017, 7.613302404721897, 7.491154999309776, 10.271559833735793, 7.684958280837578, 9.016321636920765, 12.077904935896862, 15.744224558929268, 73.4805161174645, 60.62037862655804, 109.57381027392891, 212.02401740147388, 131.1714919259371, 172.3219817038487, 75.08788628747931, 45.81938310040669, 145.45948840136313, 53.13478836949849, 38.91527602848612, 129.33851919601798, 93.12855106913456, 89.72453018192408, 34.95846167740658, 127.83226023648385, 48.705292821780596, 96.70890815366874, 52.112275772829264, 151.1773313203697, 157.8055251504185, 114.08106844441002, 97.547409261347, 159.38365990496865, 78.14248194984053, 185.36756265810652, 173.9706377472471, 147.92898547113575, 61.98683702526674, 107.8769797238055, 130.93017395768507, 130.46117180543226, 150.7355731657463, 171.33326429730263, 103.00470194710718, 4.186302875150697, 5.940382132202455, 6.085600545268604, 4.12871487170492, 4.507348334164176, 4.923874561394038, 5.6015082077689, 8.827226973021961, 6.124550632283438, 6.292109882090946, 21.2046461614313, 6.437993469207087, 5.936164771059398, 9.341714035849092, 7.129010111831779, 6.987048889333356, 8.39790538672953, 4.4074280831473445, 4.495380725516867, 8.374234945070928, 8.475066946665649, 9.243364122343511, 4.939007747510835, 4.97734386440083, 4.812873526526367, 5.178062137331259, 14.986114967280756, 5.2409443743373725, 13.404556678625758, 5.500094417722417, 10.792519744509958, 16.382846556012506, 10.18900952836414, 30.193542536626556, 10.80731984969551, 11.17061103063793, 129.33851919601798, 58.26788630165946, 50.21883217153855, 151.1773313203697, 29.854995165736934, 93.12855106913456, 133.14318542778017, 127.83226023648385, 145.45948840136313, 88.51999326266139, 159.38365990496865, 130.93017395768507, 90.57973824401714, 212.02401740147388, 87.1919222037357, 68.17403852521909, 78.14248194984053, 60.94825622012466, 114.08106844441002, 185.36756265810652, 157.8055251504185, 57.1943939045299, 97.547409261347, 107.52316815225879, 135.70527648543114, 80.5907670477991, 80.75397189830358, 90.06418665139202, 150.7355731657463, 107.8769797238055, 172.3219817038487, 134.82396682354695, 131.1714919259371, 147.92898547113575, 130.46117180543226, 148.14376096336994, 4.664094834175114, 6.309207913414383, 4.4074280831473445, 6.611101392095829, 4.884339593237538, 5.0990517867597225, 5.22030302432573, 5.931508205671282, 5.940382132202455, 6.085600545268604, 10.361365508602914, 6.5696320994476, 8.42402024842096, 7.964775308903095, 14.866564098013585, 7.32076167838823, 14.567348609134841, 7.219594055827577, 12.790974630894963, 8.319216276698885, 8.158456036799207, 4.305940512620106, 7.428020201817226, 4.643854007177201, 10.243004338516952, 4.574727462071092, 9.153918277438377, 6.425850724335217, 11.717965021008583, 9.214554285883755, 10.054364601868357, 29.626630097220822, 18.018222406110933, 77.5008559293364, 46.92952967736379, 185.36756265810652, 157.8055251504185, 53.13478836949849, 107.52316815225879, 159.38365990496865, 29.854995165736934, 109.57381027392891, 148.14376096336994, 66.3150649637975, 130.93017395768507, 151.1773313203697, 172.3219817038487, 145.45948840136313, 133.14318542778017, 127.83226023648385, 212.02401740147388, 129.33851919601798, 114.08106844441002, 54.87494091460996, 100.37130600952021, 101.0801649002684, 101.6797750591423, 89.72453018192408, 70.24582349853857, 81.83480645191477, 134.82396682354695, 103.00470194710718, 171.33326429730263, 87.1919222037357, 173.9706377472471, 96.70890815366874, 107.8769797238055], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -7.4936, -7.1513, -7.5594, -7.1507, -7.297, -6.3513, -7.305, -6.4669, -6.9614, -7.4834, -6.752, -7.3217, -7.48, -6.5602, -6.7918, -7.0084, -7.2997, -7.6997, -6.1424, -7.6956, -7.3165, -6.7989, -7.6967, -7.4713, -7.6962, -6.3933, -7.6917, -6.8234, -6.6767, -7.6915, -6.0022, -6.2017, -5.2351, -6.2725, -4.554, -5.7182, -4.6594, -4.425, -4.9991, -4.4584, -5.3317, -4.9854, -4.7292, -5.2821, -5.435, -5.9171, -5.3397, -5.3769, -5.8512, -5.5055, -5.3078, -5.1618, -4.7929, -5.3185, -4.549, -5.0365, -5.2152, -4.6941, -4.8631, -4.9543, -4.774, -5.1075, -4.8661, -4.8834, -5.0184, -5.2619, -5.2531, -5.1089, -5.1963, -5.219, -5.066, -5.1197, -5.1266, -5.1652, -5.2373, -6.2172, -6.6793, -6.6744, -6.1026, -6.6829, -6.0867, -6.6667, -6.6757, -6.666, -6.9278, -6.9552, -6.8858, -6.8751, -6.9671, -6.6568, -6.8788, -6.9616, -7.1594, -7.1458, -6.8734, -5.9096, -6.8586, -7.1458, -7.1966, -7.1694, -6.4998, -7.1387, -6.8636, -6.1757, -7.2015, -5.7955, -6.0749, -5.7728, -5.3612, -5.7292, -4.4395, -4.8361, -4.8727, -5.5807, -5.6339, -5.7707, -5.9546, -5.3173, -5.075, -4.7201, -5.5108, -4.8799, -4.8033, -4.5909, -5.1854, -5.5481, -5.5399, -5.2713, -4.9302, -5.1439, -5.4278, -5.3318, -5.6491, -5.1775, -5.0939, -4.9917, -5.275, -5.0529, -5.2013, -5.0537, -5.2001, -5.4932, -5.2162, -5.3911, -5.3969, -5.3645, -5.3175, -5.4019, -5.402, -6.6957, -5.3986, -6.0096, -6.6683, -6.052, -6.6302, -6.4146, -7.0552, -6.3993, -7.0219, -7.0475, -6.1635, -7.0201, -7.0673, -5.6669, -7.1043, -7.0208, -6.1551, -6.5974, -7.0919, -7.0782, -7.061, -6.2995, -6.6139, -6.6106, -6.6378, -6.608, -6.4016, -7.042, -6.3813, -6.6486, -5.8631, -4.8489, -6.2998, -6.4283, -5.2274, -4.6043, -5.7233, -4.721, -4.6023, -5.2815, -5.165, -4.5136, -5.043, -5.3422, -4.8153, -4.7637, -4.5387, -4.974, -5.0538, -4.9944, -5.0329, -4.9048, -5.1928, -4.8566, -5.2481, -5.1301, -4.8023, -5.2475, -5.0129, -4.9432, -5.0026, -4.9633, -5.2721, -5.087, -5.151, -5.1875, -5.2461, -6.4852, -6.5924, -6.5603, -6.9902, -6.3567, -6.9532, -6.5858, -6.5763, -5.5423, -6.8492, -6.9187, -6.9644, -6.9644, -6.9625, -6.4946, -6.9557, -5.8303, -7.0291, -5.5425, -5.6755, -6.8807, -6.4477, -6.21, -6.9557, -7.0382, -6.5198, -6.9129, -6.611, -6.8633, -6.4914, -5.7812, -5.3724, -6.2121, -5.4782, -6.0379, -5.204, -5.4971, -4.8447, -6.1434, -4.4853, -4.69, -4.4995, -4.9386, -4.9656, -4.7223, -5.1519, -5.5019, -5.6553, -4.7614, -4.8843, -5.3777, -4.8331, -4.9072, -4.8252, -5.1643, -5.1271, -4.8496, -5.0435, -5.1566, -4.8232, -5.0272, -5.1632, -5.0577, -5.02, -5.1877, -5.2068, -5.2384, -5.2821, -5.2559, -5.343, -5.341, -6.5541, -5.6037, -6.5627, -6.2648, -6.5687, -6.4048, -6.2722, -6.923, -6.4297, -6.5441, -6.9324, -6.8121, -6.8945, -6.8845, -6.9247, -6.9172, -6.9553, -6.8765, -6.8859, -6.9243, -6.0026, -6.0077, -5.239, -6.8696, -6.2664, -6.5218, -6.2969, -6.8726, -6.4997, -6.8879, -6.4586, -5.7338, -4.2823, -4.6716, -4.4589, -4.4743, -5.7842, -5.0625, -4.8736, -4.619, -4.9897, -4.9891, -4.9147, -5.8722, -5.1437, -5.4497, -5.0652, -4.6175, -4.9344, -4.7696, -5.1766, -4.924, -4.9063, -5.3646, -5.2957, -4.9318, -5.1732, -5.2308, -5.1114, -5.2042, -5.3631, -5.1719, -5.237, -5.1818, -5.2004, -5.325, -5.3081, -6.2499, -6.2734, -6.5427, -6.9298, -6.5289, -6.959, -6.903, -6.2531, -6.9336, -6.9031, -6.4991, -6.938, -6.8948, -6.1837, -6.5263, -6.8997, -6.4792, -6.496, -6.8948, -6.8554, -6.0495, -6.914, -6.9287, -6.8702, -6.8325, -5.815, -6.4904, -6.5144, -5.4618, -6.93, -5.7939, -5.3995, -6.3343, -6.2328, -6.4812, -4.523, -4.3496, -5.0972, -5.9409, -4.6325, -4.6999, -5.9417, -4.9431, -5.057, -4.7593, -5.0845, -4.9836, -4.6902, -5.0319, -4.7249, -4.8046, -5.5212, -5.4669, -4.8889, -4.6799, -4.9635, -4.9712, -5.0722, -4.8425, -4.8918, -5.0247, -5.1666, -5.2128, -5.2484, -5.2386, -5.311, -5.2485, -5.2634, -5.259, -5.2019, -5.2274, -6.1135, -6.4987, -6.2286, -6.5078, -6.5265, -6.5031, -6.3727, -6.491, -6.2194, -5.8791, -6.8793, -6.8411, -6.4501, -6.8747, -6.5362, -6.4338, -6.8585, -6.4824, -5.9533, -6.5357, -6.188, -6.7932, -6.7659, -6.7946, -7.0764, -6.8426, -6.8341, -6.8629, -6.3895, -6.3334, -6.0811, -5.6745, -6.542, -4.6272, -5.7368, -5.0452, -4.5124, -4.5435, -4.6937, -5.4382, -5.5416, -5.1832, -4.8206, -4.6775, -4.9965, -5.062, -4.5997, -5.3586, -4.8155, -5.1023, -5.4835, -5.2628, -5.2714, -5.2512, -4.9918, -5.1353, -5.0617, -4.9764, -5.2274, -5.2721, -5.0805, -5.1746, -5.367, -5.1353, -5.3697, -5.1189, -5.1666, -5.2998, -5.3433, -6.4516, -6.1768, -6.3985, -6.4064, -6.0931, -6.7645, -6.3832, -6.7645, -6.788, -6.7524, -6.3613, -6.3762, -6.7364, -6.4168, -6.7583, -6.1171, -6.7475, -5.9654, -6.7992, -6.7126, -6.7367, -5.8975, -6.7611, -6.7303, -6.6885, -6.1596, -6.6678, -6.6894, -6.3777, -6.7573, -6.6034, -6.3395, -6.1021, -4.9081, -5.065, -4.6357, -4.1929, -4.7105, -4.5744, -5.1351, -5.4644, -4.8261, -5.4231, -5.6051, -4.9695, -5.1548, -5.1885, -5.6927, -5.0306, -5.5286, -5.1858, -5.4978, -5.0251, -5.0099, -5.1752, -5.2465, -5.0449, -5.365, -5.0587, -5.0841, -5.1533, -5.46, -5.2929, -5.2406, -5.2491, -5.2619, -5.2443, -5.454, -6.3924, -6.3376, -6.3427, -6.754, -6.7586, -6.728, -6.6802, -6.2832, -6.6831, -6.6831, -5.4756, -6.6677, -6.7634, -6.3372, -6.6692, -6.7031, -6.6195, -7.2883, -7.2902, -6.6829, -6.6858, -6.6403, -7.2895, -7.2902, -7.3301, -7.2605, -6.2038, -7.2556, -6.3241, -7.2157, -6.5615, -6.1811, -6.639, -5.6404, -6.5878, -6.5623, -4.5134, -5.1805, -5.3593, -4.5152, -5.7994, -4.9851, -4.7261, -4.786, -4.6968, -5.0746, -4.757, -4.8878, -5.1629, -4.7351, -5.2373, -5.3768, -5.3054, -5.4527, -5.1444, -4.9214, -5.0145, -5.5131, -5.2629, -5.22, -5.1388, -5.3721, -5.3715, -5.3269, -5.1238, -5.2564, -5.1235, -5.2081, -5.2231, -5.303, -5.3263, -5.3409, -6.0451, -6.0462, -6.6631, -6.3004, -6.6731, -6.6736, -6.655, -6.5903, -6.651, -6.6337, -6.1125, -6.5766, -6.3828, -6.4576, -5.8355, -6.5673, -5.894, -6.5994, -6.0316, -6.5221, -6.599, -7.2428, -6.7035, -7.2136, -6.4372, -7.2484, -6.5604, -6.9226, -6.3239, -6.5675, -6.5276, -5.5721, -6.0418, -4.9085, -5.3529, -4.3478, -4.4885, -5.2847, -4.8084, -4.6075, -5.7228, -4.8926, -4.7253, -5.2383, -4.8675, -4.7897, -4.7197, -4.8506, -4.9005, -4.9639, -4.6978, -5.002, -5.0673, -5.4399, -5.1892, -5.1913, -5.209, -5.2759, -5.413, -5.3612, -5.2195, -5.3162, -5.2237, -5.3859, -5.2734, -5.3786, -5.3689], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8202, 0.7098, 0.705, 0.7049, 0.6835, 0.6819, 0.6745, 0.674, 0.6732, 0.6723, 0.6678, 0.6598, 0.6493, 0.6486, 0.6434, 0.631, 0.6285, 0.627, 0.6245, 0.6206, 0.611, 0.6082, 0.5994, 0.5984, 0.5982, 0.5949, 0.5946, 0.5865, 0.5864, 0.5837, 0.5834, 0.582, 0.5418, 0.5547, 0.4755, 0.5135, 0.4031, 0.3496, 0.3987, 0.3315, 0.4009, 0.3322, 0.2939, 0.3721, 0.3885, 0.4606, 0.3498, 0.3478, 0.4286, 0.3501, 0.2992, 0.2456, 0.1221, 0.255, 0.0279, 0.1602, 0.2178, 0.0171, 0.0737, 0.0878, 0.0102, 0.1451, -0.0039, -0.0112, 0.0405, 0.1564, 0.1432, -0.0519, 0.0619, 0.0927, -0.148, -0.1843, -0.173, -0.0824, 0.0185, 1.6908, 1.6058, 1.5966, 1.5525, 1.5373, 1.5109, 1.4196, 1.3979, 1.3908, 1.3574, 1.3498, 1.3481, 1.3393, 1.3214, 1.3161, 1.3152, 1.3087, 1.2649, 1.2238, 1.2182, 1.1897, 1.1862, 1.1659, 1.1656, 1.1655, 1.1583, 1.1518, 1.1415, 1.1409, 1.1274, 1.122, 1.051, 0.9757, 0.9065, 0.9407, 0.4785, 0.5635, 0.5559, 0.745, 0.7499, 0.8021, 0.8609, 0.5101, 0.3753, 0.1421, 0.5759, 0.2029, 0.1504, -0.0141, 0.3592, 0.5608, 0.547, 0.312, 0.0051, 0.1807, 0.4374, 0.3284, 0.62, 0.1213, -0.0518, -0.2075, 0.1651, -0.1807, 0.0356, -0.3425, -0.129, 0.4488, -0.1592, 0.1766, 0.0361, -0.1679, -0.3807, -0.0487, -0.6273, 1.6527, 1.5481, 1.531, 1.5062, 1.4886, 1.3671, 1.3592, 1.3141, 1.3135, 1.3128, 1.2933, 1.2901, 1.2785, 1.2704, 1.2638, 1.2578, 1.2519, 1.2266, 1.2072, 1.1949, 1.1558, 1.1488, 1.1228, 1.1079, 1.1047, 1.1037, 1.0931, 1.0894, 1.0736, 1.0731, 1.055, 0.9463, 0.7344, 0.9963, 1.0146, 0.6141, 0.3325, 0.7489, 0.3379, 0.2699, 0.5463, 0.4892, 0.1976, 0.389, 0.5184, 0.2675, 0.1716, 0.0381, 0.263, 0.308, 0.2638, 0.2788, 0.1522, 0.3513, 0.097, 0.3884, 0.2677, -0.0181, 0.2971, 0.0496, -0.0281, 0.0204, -0.1734, 0.2804, -0.2248, -0.0799, -0.2695, 0.0097, 2.0394, 1.8427, 1.6379, 1.6376, 1.5761, 1.5245, 1.5085, 1.4777, 1.44, 1.4393, 1.416, 1.3982, 1.3979, 1.3811, 1.3402, 1.3213, 1.3133, 1.3088, 1.3077, 1.2755, 1.2715, 1.2647, 1.2587, 1.2541, 1.2124, 1.2039, 1.2008, 1.1605, 1.1449, 1.0942, 1.0743, 0.9903, 1.0571, 0.9087, 0.9982, 0.6831, 0.7706, 0.5085, 0.9917, 0.2989, 0.3811, 0.2752, 0.4233, 0.4339, 0.1957, 0.3922, 0.6069, 0.6974, 0.1008, 0.1727, 0.511, 0.082, 0.0465, -0.0353, 0.2431, 0.1716, -0.1385, 0.0393, 0.1681, -0.2464, -0.0041, 0.1485, -0.0156, -0.0846, 0.0704, -0.148, -0.2089, -0.0263, -0.3837, 0.161, -0.0884, 1.9194, 1.8945, 1.8631, 1.8346, 1.7788, 1.6669, 1.6605, 1.5925, 1.5797, 1.4861, 1.4855, 1.4582, 1.4347, 1.4307, 1.4171, 1.4084, 1.3869, 1.3859, 1.3759, 1.3486, 1.3259, 1.3153, 1.2675, 1.2622, 1.2575, 1.2424, 1.2149, 1.2033, 1.1979, 1.1795, 1.1742, 1.0818, 0.653, 0.6531, 0.4591, 0.4625, 0.9408, 0.5977, 0.4381, 0.2532, 0.4376, 0.4072, 0.3379, 0.8738, 0.4396, 0.5674, 0.288, -0.0407, 0.1077, -0.0585, 0.2737, 0.0296, 0.0088, 0.4065, 0.3191, -0.1476, 0.0637, 0.131, -0.0884, 0.0516, 0.2734, -0.1008, 0.0212, -0.1229, -0.3381, 0.1404, -0.5334, 1.7054, 1.6699, 1.657, 1.5479, 1.5425, 1.4653, 1.4619, 1.4445, 1.4357, 1.4272, 1.4208, 1.4104, 1.3779, 1.3601, 1.2946, 1.2926, 1.2722, 1.2685, 1.2328, 1.2274, 1.2245, 1.2211, 1.2202, 1.183, 1.1757, 1.1673, 1.1471, 1.1434, 1.143, 1.1249, 1.0716, 0.9158, 1.084, 1.0462, 1.103, 0.5481, 0.3616, 0.5393, 0.8747, 0.2825, 0.2369, 0.8674, 0.3127, 0.3613, 0.176, 0.3476, 0.2745, 0.0845, 0.2858, 0.0593, 0.0576, 0.5655, 0.5132, 0.0647, -0.1031, 0.0936, 0.0877, 0.1647, -0.0525, -0.0196, 0.0174, 0.0859, 0.1491, 0.1846, 0.161, 0.2566, 0.1478, 0.144, 0.0527, -0.2839, 2.0945, 1.8523, 1.8421, 1.8044, 1.755, 1.7289, 1.7223, 1.6171, 1.563, 1.5608, 1.5499, 1.5109, 1.4989, 1.4811, 1.4579, 1.4567, 1.4455, 1.3923, 1.3385, 1.3373, 1.3162, 1.316, 1.3028, 1.2852, 1.2667, 1.2662, 1.2588, 1.2373, 1.2348, 1.2094, 1.1797, 1.1071, 0.9826, 1.1591, 0.4298, 0.8476, 0.538, 0.2775, 0.1677, 0.2417, 0.6467, 0.7012, 0.4769, 0.2383, 0.1067, 0.3023, 0.3454, -0.0228, 0.5284, 0.0567, 0.2972, 0.6019, 0.3521, 0.3452, 0.291, -0.0382, 0.1228, 0.0094, -0.1142, 0.2229, 0.2721, -0.1437, 0.022, 0.3721, -0.1122, 0.3119, -0.3442, -0.2486, -0.2702, -0.1063, 2.1762, 1.7381, 1.7152, 1.6998, 1.6736, 1.666, 1.6343, 1.6335, 1.625, 1.58, 1.5681, 1.5551, 1.5406, 1.5378, 1.5035, 1.5033, 1.4447, 1.441, 1.3903, 1.3871, 1.3577, 1.3572, 1.3563, 1.3228, 1.292, 1.2803, 1.2359, 1.2304, 1.2264, 1.1369, 1.1311, 1.1027, 1.0749, 0.7284, 0.7639, 0.6012, 0.384, 0.3465, 0.2098, 0.4798, 0.6444, 0.1275, 0.5376, 0.667, 0.1016, 0.2448, 0.2483, 0.6866, 0.0522, 0.5191, 0.176, 0.4823, -0.11, -0.1377, 0.0214, 0.1067, -0.1827, 0.21, -0.3475, -0.3095, -0.2165, 0.3466, -0.0404, -0.1817, -0.1866, -0.3439, -0.4543, -0.1552, 2.1094, 1.8142, 1.785, 1.7615, 1.6692, 1.6115, 1.5303, 1.4725, 1.4381, 1.4111, 1.4037, 1.4036, 1.389, 1.3618, 1.3002, 1.2864, 1.1861, 1.1619, 1.1403, 1.1255, 1.1106, 1.0694, 1.0469, 1.0385, 1.0322, 1.0286, 1.0226, 1.0214, 1.0139, 1.013, 0.9932, 0.9562, 0.9733, 0.8855, 0.9655, 0.9579, 0.5577, 0.688, 0.6579, 0.3999, 0.7378, 0.4144, 0.316, 0.2968, 0.2568, 0.3757, 0.1052, 0.1711, 0.2644, -0.1583, 0.2281, 0.3347, 0.2696, 0.3708, 0.0522, -0.2102, -0.1423, 0.374, 0.0903, 0.0358, -0.1157, 0.172, 0.1706, 0.1061, -0.2058, -0.0038, -0.3393, -0.1785, -0.166, -0.3662, -0.2639, -0.4055, 2.3486, 2.0454, 1.7872, 1.7444, 1.6744, 1.6309, 1.626, 1.563, 1.5008, 1.4939, 1.483, 1.4745, 1.4196, 1.4009, 1.3989, 1.3755, 1.3607, 1.3573, 1.3532, 1.2929, 1.2355, 1.2308, 1.2248, 1.1844, 1.1698, 1.1646, 1.159, 1.1506, 1.1485, 1.1452, 1.0979, 0.9728, 1.0004, 0.6748, 0.732, 0.3634, 0.3836, 0.676, 0.4475, 0.2548, 0.8143, 0.3443, 0.21, 0.5008, 0.1914, 0.1254, 0.0645, 0.1031, 0.1416, 0.1189, -0.1209, 0.0691, 0.1293, 0.4885, 0.1354, 0.1263, 0.1027, 0.1609, 0.2686, 0.1677, -0.1899, -0.0175, -0.4337, 0.0795, -0.4987, -0.0168, -0.1164]}, \"token.table\": {\"Topic\": [1, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 1, 4, 1, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 6, 1, 3, 4, 6, 8, 1, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 8, 9, 10, 1, 3, 4, 1, 7, 1, 3, 4, 5, 1, 4, 8, 1, 2, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 5, 8, 2, 5, 1, 2, 5, 8, 9, 1, 2, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 7, 10, 1, 2, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 5, 8, 9, 1, 3, 4, 1, 2, 3, 9, 10, 1, 2, 2, 9, 4, 5, 10, 2, 4, 6, 8, 2, 6, 1, 2, 5, 8, 1, 2, 4, 5, 7, 8, 10, 1, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 4, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 6, 8, 9, 1, 2, 3, 4, 6, 7, 9, 1, 2, 9, 1, 2, 4, 5, 9, 10, 1, 2, 5, 1, 3, 1, 2, 3, 4, 5, 9, 10, 1, 5, 7, 8, 9, 2, 3, 5, 6, 8, 2, 5, 7, 8, 9, 2, 3, 5, 10, 1, 2, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 6, 7, 2, 6, 1, 7, 1, 6, 7, 8, 1, 2, 3, 4, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 1, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 7, 9, 2, 3, 5, 10, 1, 3, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 6, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 5, 8, 10, 3, 4, 5, 8, 1, 2, 3, 6, 8, 9, 1, 5, 7, 1, 4, 6, 7, 1, 2, 5, 1, 3, 6, 8, 9, 2, 5, 7, 1, 6, 7, 9, 1, 4, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 8, 10, 1, 4, 1, 2, 3, 9, 1, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 5, 7, 8, 3, 7, 8, 9, 10, 2, 5, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 8, 1, 2, 5, 10, 1, 3, 4, 7, 8, 1, 8, 2, 4, 6, 8, 1, 8, 10, 2, 3, 7, 1, 6, 2, 7, 1, 4, 5, 8, 1, 3, 10, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 6, 8, 1, 2, 3, 6, 7, 10, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 5, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 7, 1, 3, 5, 6, 2, 7, 9, 1, 5, 6, 8, 1, 2, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 9, 1, 2, 3, 4, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 6, 9, 1, 9, 5, 6, 8, 9, 1, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 6, 7, 9, 10, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 9, 1, 5, 1, 3, 7, 1, 4, 8, 3, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 5, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 7, 9, 1, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 7, 8, 9, 1, 5, 6, 2, 4, 7, 9, 3, 5, 7, 9, 10, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 10, 3, 4, 6, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 6, 8, 9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 7, 9, 1, 3, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 6, 8, 10, 1, 2, 3, 5, 8, 9, 1, 2, 3, 5, 7, 1, 2, 4, 7, 9, 1, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 1, 2, 3, 5, 7, 2, 4, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 7, 8, 2, 3, 4, 7, 2, 3, 8, 1, 2, 3, 4, 6, 9, 3, 6, 8, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 5, 7, 1, 3, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 6, 7, 1, 2, 3, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 1, 10, 1, 2, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 8, 10, 1, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 8, 10, 3, 4, 5, 6, 8, 1, 2, 3, 8, 9, 1, 2, 4, 7, 9, 1, 2, 3, 4, 7, 9, 1, 5, 6, 10, 1, 2, 4, 5, 6, 10, 1, 2, 6, 7, 8, 10, 1, 2, 3, 8, 2, 5, 6, 8, 9, 1, 5, 7, 9, 1, 3, 2, 7, 1, 4, 5, 1, 2, 3, 8, 9, 10, 1, 2, 3, 4, 9, 1, 2, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 8, 10, 1, 2, 7, 9, 10, 1, 3, 5, 6, 1, 2, 2, 3, 4, 5, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 8, 9, 4, 6, 7, 9, 1, 2, 4, 7, 1, 2, 6, 7, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 10, 1, 6, 1, 6, 8, 2, 6, 8, 10, 1, 3, 6, 8, 2, 3, 4, 6, 8, 10, 1, 3, 8, 9, 10, 1, 3, 1, 4, 9, 1, 3, 9, 1, 2, 8, 1, 2, 4, 6, 2, 4, 1, 4, 6, 7, 9, 1, 2, 3, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 1, 2, 4, 5, 7, 2, 4, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 8, 3, 4, 10, 1, 5, 6, 1, 2, 4, 1, 6, 10, 1, 9, 2, 6, 7, 9, 1, 3, 1, 2, 1, 2, 4, 6, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 7, 8, 1, 3, 6, 2, 6, 7, 8, 1, 2, 1, 3, 4, 8, 9, 10, 1, 2, 5, 6, 10, 1, 6, 7, 8, 1, 2, 3, 6, 7, 9, 1, 2, 3, 4, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 8, 10, 1, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 7, 2, 3, 7, 1, 2, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 2, 3, 5, 1, 3, 6, 10, 1, 2, 5, 6, 2, 3, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 6, 1, 2, 5, 7, 2, 4, 9, 10, 1, 6, 10, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 7, 1, 2, 10, 2, 3, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 9, 1, 2, 3, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 8, 9, 1, 2, 3, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 10, 1, 4, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"Freq\": [0.5244217886761695, 0.1748072628920565, 0.1748072628920565, 0.41243357713289297, 0.09165190602953177, 0.1069272237011204, 0.0534636118505602, 0.0534636118505602, 0.06873892952214883, 0.06873892952214883, 0.03818829417897157, 0.0534636118505602, 0.045825953014765886, 0.7552462619733408, 0.12587437699555679, 0.6083500540680746, 0.20278335135602488, 0.5475377127485765, 0.21901508509943063, 0.10950754254971531, 0.4561348403531583, 0.08049438359173382, 0.08049438359173382, 0.053662922394489214, 0.026831461197244607, 0.16098876718346763, 0.026831461197244607, 0.026831461197244607, 0.08049438359173382, 0.026831461197244607, 0.5052747325429758, 0.16842491084765862, 0.16842491084765862, 0.3280185764210575, 0.14578603396491444, 0.10933952547368582, 0.09111627122807152, 0.03644650849122861, 0.05466976273684291, 0.07289301698245722, 0.05466976273684291, 0.03644650849122861, 0.05466976273684291, 0.23446662647708433, 0.20097139412321513, 0.06699046470773838, 0.06699046470773838, 0.06699046470773838, 0.06699046470773838, 0.06699046470773838, 0.06699046470773838, 0.10048569706160757, 0.10048569706160757, 0.9897799446473678, 0.38615601394993315, 0.16549543454997137, 0.1103302896999809, 0.16549543454997137, 0.05516514484999045, 0.05516514484999045, 0.05516514484999045, 0.21970649952119847, 0.2510931423099411, 0.1569332139437132, 0.06277328557748528, 0.06277328557748528, 0.06277328557748528, 0.06277328557748528, 0.03138664278874264, 0.03138664278874264, 0.03138664278874264, 0.20892958004668552, 0.2785727733955807, 0.06964319334889517, 0.20892958004668552, 0.06964319334889517, 0.06964319334889517, 0.06964319334889517, 0.41288228416252987, 0.13762742805417663, 0.08028266636493636, 0.057344761689240256, 0.06881371402708832, 0.06881371402708832, 0.045875809351392204, 0.022937904675696102, 0.057344761689240256, 0.045875809351392204, 0.22141273516258825, 0.4428254703251765, 0.22141273516258825, 0.05549937044071823, 0.3329962226443094, 0.05549937044071823, 0.11099874088143646, 0.05549937044071823, 0.05549937044071823, 0.11099874088143646, 0.05549937044071823, 0.05549937044071823, 0.11099874088143646, 0.3952726398525626, 0.16940255993681255, 0.06274168886548613, 0.06901585775203474, 0.04391918220584029, 0.06901585775203474, 0.050193351092388906, 0.03764501331929168, 0.050193351092388906, 0.050193351092388906, 0.7963579329994633, 0.06125830253842025, 0.06125830253842025, 0.5648158403771532, 0.059454298987068756, 0.08918144848060314, 0.059454298987068756, 0.04459072424030157, 0.04459072424030157, 0.029727149493534378, 0.04459072424030157, 0.04459072424030157, 0.04459072424030157, 0.5659578022970342, 0.08085111461386203, 0.16170222922772406, 0.1383150820949086, 0.1383150820949086, 0.1383150820949086, 0.2766301641898172, 0.1383150820949086, 0.7703274664542158, 0.19258186661355395, 0.5540157166863833, 0.08903824018174017, 0.04946568898985565, 0.01978627559594226, 0.04946568898985565, 0.07914510238376904, 0.03957255119188452, 0.02967941339391339, 0.02967941339391339, 0.03957255119188452, 0.43598783350493703, 0.17128093459122526, 0.07785497026873875, 0.0311419881074955, 0.09342596432248651, 0.062283976214991, 0.0311419881074955, 0.046712982161243255, 0.0311419881074955, 0.01557099405374775, 0.16833933874035814, 0.16833933874035814, 0.3366786774807163, 0.16833933874035814, 0.6922071618066314, 0.1384414323613263, 0.06922071618066315, 0.5601955938369809, 0.37346372922465393, 0.31210521089141985, 0.15605260544570992, 0.15605260544570992, 0.15605260544570992, 0.48794095911893115, 0.16264698637297706, 0.16264698637297706, 0.44363989674242277, 0.22181994837121138, 0.11090997418560569, 0.11090997418560569, 0.3619108174025253, 0.1034030906864358, 0.1034030906864358, 0.1034030906864358, 0.07238216348050507, 0.07238216348050507, 0.041361236274574324, 0.06204185441186148, 0.041361236274574324, 0.041361236274574324, 0.32473134413960764, 0.32473134413960764, 0.16236567206980382, 0.3657317866024033, 0.1406660717701551, 0.05626642870806205, 0.16879928612418613, 0.028133214354031023, 0.08439964306209306, 0.05626642870806205, 0.05626642870806205, 0.028133214354031023, 0.028133214354031023, 0.12555281991221917, 0.25110563982443834, 0.12555281991221917, 0.12555281991221917, 0.12555281991221917, 0.3999138114969753, 0.11160385437124892, 0.07440256958083262, 0.06510224838322853, 0.06510224838322853, 0.08370289077843669, 0.04650160598802038, 0.03720128479041631, 0.04650160598802038, 0.05580192718562446, 0.3217069506572091, 0.16085347532860456, 0.3217069506572091, 0.6306652197357101, 0.15766630493392753, 0.14780203455850952, 0.22170305183776426, 0.22170305183776426, 0.14780203455850952, 0.07390101727925476, 0.13134904497945377, 0.13134904497945377, 0.13134904497945377, 0.13134904497945377, 0.13134904497945377, 0.13134904497945377, 0.4602040451000843, 0.12842903584188398, 0.07491693757443232, 0.04280967861396133, 0.09632177688141298, 0.07491693757443232, 0.032107258960470994, 0.021404839306980665, 0.04280967861396133, 0.032107258960470994, 0.3462609121857173, 0.1598127287011003, 0.05327090956703343, 0.06658863695879179, 0.07990636435055015, 0.06658863695879179, 0.07990636435055015, 0.07990636435055015, 0.039953182175275076, 0.026635454783516716, 0.38375085806083437, 0.1279169526869448, 0.10659746057245399, 0.0639584763434724, 0.042638984228981595, 0.042638984228981595, 0.10659746057245399, 0.042638984228981595, 0.042638984228981595, 0.042638984228981595, 0.15921018129957273, 0.15921018129957273, 0.15921018129957273, 0.15921018129957273, 0.15921018129957273, 0.35808247096144463, 0.08952061774036116, 0.08952061774036116, 0.08952061774036116, 0.08952061774036116, 0.08952061774036116, 0.08952061774036116, 0.28617504925166, 0.071543762312915, 0.214631286938745, 0.071543762312915, 0.071543762312915, 0.071543762312915, 0.071543762312915, 0.071543762312915, 0.24220611765981745, 0.24220611765981745, 0.24220611765981745, 0.35679744394134716, 0.17839872197067358, 0.17839872197067358, 0.4155358288570855, 0.13851194295236183, 0.13851194295236183, 0.13851194295236183, 0.13851194295236183, 0.1962227765077274, 0.5886683295231823, 0.7585912742069035, 0.18964781855172588, 0.20473596909283687, 0.40947193818567373, 0.20473596909283687, 0.3050735453028262, 0.1525367726514131, 0.1525367726514131, 0.1525367726514131, 0.7694316037302626, 0.19235790093256566, 0.3179202984305407, 0.15896014921527035, 0.15896014921527035, 0.15896014921527035, 0.0826161066455372, 0.1652322132910744, 0.1652322132910744, 0.1652322132910744, 0.0826161066455372, 0.1652322132910744, 0.0826161066455372, 0.46414442953835533, 0.15471480984611843, 0.15471480984611843, 0.2325624449540545, 0.13953746697243272, 0.0930249779816218, 0.0465124889908109, 0.0465124889908109, 0.1860499559632436, 0.0930249779816218, 0.0930249779816218, 0.0930249779816218, 0.0465124889908109, 0.061423375032080306, 0.24569350012832122, 0.18427012509624094, 0.24569350012832122, 0.061423375032080306, 0.061423375032080306, 0.061423375032080306, 0.061423375032080306, 0.061423375032080306, 0.17633620369218544, 0.3526724073843709, 0.17633620369218544, 0.17633620369218544, 0.31941939330668334, 0.1551465624632462, 0.10038895218210048, 0.054757610281145716, 0.06388387866133667, 0.07301014704152763, 0.054757610281145716, 0.09126268380190952, 0.03650507352076381, 0.054757610281145716, 0.35879989294465753, 0.14351995717786303, 0.07175997858893152, 0.11276568063974951, 0.08201140410163602, 0.04100570205081801, 0.051257127563522506, 0.051257127563522506, 0.051257127563522506, 0.04100570205081801, 0.18181506062474084, 0.18181506062474084, 0.18181506062474084, 0.18181506062474084, 0.18181506062474084, 0.15586947798886636, 0.07793473899443318, 0.2338042169832995, 0.07793473899443318, 0.15586947798886636, 0.15586947798886636, 0.07793473899443318, 0.15309750991019264, 0.6123900396407705, 0.15309750991019264, 0.2895371268882598, 0.09651237562941992, 0.09651237562941992, 0.09651237562941992, 0.09651237562941992, 0.19302475125883983, 0.1453716600882964, 0.43611498026488926, 0.1453716600882964, 0.6232617892513331, 0.20775392975044438, 0.27797030452745464, 0.18531353635163642, 0.09265676817581821, 0.09265676817581821, 0.09265676817581821, 0.09265676817581821, 0.09265676817581821, 0.25639077730755383, 0.12819538865377691, 0.25639077730755383, 0.12819538865377691, 0.12819538865377691, 0.1471904688911471, 0.1471904688911471, 0.1471904688911471, 0.1471904688911471, 0.2943809377822942, 0.28370524546095843, 0.14185262273047922, 0.14185262273047922, 0.14185262273047922, 0.14185262273047922, 0.2626926885986394, 0.0875642295328798, 0.43782114766439906, 0.0875642295328798, 0.3905104271955376, 0.1952552135977688, 0.0976276067988844, 0.0976276067988844, 0.417189203872298, 0.1549559900097107, 0.0834378407744596, 0.0715181492352511, 0.047678766156834064, 0.047678766156834064, 0.047678766156834064, 0.047678766156834064, 0.047678766156834064, 0.023839383078417032, 0.21454148252679303, 0.21454148252679303, 0.10727074126339652, 0.10727074126339652, 0.21454148252679303, 0.442168093527043, 0.2210840467635215, 0.8021407192591952, 0.2005351798147988, 0.15533861778860705, 0.3106772355772141, 0.15533861778860705, 0.15533861778860705, 0.11168748083600707, 0.33506244250802125, 0.22337496167201415, 0.11168748083600707, 0.11168748083600707, 0.11168748083600707, 0.43341521385569465, 0.1362162100689326, 0.06191645912224209, 0.06191645912224209, 0.049533167297793675, 0.049533167297793675, 0.07429975094669052, 0.049533167297793675, 0.049533167297793675, 0.03714987547334526, 0.5481675719313361, 0.36544504795422406, 0.42734452547737967, 0.21367226273868983, 0.21367226273868983, 0.4004132224905752, 0.12186489380147941, 0.07544017235329678, 0.08704635271534243, 0.05222781162920546, 0.06383399199125112, 0.06383399199125112, 0.05803090181022829, 0.029015450905114145, 0.0406216312671598, 0.15948958273228409, 0.15948958273228409, 0.15948958273228409, 0.15948958273228409, 0.15948958273228409, 0.1597382132842996, 0.1597382132842996, 0.3194764265685992, 0.1597382132842996, 0.22907743492708832, 0.11453871746354416, 0.11453871746354416, 0.11453871746354416, 0.22907743492708832, 0.09799071563691161, 0.44095822036610227, 0.09799071563691161, 0.09799071563691161, 0.048995357818455806, 0.048995357818455806, 0.048995357818455806, 0.048995357818455806, 0.048995357818455806, 0.048995357818455806, 0.2736848278223995, 0.22236892260569963, 0.08552650869449985, 0.06842120695559988, 0.06842120695559988, 0.08552650869449985, 0.06842120695559988, 0.03421060347779994, 0.03421060347779994, 0.03421060347779994, 0.30825904161425677, 0.5137650693570947, 0.10275301387141893, 0.590213922919814, 0.09964650646698157, 0.08431627470283057, 0.03832557941037753, 0.03832557941037753, 0.030660463528302023, 0.015330231764151011, 0.03832557941037753, 0.030660463528302023, 0.022995347646226516, 0.5110772662088324, 0.14054624820742892, 0.08943852158654567, 0.038330794965662426, 0.038330794965662426, 0.038330794965662426, 0.02555386331044162, 0.05110772662088324, 0.038330794965662426, 0.02555386331044162, 0.1899886397375474, 0.1899886397375474, 0.1899886397375474, 0.1899886397375474, 0.5544749315880256, 0.0700389387269085, 0.06420236049966613, 0.06420236049966613, 0.03501946936345425, 0.058365782272423755, 0.07587551695415089, 0.029182891136211878, 0.0233463129089695, 0.0233463129089695, 0.5086637974740519, 0.06493580393285768, 0.0757584379216673, 0.08658107191047691, 0.03246790196642884, 0.0757584379216673, 0.0757584379216673, 0.03246790196642884, 0.03246790196642884, 0.021645267977619228, 0.1568265959018006, 0.1568265959018006, 0.052275531967266864, 0.2613776598363343, 0.052275531967266864, 0.20910212786906746, 0.052275531967266864, 0.45170880373500577, 0.11292720093375144, 0.03226491455250041, 0.08066228638125103, 0.06452982910500082, 0.04839737182875062, 0.06452982910500082, 0.06452982910500082, 0.03226491455250041, 0.03226491455250041, 0.4077483765893227, 0.1650410095718687, 0.07766635744558528, 0.07766635744558528, 0.0485414734034908, 0.029124884042094477, 0.07766635744558528, 0.03883317872279264, 0.03883317872279264, 0.03883317872279264, 0.1284521851589728, 0.2569043703179456, 0.0642260925794864, 0.0642260925794864, 0.1284521851589728, 0.1284521851589728, 0.1926782777384592, 0.0642260925794864, 0.46447460064491863, 0.23223730032245932, 0.23223730032245932, 0.18792672855854536, 0.18792672855854536, 0.18792672855854536, 0.18792672855854536, 0.11897084515874777, 0.11897084515874777, 0.23794169031749554, 0.11897084515874777, 0.11897084515874777, 0.11897084515874777, 0.4074866263988693, 0.20374331319943464, 0.20374331319943464, 0.4339454570406531, 0.21697272852032656, 0.10848636426016328, 0.10848636426016328, 0.2704488969096965, 0.2704488969096965, 0.2704488969096965, 0.2805438579306638, 0.1402719289653319, 0.1402719289653319, 0.1402719289653319, 0.1402719289653319, 0.4393492579911157, 0.21967462899555784, 0.21967462899555784, 0.45714358923938203, 0.22857179461969102, 0.11428589730984551, 0.11428589730984551, 0.7214763538138121, 0.08016403931264579, 0.08016403931264579, 0.08275080873199592, 0.4137540436599796, 0.08275080873199592, 0.08275080873199592, 0.04137540436599796, 0.04137540436599796, 0.08275080873199592, 0.04137540436599796, 0.08275080873199592, 0.08275080873199592, 0.8029236125420351, 0.1003654515677544, 0.1003654515677544, 0.16559832268554456, 0.2318376517597624, 0.13247865814843565, 0.06623932907421783, 0.06623932907421783, 0.03311966453710891, 0.06623932907421783, 0.06623932907421783, 0.09935899361132673, 0.06623932907421783, 0.4399106145763686, 0.14663687152545618, 0.0855381750565161, 0.07331843576272809, 0.04887895717515207, 0.04887895717515207, 0.036659217881364045, 0.04887895717515207, 0.036659217881364045, 0.04887895717515207, 0.3677674690536175, 0.14010189297280667, 0.105076419729605, 0.105076419729605, 0.0525382098648025, 0.03502547324320167, 0.0525382098648025, 0.07005094648640334, 0.0525382098648025, 0.017512736621600834, 0.26906015227381724, 0.20179511420536295, 0.06726503806845431, 0.20179511420536295, 0.06726503806845431, 0.13453007613690862, 0.24415781386489271, 0.24415781386489271, 0.12207890693244636, 0.06103945346622318, 0.06103945346622318, 0.12207890693244636, 0.06103945346622318, 0.06103945346622318, 0.12207890693244636, 0.06103945346622318, 0.29206859022003984, 0.09735619674001328, 0.09735619674001328, 0.09735619674001328, 0.19471239348002656, 0.09735619674001328, 0.7099334397835614, 0.1419866879567123, 0.4776555740598607, 0.11941389351496517, 0.11941389351496517, 0.11941389351496517, 0.2863550750009252, 0.1431775375004626, 0.2863550750009252, 0.1431775375004626, 0.2986927284322878, 0.07965139424861006, 0.07965139424861006, 0.07965139424861006, 0.1194770913729151, 0.05973854568645755, 0.07965139424861006, 0.03982569712430503, 0.07965139424861006, 0.05973854568645755, 0.286700778545473, 0.19113385236364863, 0.09556692618182432, 0.09556692618182432, 0.09556692618182432, 0.17852334816060253, 0.17852334816060253, 0.17852334816060253, 0.17852334816060253, 0.17852334816060253, 0.5376910208770233, 0.17923034029234106, 0.17923034029234106, 0.3233235068704934, 0.08083087671762335, 0.12124631507643502, 0.08083087671762335, 0.1616617534352467, 0.04041543835881167, 0.08083087671762335, 0.08083087671762335, 0.04041543835881167, 0.04041543835881167, 0.7765391317170456, 0.1941347829292614, 0.2709600642096815, 0.541920128419363, 0.6715283853851077, 0.1790742361026954, 0.04476855902567385, 0.04476855902567385, 0.15979600058777335, 0.15979600058777335, 0.15979600058777335, 0.15979600058777335, 0.15979600058777335, 0.6641793087800416, 0.2656717235120167, 0.20213079405166928, 0.20213079405166928, 0.20213079405166928, 0.20213079405166928, 0.7700493494858899, 0.055003524963277844, 0.055003524963277844, 0.298991901345053, 0.1494959506725265, 0.298991901345053, 0.7618304462085551, 0.19045761155213878, 0.7915971665552827, 0.13193286109254712, 0.16205467658209788, 0.16205467658209788, 0.16205467658209788, 0.32410935316419576, 0.3922297877407779, 0.19611489387038894, 0.19611489387038894, 0.8096748392813121, 0.16193496785626244, 0.5679197353852785, 0.06925850431527786, 0.12466530776750015, 0.027703401726111146, 0.04155510258916672, 0.05540680345222229, 0.04155510258916672, 0.027703401726111146, 0.027703401726111146, 0.027703401726111146, 0.40493963610564526, 0.20246981805282263, 0.20246981805282263, 0.20246981805282263, 0.400923793281252, 0.1837567385872405, 0.1503464224804695, 0.08352579026692751, 0.08352579026692751, 0.016705158053385502, 0.016705158053385502, 0.033410316106771004, 0.016705158053385502, 0.016705158053385502, 0.17529364098855105, 0.17529364098855105, 0.17529364098855105, 0.17529364098855105, 0.19369470901658056, 0.19369470901658056, 0.09684735450829028, 0.09684735450829028, 0.19369470901658056, 0.09684735450829028, 0.23320161562768407, 0.23320161562768407, 0.23320161562768407, 0.42689530855860935, 0.10672382713965234, 0.10672382713965234, 0.05336191356982617, 0.06670239196228271, 0.026680956784913085, 0.08004287035473925, 0.04002143517736963, 0.04002143517736963, 0.04002143517736963, 0.4837163061736922, 0.1319226289564615, 0.2198710482607692, 0.04397420965215384, 0.04397420965215384, 0.33985210730559995, 0.11328403576853331, 0.07552269051235555, 0.037761345256177775, 0.11328403576853331, 0.037761345256177775, 0.1510453810247111, 0.07552269051235555, 0.07552269051235555, 0.037761345256177775, 0.4379741938465735, 0.14599139794885782, 0.29198279589771564, 0.5700877071728118, 0.1900292357242706, 0.4379168232635507, 0.11260718312491302, 0.10009527388881158, 0.05004763694440579, 0.06255954618050724, 0.05004763694440579, 0.06255954618050724, 0.025023818472202895, 0.037535727708304344, 0.05004763694440579, 0.3522989813034276, 0.1761494906517138, 0.0880747453258569, 0.1761494906517138, 0.17222036866387047, 0.34444073732774094, 0.17222036866387047, 0.17222036866387047, 0.2388742596566194, 0.2388742596566194, 0.4777485193132388, 0.20103186171803655, 0.20103186171803655, 0.20103186171803655, 0.20103186171803655, 0.29209975697993, 0.438149635469895, 0.0730249392449825, 0.0730249392449825, 0.43111375409457003, 0.25424657292756697, 0.03316259646881308, 0.05527099411468847, 0.04421679529175077, 0.05527099411468847, 0.03316259646881308, 0.03316259646881308, 0.04421679529175077, 0.022108397645875386, 0.48984057173368684, 0.06997722453338383, 0.2099316736001515, 0.06997722453338383, 0.2176201403735903, 0.2176201403735903, 0.2176201403735903, 0.10881007018679514, 0.10881007018679514, 0.4046015328924113, 0.10789374210464302, 0.09710436789417871, 0.05394687105232151, 0.05934155815755366, 0.09170968078894656, 0.0647362452627858, 0.0323681226313929, 0.037762809736625054, 0.05394687105232151, 0.4605381743050692, 0.04605381743050692, 0.04605381743050692, 0.18421526972202767, 0.04605381743050692, 0.04605381743050692, 0.04605381743050692, 0.04605381743050692, 0.04605381743050692, 0.04605381743050692, 0.8511170123602435, 0.14185283539337393, 0.5596194174237821, 0.08609529498827416, 0.04304764749413708, 0.04304764749413708, 0.07174607915689514, 0.04304764749413708, 0.04304764749413708, 0.028698431662758053, 0.028698431662758053, 0.028698431662758053, 0.1571288482857792, 0.1571288482857792, 0.1571288482857792, 0.1571288482857792, 0.1571288482857792, 0.673835743155448, 0.168458935788862, 0.22245056894152668, 0.22245056894152668, 0.22245056894152668, 0.22245056894152668, 0.4293658234708923, 0.1431219411569641, 0.1431219411569641, 0.1431219411569641, 0.330116331426089, 0.14147842775403813, 0.047159475918012714, 0.09431895183602543, 0.047159475918012714, 0.047159475918012714, 0.047159475918012714, 0.09431895183602543, 0.18863790367205086, 0.047159475918012714, 0.19328122034925238, 0.2577082937990032, 0.0644270734497508, 0.09664061017462619, 0.0644270734497508, 0.07516491902470926, 0.07516491902470926, 0.0644270734497508, 0.0644270734497508, 0.0322135367248754, 0.12020363057525967, 0.12020363057525967, 0.12020363057525967, 0.12020363057525967, 0.12020363057525967, 0.12020363057525967, 0.12020363057525967, 0.19753292255123572, 0.39506584510247145, 0.19753292255123572, 0.5305615364759209, 0.09579583297481904, 0.08105801251715458, 0.06632019205949011, 0.05895128183065787, 0.03684455114416117, 0.05158237160182564, 0.029475640915328935, 0.03684455114416117, 0.0221067306864967, 0.20309209496126604, 0.20309209496126604, 0.20309209496126604, 0.20309209496126604, 0.7753997053932572, 0.15507994107865145, 0.40548120310640895, 0.20274060155320447, 0.20274060155320447, 0.41570501893778294, 0.20785250946889147, 0.20785250946889147, 0.2033593473824477, 0.4067186947648954, 0.2033593473824477, 0.32998878607048765, 0.17186915941171232, 0.08249719651762191, 0.06874766376468493, 0.061872897388216434, 0.06874766376468493, 0.05499813101174794, 0.05499813101174794, 0.05499813101174794, 0.041248598258810956, 0.15539088182942792, 0.15539088182942792, 0.31078176365885585, 0.15539088182942792, 0.13729334374176005, 0.13729334374176005, 0.06864667187088003, 0.06864667187088003, 0.06864667187088003, 0.06864667187088003, 0.06864667187088003, 0.20594001561264008, 0.06864667187088003, 0.13729334374176005, 0.43718451352171933, 0.21859225676085967, 0.21859225676085967, 0.5133919121874043, 0.11734672278569243, 0.04400502104463466, 0.07334170174105777, 0.029336680696423107, 0.04400502104463466, 0.04400502104463466, 0.058673361392846214, 0.058673361392846214, 0.029336680696423107, 0.1632772851495174, 0.1632772851495174, 0.1632772851495174, 0.1632772851495174, 0.1632772851495174, 0.5204972953430325, 0.13012432383575812, 0.13012432383575812, 0.4885404691468442, 0.15544469472854133, 0.08882553984488076, 0.04441276992244038, 0.03330957744183029, 0.07772234736427067, 0.02220638496122019, 0.02220638496122019, 0.04441276992244038, 0.02220638496122019, 0.3701195167377875, 0.09252987918444687, 0.09252987918444687, 0.09252987918444687, 0.09252987918444687, 0.09252987918444687, 0.6289424319122998, 0.17969783768922853, 0.08984891884461427, 0.4018207410391082, 0.2009103705195541, 0.2009103705195541, 0.2009103705195541, 0.1522155251409107, 0.1522155251409107, 0.1522155251409107, 0.1522155251409107, 0.1522155251409107, 0.14424701557304614, 0.2884940311460923, 0.14424701557304614, 0.14424701557304614, 0.14424701557304614, 0.5895474697220084, 0.09647140413632865, 0.09647140413632865, 0.032157134712109556, 0.021438089808073036, 0.04287617961614607, 0.05359522452018259, 0.021438089808073036, 0.021438089808073036, 0.010719044904036518, 0.2560171492764693, 0.0853390497588231, 0.0853390497588231, 0.1706780995176462, 0.1706780995176462, 0.0853390497588231, 0.0853390497588231, 0.22347535091962353, 0.44695070183924707, 0.22347535091962353, 0.6085599181027437, 0.0869371311575348, 0.2608113934726044, 0.04424716388818562, 0.35397731110548497, 0.04424716388818562, 0.08849432777637124, 0.17698865555274249, 0.13274149166455687, 0.04424716388818562, 0.04424716388818562, 0.04424716388818562, 0.23071443358730054, 0.11535721679365027, 0.34607165038095083, 0.11535721679365027, 0.11535721679365027, 0.2362739190056127, 0.2362739190056127, 0.06750683400160362, 0.06750683400160362, 0.06750683400160362, 0.06750683400160362, 0.06750683400160362, 0.06750683400160362, 0.06750683400160362, 0.10126025100240545, 0.17459859689662632, 0.26189789534493946, 0.13094894767246973, 0.10912412306039145, 0.04364964922415658, 0.02182482461207829, 0.02182482461207829, 0.08729929844831316, 0.06547447383623486, 0.04364964922415658, 0.46458190410292394, 0.12271974825360256, 0.06135987412680128, 0.05259417782297252, 0.04382848151914377, 0.05259417782297252, 0.06135987412680128, 0.05259417782297252, 0.04382848151914377, 0.04382848151914377, 0.41087739968029946, 0.10271934992007486, 0.20543869984014973, 0.10271934992007486, 0.10271934992007486, 0.7536685092955293, 0.4137449221309015, 0.1789167230836331, 0.08945836154181655, 0.044729180770908275, 0.0670937711563624, 0.0670937711563624, 0.05591147596363534, 0.022364590385454138, 0.044729180770908275, 0.0335468855781812, 0.5720272582540286, 0.24515453925172656, 0.08171817975057552, 0.1334907634526503, 0.1334907634526503, 0.1334907634526503, 0.2669815269053006, 0.1334907634526503, 0.1334907634526503, 0.28947413319002696, 0.20433468225178375, 0.13622312150118918, 0.05108367056294594, 0.06811156075059459, 0.06811156075059459, 0.06811156075059459, 0.05108367056294594, 0.034055780375297294, 0.05108367056294594, 0.5590971776589335, 0.0847116935846869, 0.05082701615081214, 0.03388467743387476, 0.05082701615081214, 0.06776935486774952, 0.05082701615081214, 0.03388467743387476, 0.01694233871693738, 0.03388467743387476, 0.40947618702310407, 0.08685858512611298, 0.11167532373357385, 0.09926695442984342, 0.04963347721492171, 0.04963347721492171, 0.07445021582238256, 0.03722510791119128, 0.04963347721492171, 0.03722510791119128, 0.29553320009420964, 0.24012072507654533, 0.11082495003532862, 0.07388330002355241, 0.07388330002355241, 0.07388330002355241, 0.036941650011776204, 0.05541247501766431, 0.05541247501766431, 0.036941650011776204, 0.2012602525403772, 0.2012602525403772, 0.2012602525403772, 0.2012602525403772, 0.4542210963400474, 0.09269818292654028, 0.07415854634123223, 0.05561890975592417, 0.08342836463388625, 0.0648887280485782, 0.037079273170616114, 0.04634909146327014, 0.04634909146327014, 0.037079273170616114, 0.07818012535062245, 0.39090062675311227, 0.07818012535062245, 0.1563602507012449, 0.07818012535062245, 0.1563602507012449, 0.496421791512612, 0.0496421791512612, 0.29785307490756724, 0.0496421791512612, 0.0496421791512612, 0.0496421791512612, 0.28460827678782774, 0.07115206919695694, 0.2134562075908708, 0.14230413839391387, 0.2134562075908708, 0.3106557982026538, 0.1553278991013269, 0.1553278991013269, 0.1553278991013269, 0.1553278991013269, 0.7361922369759948, 0.15775547935199888, 0.05258515978399963, 0.40349494696795485, 0.14410533820284102, 0.028821067640568205, 0.17292640584340924, 0.05764213528113641, 0.08646320292170462, 0.028821067640568205, 0.028821067640568205, 0.028821067640568205, 0.028821067640568205, 0.4122775967413472, 0.13742586558044906, 0.22904310930074842, 0.04580862186014968, 0.09161724372029936, 0.22050102319302925, 0.22050102319302925, 0.07350034106434308, 0.22050102319302925, 0.07350034106434308, 0.4155521621286973, 0.20777608106434864, 0.20777608106434864, 0.3789894583804607, 0.13182242030624722, 0.13182242030624722, 0.0494334076148427, 0.0494334076148427, 0.032955605076561804, 0.0494334076148427, 0.06591121015312361, 0.0494334076148427, 0.0494334076148427, 0.27767024678364227, 0.3470878084795529, 0.10412634254386587, 0.034708780847955284, 0.034708780847955284, 0.034708780847955284, 0.06941756169591057, 0.034708780847955284, 0.034708780847955284, 0.06941756169591057, 0.4078954346834174, 0.2039477173417087, 0.2039477173417087, 0.4194903542036139, 0.12118610232548846, 0.10254208658310562, 0.06525405509833994, 0.06525405509833994, 0.08389807084072277, 0.06525405509833994, 0.03728803148476568, 0.01864401574238284, 0.02796602361357426, 0.1350231635390615, 0.1350231635390615, 0.270046327078123, 0.270046327078123, 0.1949655486034173, 0.1949655486034173, 0.1949655486034173, 0.1949655486034173, 0.5268426700044546, 0.17561422333481821, 0.17561422333481821, 0.3531247085513102, 0.050446386935901456, 0.3531247085513102, 0.050446386935901456, 0.050446386935901456, 0.050446386935901456, 0.4098673078435052, 0.2049336539217526, 0.2049336539217526, 0.7585835051090408, 0.08428705612322676, 0.08428705612322676, 0.04214352806161338, 0.2786898722505692, 0.2090174041879269, 0.0696724680626423, 0.05573797445011384, 0.11147594890022768, 0.04180348083758538, 0.08360696167517076, 0.04180348083758538, 0.05573797445011384, 0.04180348083758538, 0.3300064709085347, 0.16500323545426734, 0.16500323545426734, 0.16500323545426734, 0.5035542495392412, 0.16785141651308041, 0.16785141651308041, 0.43259706067167, 0.14419902022389, 0.11092232324914615, 0.06655339394948769, 0.03327669697474384, 0.08873785859931692, 0.04436892929965846, 0.02218446464982923, 0.03327669697474384, 0.02218446464982923, 0.2322012968579385, 0.11610064842896925, 0.11610064842896925, 0.11610064842896925, 0.2322012968579385, 0.23815462402808488, 0.11907731201404244, 0.11907731201404244, 0.11907731201404244, 0.11907731201404244, 0.11907731201404244, 0.45695418986166914, 0.14488791385857802, 0.06687134485780524, 0.04458089657187016, 0.0557261207148377, 0.04458089657187016, 0.0557261207148377, 0.06687134485780524, 0.02229044828593508, 0.04458089657187016, 0.5482986430191646, 0.18276621433972154, 0.8077522458180891, 0.13462537430301486, 0.7867149426481267, 0.06555957855401055, 0.06555957855401055, 0.4065203264068835, 0.17785264280301152, 0.05081504080086044, 0.05081504080086044, 0.06351880100107554, 0.07622256120129066, 0.05081504080086044, 0.03811128060064533, 0.03811128060064533, 0.05081504080086044, 0.39288865165463804, 0.1267382747273026, 0.10139061978184208, 0.04435839615455591, 0.08237987857274669, 0.0633691373636513, 0.05703222362728617, 0.04435839615455591, 0.03802148241819078, 0.05703222362728617, 0.3079747421884327, 0.1437215463546019, 0.1437215463546019, 0.061594948437686536, 0.061594948437686536, 0.061594948437686536, 0.061594948437686536, 0.08212659791691539, 0.04106329895845769, 0.061594948437686536, 0.5081228338719823, 0.06351535423399779, 0.12703070846799558, 0.12703070846799558, 0.06351535423399779, 0.36150025345175796, 0.2146407754869813, 0.045187531681469745, 0.06778129752220462, 0.07907818044257206, 0.03389064876110231, 0.06778129752220462, 0.022593765840734872, 0.06778129752220462, 0.03389064876110231, 0.30926594991688683, 0.13143802871467689, 0.06958483873129953, 0.09277978497506605, 0.05412154123545519, 0.10824308247091038, 0.05412154123545519, 0.05412154123545519, 0.07731648747922171, 0.038658243739610854, 0.31124283550905374, 0.15562141775452687, 0.15562141775452687, 0.15562141775452687, 0.37620524287969787, 0.37620524287969787, 0.18810262143984893, 0.30598114076709565, 0.38247642595886955, 0.038247642595886956, 0.038247642595886956, 0.038247642595886956, 0.038247642595886956, 0.038247642595886956, 0.038247642595886956, 0.07649528519177391, 0.038247642595886956, 0.4537821797116547, 0.15126072657055156, 0.15126072657055156, 0.14583205233745897, 0.14583205233745897, 0.14583205233745897, 0.14583205233745897, 0.14583205233745897, 0.2968542063991598, 0.09895140213305327, 0.19790280426610654, 0.19790280426610654, 0.09895140213305327, 0.12069842393539369, 0.4224444837738779, 0.12069842393539369, 0.12069842393539369, 0.060349211967696846, 0.3352677788333718, 0.08381694470834294, 0.25145083412502883, 0.08381694470834294, 0.08381694470834294, 0.08381694470834294, 0.1982119586313326, 0.1982119586313326, 0.1982119586313326, 0.1982119586313326, 0.2374163334157334, 0.2374163334157334, 0.1187081667078667, 0.1187081667078667, 0.1187081667078667, 0.1187081667078667, 0.15978195028944836, 0.23967292543417254, 0.15978195028944836, 0.07989097514472418, 0.23967292543417254, 0.07989097514472418, 0.27409567515919603, 0.18273045010613068, 0.36546090021226135, 0.09136522505306534, 0.2695761968013051, 0.13478809840065256, 0.13478809840065256, 0.2695761968013051, 0.13478809840065256, 0.4719726729207399, 0.11799316823018498, 0.11799316823018498, 0.11799316823018498, 0.7084627127902342, 0.14169254255804684, 0.7171582613757815, 0.10245118019654022, 0.44720355202340134, 0.14906785067446712, 0.29813570134893425, 0.0994592935106187, 0.29837788053185615, 0.0994592935106187, 0.0994592935106187, 0.0994592935106187, 0.0994592935106187, 0.41428389697073165, 0.08285677939414633, 0.248570338182439, 0.08285677939414633, 0.08285677939414633, 0.07341623292206824, 0.14683246584413648, 0.07341623292206824, 0.07341623292206824, 0.4404973975324094, 0.07341623292206824, 0.43959496765439376, 0.08791899353087874, 0.04395949676543937, 0.04395949676543937, 0.04395949676543937, 0.13187849029631812, 0.04395949676543937, 0.04395949676543937, 0.04395949676543937, 0.04395949676543937, 0.20038032191912938, 0.10019016095956469, 0.10019016095956469, 0.20038032191912938, 0.10019016095956469, 0.10019016095956469, 0.10019016095956469, 0.19312244107510726, 0.19312244107510726, 0.19312244107510726, 0.19312244107510726, 0.19312244107510726, 0.3286344187856858, 0.1643172093928429, 0.1643172093928429, 0.1643172093928429, 0.40436001462163945, 0.40436001462163945, 0.18801977337862003, 0.18801977337862003, 0.18801977337862003, 0.18801977337862003, 0.13523419072642023, 0.13523419072642023, 0.27046838145284047, 0.13523419072642023, 0.27218099513656646, 0.14969954732511156, 0.10887239805462659, 0.054436199027313295, 0.08165429854096994, 0.10887239805462659, 0.04082714927048497, 0.09526334829779827, 0.04082714927048497, 0.04082714927048497, 0.20265586866787338, 0.25331983583484174, 0.050663967166968346, 0.25331983583484174, 0.050663967166968346, 0.050663967166968346, 0.050663967166968346, 0.050663967166968346, 0.33118326574268353, 0.08279581643567088, 0.08279581643567088, 0.16559163287134177, 0.08279581643567088, 0.16559163287134177, 0.08279581643567088, 0.31785840321901293, 0.15892920160950647, 0.15892920160950647, 0.15892920160950647, 0.1600616835666012, 0.3201233671332024, 0.1600616835666012, 0.1600616835666012, 0.24184126496315667, 0.12092063248157833, 0.24184126496315667, 0.24184126496315667, 0.3206686838252458, 0.10688956127508194, 0.21377912255016387, 0.21377912255016387, 0.48629118312868674, 0.08957995478686336, 0.06398568199061667, 0.05118854559249334, 0.03839140919437001, 0.07678281838874002, 0.05118854559249334, 0.06398568199061667, 0.06398568199061667, 0.02559427279624667, 0.3053126025150347, 0.3053126025150347, 0.3053126025150347, 0.3336421755015573, 0.06672843510031146, 0.06672843510031146, 0.06672843510031146, 0.06672843510031146, 0.1334568702006229, 0.06672843510031146, 0.06672843510031146, 0.1334568702006229, 0.06672843510031146, 0.47549550453417794, 0.3169970030227853, 0.8450370795402332, 0.16900741590804663, 0.4966743628357578, 0.16555812094525257, 0.16555812094525257, 0.21533838024504498, 0.21533838024504498, 0.21533838024504498, 0.21533838024504498, 0.19267332082833896, 0.19267332082833896, 0.19267332082833896, 0.19267332082833896, 0.16859118546677082, 0.16859118546677082, 0.16859118546677082, 0.16859118546677082, 0.16859118546677082, 0.16859118546677082, 0.21704793720341978, 0.3255719058051297, 0.10852396860170989, 0.10852396860170989, 0.10852396860170989, 0.7511672418293516, 0.12519454030489194, 0.7059605043048252, 0.10085150061497503, 0.10085150061497503, 0.42818694563437576, 0.21409347281718788, 0.21409347281718788, 0.27645044502119653, 0.27645044502119653, 0.27645044502119653, 0.21942270412872977, 0.32913405619309466, 0.21942270412872977, 0.10971135206436489, 0.24441710061824018, 0.48883420123648036, 0.38174378782176643, 0.09543594695544161, 0.19087189391088322, 0.09543594695544161, 0.09543594695544161, 0.32455715909192123, 0.21637143939461417, 0.10818571969730709, 0.10818571969730709, 0.10818571969730709, 0.4659159972378846, 0.10751907628566568, 0.03583969209522189, 0.07167938419044378, 0.07167938419044378, 0.17919846047610946, 0.03583969209522189, 0.03583969209522189, 0.03583969209522189, 0.03583969209522189, 0.2731956165031626, 0.1365978082515813, 0.1365978082515813, 0.1365978082515813, 0.1365978082515813, 0.4356212435029992, 0.14270351080270663, 0.05257497766415508, 0.06759639985391366, 0.06759639985391366, 0.060085688759034375, 0.03755355547439648, 0.030042844379517188, 0.060085688759034375, 0.04506426656927578, 0.353206696093154, 0.353206696093154, 0.176603348046577, 0.45952551389213137, 0.14223408763327877, 0.0656465019845902, 0.05470541832049183, 0.0328232509922951, 0.08752866931278692, 0.0328232509922951, 0.04376433465639346, 0.04376433465639346, 0.0328232509922951, 0.3350117696394627, 0.251258827229597, 0.08375294240986568, 0.16750588481973136, 0.35505711386294647, 0.08876427846573662, 0.08876427846573662, 0.17752855693147324, 0.08876427846573662, 0.5790357380388659, 0.19301191267962198, 0.5685556559461116, 0.18951855198203718, 0.28093067344271067, 0.210698005082033, 0.03511633418033883, 0.03511633418033883, 0.03511633418033883, 0.07023266836067767, 0.14046533672135533, 0.07023266836067767, 0.07023266836067767, 0.07023266836067767, 0.4071403367086239, 0.20357016835431194, 0.20357016835431194, 0.21440387375332146, 0.21440387375332146, 0.4288077475066429, 0.7399817630552369, 0.08222019589502631, 0.08222019589502631, 0.34525111089581306, 0.34525111089581306, 0.23016740726387538, 0.7195296304647544, 0.07994773671830605, 0.07994773671830605, 0.7936600197698839, 0.19841500494247097, 0.4026413299040111, 0.20132066495200554, 0.10066033247600277, 0.10066033247600277, 0.8230047577585768, 0.1646009515517154, 0.18026433279757154, 0.7210573311902861, 0.3400943023498079, 0.17004715117490396, 0.2550707267623559, 0.08502357558745198, 0.17776094632975067, 0.08888047316487534, 0.08888047316487534, 0.08888047316487534, 0.17776094632975067, 0.17776094632975067, 0.08888047316487534, 0.08888047316487534, 0.08888047316487534, 0.40338790325915275, 0.20169395162957637, 0.20169395162957637, 0.20924263118233777, 0.20924263118233777, 0.20924263118233777, 0.2083381954669385, 0.2083381954669385, 0.2083381954669385, 0.2083381954669385, 0.19237692962247857, 0.5771307888674357, 0.7778699228338218, 0.19446748070845546, 0.19080530693982664, 0.19080530693982664, 0.19080530693982664, 0.19080530693982664, 0.11753266195143701, 0.47013064780574804, 0.1762989929271555, 0.058766330975718505, 0.058766330975718505, 0.2032083409417451, 0.2032083409417451, 0.2032083409417451, 0.2032083409417451, 0.0981449666148807, 0.3925798664595228, 0.1962899332297614, 0.0981449666148807, 0.0981449666148807, 0.0981449666148807, 0.18330643938884633, 0.09165321969442317, 0.09165321969442317, 0.09165321969442317, 0.27495965908326947, 0.09165321969442317, 0.09165321969442317, 0.09165321969442317, 0.3899186221663118, 0.08355399046420967, 0.08355399046420967, 0.1392566507736828, 0.055702660309473115, 0.055702660309473115, 0.055702660309473115, 0.055702660309473115, 0.027851330154736557, 0.027851330154736557, 0.20168033256696014, 0.20168033256696014, 0.20168033256696014, 0.20168033256696014, 0.27332589673044977, 0.13666294836522488, 0.13666294836522488, 0.27332589673044977, 0.31962817661126425, 0.21308545107417617, 0.042617090214835236, 0.06392563532225284, 0.08523418042967047, 0.042617090214835236, 0.10654272553708809, 0.021308545107417618, 0.042617090214835236, 0.08523418042967047, 0.20644933282528458, 0.20644933282528458, 0.16774008292054374, 0.051612333206321144, 0.10322466641264229, 0.051612333206321144, 0.090321583111062, 0.025806166603160572, 0.025806166603160572, 0.07741849980948172, 0.37355677884388705, 0.1296013314356343, 0.09148329277809479, 0.07623607731507899, 0.038118038657539495, 0.06861246958357109, 0.08385968504658689, 0.06861246958357109, 0.038118038657539495, 0.022870823194523698, 0.3129710127590368, 0.24708237849397643, 0.06588863426506039, 0.08236079283132548, 0.04941647569879529, 0.04941647569879529, 0.06588863426506039, 0.04941647569879529, 0.032944317132530196, 0.04941647569879529, 0.33075979495427926, 0.14850439773457436, 0.09450279855836549, 0.06075179907323496, 0.12150359814646992, 0.07425219886728718, 0.07425219886728718, 0.02700079958810443, 0.02700079958810443, 0.047251399279182746, 0.36239491052557626, 0.18119745526278813, 0.36239491052557626, 0.5630956082532893, 0.14077390206332233, 0.14077390206332233, 0.32772851024834765, 0.21848567349889844, 0.10924283674944922, 0.10924283674944922, 0.10924283674944922, 0.44980288649160494, 0.07937697996910675, 0.07276223163834786, 0.07276223163834786, 0.06614748330758896, 0.07937697996910675, 0.03307374165379448, 0.04630323831531227, 0.06614748330758896, 0.04630323831531227, 0.2889132726575506, 0.12381997399609314, 0.04127332466536438, 0.04127332466536438, 0.16509329866145753, 0.08254664933072876, 0.08254664933072876, 0.08254664933072876, 0.08254664933072876, 0.04127332466536438, 0.2923371827109134, 0.032481909190101486, 0.09744572757030447, 0.032481909190101486, 0.22737336433071043, 0.06496381838020297, 0.09744572757030447, 0.06496381838020297, 0.032481909190101486, 0.032481909190101486, 0.41559436940303296, 0.20779718470151648, 0.20779718470151648, 0.29692985111304016, 0.19795323407536008, 0.06598441135845336, 0.04948830851884002, 0.06598441135845336, 0.04948830851884002, 0.0824805141980667, 0.09897661703768004, 0.04948830851884002, 0.04948830851884002, 0.18585822379457614, 0.18585822379457614, 0.18585822379457614, 0.18585822379457614, 0.44159986300956255, 0.14351995547810784, 0.05519998287619532, 0.04415998630095625, 0.09935996917715158, 0.04415998630095625, 0.05519998287619532, 0.04415998630095625, 0.05519998287619532, 0.022079993150478126, 0.5633134491486954, 0.08047334987838506, 0.04598477135907718, 0.08622144629826971, 0.03448857851930789, 0.06897715703861577, 0.04023667493919253, 0.03448857851930789, 0.02299238567953859, 0.02299238567953859, 0.4118906918254985, 0.13729689727516617, 0.08581056079697885, 0.05148633647818731, 0.03432422431879154, 0.05148633647818731, 0.05148633647818731, 0.03432422431879154, 0.08581056079697885, 0.03432422431879154, 0.47414002708358066, 0.14972842960534125, 0.024954738267556877, 0.024954738267556877, 0.04990947653511375, 0.0998189530702275, 0.12477369133778438, 0.024954738267556877, 0.024954738267556877, 0.024954738267556877, 0.41863082965715037, 0.20931541482857519, 0.20931541482857519, 0.23482383226076897, 0.25617145337538433, 0.04269524222923072, 0.08539048445846144, 0.04269524222923072, 0.10673810557307681, 0.06404286334384608, 0.04269524222923072, 0.04269524222923072, 0.04269524222923072, 0.281315018353731, 0.35164377294216376, 0.21098626376529825, 0.07032875458843275, 0.06090520732161634, 0.36543124392969806, 0.06090520732161634, 0.18271562196484903, 0.06090520732161634, 0.12181041464323268, 0.06090520732161634, 0.06090520732161634, 0.40060409735143315, 0.20030204867571658, 0.20030204867571658, 0.4301396118990386, 0.2150698059495193, 0.10753490297475965, 0.10753490297475965, 0.4570235114044042, 0.11425587785110106, 0.2285117557022021, 0.11425587785110106, 0.20211553875042718, 0.20211553875042718, 0.20211553875042718, 0.20211553875042718, 0.376988245636914, 0.18095435790571873, 0.06031811930190624, 0.06031811930190624, 0.0753976491273828, 0.03015905965095312, 0.0753976491273828, 0.03015905965095312, 0.04523858947642968, 0.06031811930190624, 0.4128359318131324, 0.09965005250661815, 0.05694288714663895, 0.05694288714663895, 0.07117860893329868, 0.05694288714663895, 0.07117860893329868, 0.05694288714663895, 0.04270716535997921, 0.04270716535997921, 0.22688969193251135, 0.22688969193251135, 0.22688969193251135, 0.22688969193251135, 0.3597558961101021, 0.17987794805505106, 0.07709054916645046, 0.07709054916645046, 0.051393699444300306, 0.07709054916645046, 0.025696849722150153, 0.10278739888860061, 0.025696849722150153, 0.025696849722150153, 0.4584205654653849, 0.1528068551551283, 0.1528068551551283, 0.2872368349829444, 0.1436184174914722, 0.1436184174914722, 0.2872368349829444, 0.16432231996848265, 0.16432231996848265, 0.3286446399369653, 0.16432231996848265, 0.19155976105987887, 0.19155976105987887, 0.19155976105987887, 0.6109496778172222, 0.20364989260574073, 0.32621872194005397, 0.1343253560929634, 0.09594668292354529, 0.05756800975412717, 0.038378673169418114, 0.09594668292354529, 0.05756800975412717, 0.07675734633883623, 0.05756800975412717, 0.05756800975412717, 0.36716885286088785, 0.12238961762029595, 0.06993692435445482, 0.12238961762029595, 0.03496846217722741, 0.03496846217722741, 0.08742115544306853, 0.03496846217722741, 0.06993692435445482, 0.03496846217722741, 0.19338235403741652, 0.38676470807483304, 0.19338235403741652, 0.5842846117160585, 0.089889940264009, 0.07490828355334082, 0.0449449701320045, 0.029963313421336332, 0.059926626842672664, 0.029963313421336332, 0.014981656710668166, 0.029963313421336332, 0.029963313421336332, 0.5955648138216242, 0.10357648936028246, 0.05178824468014123, 0.07768236702021185, 0.025894122340070616, 0.05178824468014123, 0.025894122340070616, 0.025894122340070616, 0.05178824468014123, 0.025894122340070616, 0.1397687978737189, 0.2795375957474378, 0.41930639362115674, 0.20096846154482098, 0.40193692308964196, 0.20096846154482098, 0.6226965648339927, 0.15567414120849818, 0.15567414120849818, 0.5285315689901187, 0.10570631379802373, 0.07927973534851779, 0.05285315689901186, 0.039639867674258894, 0.05285315689901186, 0.039639867674258894, 0.02642657844950593, 0.039639867674258894, 0.02642657844950593, 0.4056144254504734, 0.14620985103447295, 0.08017959572858195, 0.05188091488320008, 0.06131380849832737, 0.05659736169076373, 0.05659736169076373, 0.07074670211345466, 0.03773157446050915, 0.03301512765294551, 0.44371986625499443, 0.22185993312749722, 0.22185993312749722, 0.34758938768239095, 0.21103641395002307, 0.09931125362354026, 0.0744834402176552, 0.04965562681177013, 0.04965562681177013, 0.0372417201088276, 0.04965562681177013, 0.0372417201088276, 0.024827813405885066, 0.3383408370625377, 0.23882882616179132, 0.05970720654044783, 0.07960960872059711, 0.09951201090074638, 0.04643893842034831, 0.04643893842034831, 0.0331706703002488, 0.0331706703002488, 0.019902402180149277, 0.18976945321086472, 0.3253190626471966, 0.08132976566179916, 0.10843968754906555, 0.027109921887266387, 0.054219843774532775, 0.10843968754906555, 0.027109921887266387, 0.054219843774532775, 0.054219843774532775, 0.37418238381959945, 0.24945492254639964, 0.12472746127319982, 0.24945492254639964, 0.6378687856926349, 0.06675371013062459, 0.02966831561361093, 0.051919552323819125, 0.0222512367102082, 0.0445024734204164, 0.0445024734204164, 0.02966831561361093, 0.03708539451701366, 0.02966831561361093, 0.2255819630066419, 0.11279098150332095, 0.2255819630066419, 0.11279098150332095, 0.11279098150332095, 0.36766931847291245, 0.17992328350802098, 0.1016957689393162, 0.0704047631118343, 0.04693650874122286, 0.054759260198093344, 0.031291005827481906, 0.054759260198093344, 0.06258201165496381, 0.03911375728435239, 0.15378370228741015, 0.15378370228741015, 0.15378370228741015, 0.15378370228741015, 0.15378370228741015, 0.23485349159367602, 0.2562038090112829, 0.08540126967042765, 0.06405095225282073, 0.06405095225282073, 0.08540126967042765, 0.08540126967042765, 0.04270063483521382, 0.04270063483521382, 0.06405095225282073, 0.20465040988718455, 0.35813821730257295, 0.02558130123589807, 0.12790650617949034, 0.05116260247179614, 0.07674390370769421, 0.05116260247179614, 0.05116260247179614, 0.02558130123589807, 0.02558130123589807, 0.35866824325854046, 0.17933412162927023, 0.07970405405745344, 0.07970405405745344, 0.11955608108618015, 0.049815033785908396, 0.02988902027154504, 0.02988902027154504, 0.03985202702872672, 0.03985202702872672, 0.5968119790754739, 0.07460149738443424, 0.07460149738443424, 0.14920299476886847, 0.1132858601071696, 0.1132858601071696, 0.1132858601071696, 0.1132858601071696, 0.1132858601071696, 0.1132858601071696, 0.2265717202143392, 0.666329917800866, 0.024678885844476518, 0.024678885844476518, 0.07403665753342956, 0.024678885844476518, 0.049357771688953035, 0.07403665753342956, 0.049357771688953035, 0.024678885844476518, 0.024678885844476518, 0.7286412164928757, 0.04286124802899269, 0.08572249605798538, 0.04286124802899269, 0.04286124802899269, 0.04286124802899269, 0.43273109105923263, 0.07867838019258776, 0.09834797524073469, 0.07867838019258776, 0.09834797524073469, 0.06884358266851429, 0.04917398762036734, 0.029504392572220407, 0.029504392572220407, 0.03933919009629388, 0.6733438559940504, 0.015659159441722102, 0.09395495665033263, 0.046977478325166314, 0.015659159441722102, 0.046977478325166314, 0.015659159441722102, 0.015659159441722102, 0.031318318883444204, 0.015659159441722102, 0.6769985354822596, 0.09671407649746566, 0.09671407649746566, 0.22884302157867392, 0.17163226618400546, 0.11442151078933696, 0.02860537769733424, 0.05721075539466848, 0.08581613309200273, 0.11442151078933696, 0.08581613309200273, 0.05721075539466848, 0.02860537769733424, 0.16234670649868382, 0.16234670649868382, 0.24352005974802574, 0.08117335324934191, 0.08117335324934191, 0.08117335324934191, 0.08117335324934191, 0.7031626207639639, 0.0351581310381982, 0.10547439311459458, 0.0351581310381982, 0.0351581310381982, 0.0351581310381982, 0.2585677243342467, 0.31602721863074595, 0.05745949429649926, 0.05745949429649926, 0.05745949429649926, 0.08618924144474889, 0.05745949429649926, 0.02872974714824963, 0.02872974714824963, 0.02872974714824963, 0.12261182189869169, 0.12261182189869169, 0.24522364379738337, 0.12261182189869169, 0.12261182189869169, 0.12261182189869169, 0.18581481928939547, 0.18581481928939547, 0.18581481928939547, 0.18581481928939547, 0.42588002479265774, 0.10140000590301375, 0.108160006296548, 0.0405600023612055, 0.10140000590301375, 0.081120004722411, 0.04732000275473975, 0.0405600023612055, 0.03380000196767125, 0.02028000118060275, 0.6726013306005328, 0.13452026612010656, 0.2451444232804442, 0.2451444232804442, 0.1225722116402221, 0.1225722116402221, 0.33876111211412535, 0.09410030892059038, 0.0752802471364723, 0.09410030892059038, 0.09410030892059038, 0.03764012356823615, 0.056460185352354225, 0.0752802471364723, 0.03764012356823615, 0.0752802471364723, 0.6282851413026828, 0.08567524654127492, 0.02855841551375831, 0.02855841551375831, 0.05711683102751662, 0.11423366205503324, 0.02855841551375831, 0.05711683102751662, 0.28182620158849464, 0.28182620158849464, 0.10248225512308895, 0.025620563780772238, 0.1281028189038612, 0.025620563780772238, 0.025620563780772238, 0.051241127561544475, 0.051241127561544475, 0.025620563780772238, 0.590664971118781, 0.1148515221619852, 0.04922208092656508, 0.06562944123542011, 0.016407360308855028, 0.016407360308855028, 0.032814720617710055, 0.032814720617710055, 0.06562944123542011, 0.016407360308855028], \"Term\": [\"able\", \"able\", \"able\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"about\", \"absolutely\", \"absolutely\", \"accident\", \"accident\", \"afraid\", \"afraid\", \"afraid\", \"afternoon\", \"afternoon\", \"afternoon\", \"afternoon\", \"afternoon\", \"afternoon\", \"afternoon\", \"afternoon\", \"afternoon\", \"afternoon\", \"afterward\", \"afterward\", \"afterward\", \"again\", \"again\", \"again\", \"again\", \"again\", \"again\", \"again\", \"again\", \"again\", \"again\", \"against\", \"against\", \"against\", \"against\", \"against\", \"against\", \"against\", \"against\", \"against\", \"against\", \"alive\", \"almost\", \"almost\", \"almost\", \"almost\", \"almost\", \"almost\", \"almost\", \"alone\", \"alone\", \"alone\", \"alone\", \"alone\", \"alone\", \"alone\", \"alone\", \"alone\", \"alone\", \"already\", \"already\", \"already\", \"already\", \"already\", \"already\", \"already\", \"am\", \"am\", \"am\", \"am\", \"am\", \"am\", \"am\", \"am\", \"am\", \"am\", \"american\", \"american\", \"american\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"among\", \"an\", \"an\", \"an\", \"an\", \"an\", \"an\", \"an\", \"an\", \"an\", \"an\", \"answered\", \"answered\", \"answered\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"any\", \"anyhow\", \"anyhow\", \"anyhow\", \"apartment\", \"apartment\", \"apartment\", \"apartment\", \"apartment\", \"appeared\", \"appeared\", \"are\", \"are\", \"are\", \"are\", \"are\", \"are\", \"are\", \"are\", \"are\", \"are\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"ash\", \"ash\", \"ash\", \"ash\", \"ask\", \"ask\", \"ask\", \"attention\", \"attention\", \"automobile\", \"automobile\", \"automobile\", \"automobile\", \"autumn\", \"autumn\", \"autumn\", \"aware\", \"aware\", \"aware\", \"aware\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"backward\", \"backward\", \"backward\", \"baker\", \"baker\", \"baker\", \"baker\", \"baker\", \"baker\", \"baker\", \"baker\", \"baker\", \"baker\", \"bay\", \"bay\", \"bay\", \"bay\", \"bay\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"be\", \"beach\", \"beach\", \"beach\", \"beauty\", \"beauty\", \"bed\", \"bed\", \"bed\", \"bed\", \"bed\", \"bedroom\", \"bedroom\", \"bedroom\", \"bedroom\", \"bedroom\", \"bedroom\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"been\", \"before\", \"before\", \"before\", \"before\", \"before\", \"before\", \"before\", \"before\", \"before\", \"before\", \"began\", \"began\", \"began\", \"began\", \"began\", \"began\", \"began\", \"began\", \"began\", \"began\", \"beginning\", \"beginning\", \"beginning\", \"beginning\", \"beginning\", \"being\", \"being\", \"being\", \"being\", \"being\", \"being\", \"being\", \"believe\", \"believe\", \"believe\", \"believe\", \"believe\", \"believe\", \"believe\", \"believe\", \"bell\", \"bell\", \"bell\", \"beneath\", \"beneath\", \"beneath\", \"best\", \"best\", \"best\", \"best\", \"best\", \"beyond\", \"beyond\", \"blew\", \"blew\", \"blind\", \"blind\", \"blind\", \"bottle\", \"bottle\", \"bottle\", \"bottle\", \"box\", \"box\", \"bridge\", \"bridge\", \"bridge\", \"bridge\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"brown\", \"brown\", \"brown\", \"buchanan\", \"buchanan\", \"buchanan\", \"buchanan\", \"buchanan\", \"buchanan\", \"buchanan\", \"buchanan\", \"buchanan\", \"buchanan\", \"butler\", \"butler\", \"butler\", \"butler\", \"butler\", \"butler\", \"butler\", \"butler\", \"butler\", \"buy\", \"buy\", \"buy\", \"buy\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"by\", \"came\", \"came\", \"came\", \"came\", \"came\", \"came\", \"came\", \"came\", \"came\", \"came\", \"casual\", \"casual\", \"casual\", \"casual\", \"casual\", \"catherine\", \"catherine\", \"catherine\", \"catherine\", \"catherine\", \"catherine\", \"catherine\", \"caught\", \"caught\", \"caught\", \"certain\", \"certain\", \"certain\", \"certain\", \"certain\", \"certain\", \"champagne\", \"champagne\", \"champagne\", \"changing\", \"changing\", \"chapter\", \"chapter\", \"chapter\", \"chapter\", \"chapter\", \"chapter\", \"chapter\", \"chauffeur\", \"chauffeur\", \"chauffeur\", \"chauffeur\", \"chauffeur\", \"cheerful\", \"cheerful\", \"cheerful\", \"cheerful\", \"cheerful\", \"clothes\", \"clothes\", \"clothes\", \"clothes\", \"clothes\", \"cody\", \"cody\", \"cody\", \"cody\", \"college\", \"college\", \"college\", \"college\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"coming\", \"coming\", \"coming\", \"coming\", \"coming\", \"continually\", \"continually\", \"control\", \"control\", \"copy\", \"copy\", \"copy\", \"copy\", \"corner\", \"corner\", \"corner\", \"corner\", \"corner\", \"corner\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"crazy\", \"crazy\", \"curiosity\", \"curiosity\", \"curiosity\", \"daisy\", \"daisy\", \"daisy\", \"daisy\", \"daisy\", \"daisy\", \"daisy\", \"daisy\", \"daisy\", \"daisy\", \"damp\", \"damp\", \"damp\", \"damp\", \"damp\", \"dan\", \"dan\", \"dan\", \"dan\", \"dance\", \"dance\", \"dance\", \"dance\", \"dance\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"deep\", \"deep\", \"deep\", \"did\", \"did\", \"did\", \"did\", \"did\", \"did\", \"did\", \"did\", \"did\", \"did\", \"did_not\", \"did_not\", \"did_not\", \"did_not\", \"did_not\", \"did_not\", \"did_not\", \"did_not\", \"did_not\", \"did_not\", \"distance\", \"distance\", \"distance\", \"distance\", \"do\", \"do\", \"do\", \"do\", \"do\", \"do\", \"do\", \"do\", \"do\", \"do\", \"do_not\", \"do_not\", \"do_not\", \"do_not\", \"do_not\", \"do_not\", \"do_not\", \"do_not\", \"do_not\", \"do_not\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"door\", \"door\", \"door\", \"door\", \"door\", \"door\", \"door\", \"door\", \"door\", \"door\", \"down\", \"down\", \"down\", \"down\", \"down\", \"down\", \"down\", \"down\", \"down\", \"down\", \"dozen\", \"dozen\", \"dozen\", \"dozen\", \"dozen\", \"dozen\", \"dozen\", \"dozen\", \"drank\", \"drank\", \"drank\", \"drawn\", \"drawn\", \"drawn\", \"drawn\", \"drew\", \"drew\", \"drew\", \"drew\", \"drew\", \"drew\", \"drop\", \"drop\", \"drop\", \"drug\", \"drug\", \"drug\", \"drug\", \"during\", \"during\", \"during\", \"dust\", \"dust\", \"dust\", \"dust\", \"dust\", \"early\", \"early\", \"early\", \"eat\", \"eat\", \"eat\", \"eat\", \"effort\", \"effort\", \"effort\", \"egg\", \"egg\", \"egg\", \"egg\", \"egg\", \"egg\", \"egg\", \"egg\", \"egg\", \"egg\", \"enough\", \"enough\", \"enough\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"far\", \"far\", \"far\", \"far\", \"far\", \"far\", \"far\", \"far\", \"far\", \"far\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"fellow\", \"fellow\", \"filled\", \"filled\", \"filled\", \"filled\", \"fine\", \"fine\", \"fine\", \"fine\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"flower\", \"flower\", \"flower\", \"flower\", \"flower\", \"form\", \"form\", \"form\", \"form\", \"form\", \"forty\", \"forty\", \"forty\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"france\", \"france\", \"fresh\", \"fresh\", \"friend\", \"friend\", \"friend\", \"friend\", \"fromthe\", \"fromthe\", \"fromthe\", \"fromthe\", \"fromthe\", \"funny\", \"funny\", \"future\", \"future\", \"future\", \"future\", \"garage\", \"garage\", \"garage\", \"gate\", \"gate\", \"gate\", \"gathered\", \"gathered\", \"gatz\", \"gatz\", \"gay\", \"gay\", \"gay\", \"gay\", \"gentleman\", \"gentleman\", \"gentleman\", \"george\", \"george\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"ghostly\", \"ghostly\", \"ghostly\", \"ghostly\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"given\", \"given\", \"given\", \"given\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glowing\", \"glowing\", \"glowing\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"god\", \"god\", \"god\", \"god\", \"god\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"goodbye\", \"goodbye\", \"goodbye\", \"gorgeous\", \"gorgeous\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"grass\", \"grass\", \"grass\", \"grass\", \"grew\", \"grew\", \"grew\", \"grew\", \"grotesque\", \"grotesque\", \"grotesque\", \"guessed\", \"guessed\", \"guessed\", \"guessed\", \"hall\", \"hall\", \"hall\", \"hall\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"happened\", \"happened\", \"happened\", \"happened\", \"hardly\", \"hardly\", \"hardly\", \"hardly\", \"hardly\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"have\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"help\", \"help\", \"here\", \"here\", \"here\", \"here\", \"here\", \"here\", \"here\", \"here\", \"here\", \"here\", \"herself\", \"herself\", \"herself\", \"herself\", \"herself\", \"holding\", \"holding\", \"horn\", \"horn\", \"horn\", \"horn\", \"host\", \"host\", \"host\", \"host\", \"hot\", \"hot\", \"hot\", \"hot\", \"hot\", \"hot\", \"hot\", \"hot\", \"hot\", \"hot\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"husky\", \"husky\", \"husky\", \"if\", \"if\", \"if\", \"if\", \"if\", \"if\", \"if\", \"if\", \"if\", \"if\", \"ihad\", \"ihad\", \"ihad\", \"ihad\", \"included\", \"included\", \"information\", \"information\", \"information\", \"innocently\", \"innocently\", \"innocently\", \"interrupted\", \"interrupted\", \"interrupted\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"into\", \"invited\", \"invited\", \"invited\", \"invited\", \"itself\", \"itself\", \"itself\", \"itself\", \"itself\", \"itself\", \"itself\", \"itself\", \"itself\", \"itself\", \"itwas\", \"itwas\", \"itwas\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"jumped\", \"jumped\", \"jumped\", \"jumped\", \"jumped\", \"june\", \"june\", \"june\", \"just\", \"just\", \"just\", \"just\", \"just\", \"just\", \"just\", \"just\", \"just\", \"just\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"killed\", \"killed\", \"killed\", \"kissed\", \"kissed\", \"kissed\", \"kissed\", \"kitchen\", \"kitchen\", \"kitchen\", \"kitchen\", \"kitchen\", \"klipspringer\", \"klipspringer\", \"klipspringer\", \"klipspringer\", \"klipspringer\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"known\", \"known\", \"known\", \"known\", \"known\", \"known\", \"known\", \"lap\", \"lap\", \"lap\", \"laughed\", \"laughed\", \"laughed\", \"lawn\", \"lawn\", \"lawn\", \"lawn\", \"lawn\", \"lawn\", \"lawn\", \"lawn\", \"lawn\", \"letter\", \"letter\", \"letter\", \"letter\", \"letter\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"light\", \"light\", \"light\", \"light\", \"light\", \"light\", \"light\", \"light\", \"light\", \"light\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"line\", \"line\", \"line\", \"line\", \"line\", \"listening\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"live\", \"live\", \"live\", \"living\", \"living\", \"living\", \"living\", \"living\", \"living\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"looked\", \"looked\", \"looked\", \"looked\", \"looked\", \"looked\", \"looked\", \"looked\", \"looked\", \"looked\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"magazine\", \"magazine\", \"magazine\", \"magazine\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"married\", \"married\", \"married\", \"married\", \"married\", \"married\", \"mckee\", \"mckee\", \"mckee\", \"mckee\", \"mckee\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"michaelis\", \"michaelis\", \"michaelis\", \"miss\", \"miss\", \"miss\", \"miss\", \"miss\", \"miss\", \"miss\", \"miss\", \"miss\", \"miss\", \"miss_baker\", \"miss_baker\", \"miss_baker\", \"miss_baker\", \"miss_baker\", \"month\", \"month\", \"month\", \"month\", \"month\", \"moonlight\", \"moonlight\", \"moonlight\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"most\", \"most\", \"most\", \"most\", \"most\", \"most\", \"most\", \"most\", \"most\", \"most\", \"movie\", \"movie\", \"movie\", \"mr\", \"mr\", \"mr\", \"mr\", \"mr\", \"mr\", \"mr\", \"mr\", \"mr\", \"mr\", \"murmur\", \"murmur\", \"murmur\", \"murmur\", \"murmured\", \"murmured\", \"murmured\", \"murmured\", \"muttered\", \"muttered\", \"muttered\", \"myrtle\", \"myrtle\", \"myrtle\", \"myrtle\", \"myrtle\", \"myrtle\", \"newspaper\", \"newspaper\", \"newspaper\", \"nick\", \"nick\", \"nick\", \"nick\", \"night\", \"night\", \"night\", \"night\", \"night\", \"night\", \"night\", \"night\", \"night\", \"night\", \"nine\", \"nine\", \"nine\", \"nine\", \"nineteen\", \"nineteen\", \"nineteen\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"nobody\", \"nobody\", \"nobody\", \"nobody\", \"nobody\", \"noon\", \"noon\", \"noon\", \"noon\", \"noon\", \"noon\", \"now\", \"now\", \"now\", \"now\", \"now\", \"now\", \"now\", \"now\", \"now\", \"now\", \"number\", \"number\", \"object\", \"object\", \"office\", \"office\", \"office\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"only\", \"only\", \"only\", \"only\", \"only\", \"only\", \"only\", \"only\", \"only\", \"only\", \"opened\", \"opened\", \"opened\", \"opened\", \"opened\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"or\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"over\", \"overhead\", \"overhead\", \"overhead\", \"overhead\", \"owl\", \"owl\", \"owl\", \"own\", \"own\", \"own\", \"own\", \"own\", \"own\", \"own\", \"own\", \"own\", \"own\", \"pair\", \"pair\", \"pair\", \"park\", \"park\", \"park\", \"park\", \"park\", \"part\", \"part\", \"part\", \"part\", \"part\", \"perhaps\", \"perhaps\", \"perhaps\", \"perhaps\", \"perhaps\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"photograph\", \"photograph\", \"photograph\", \"photograph\", \"physical\", \"physical\", \"physical\", \"physical\", \"physical\", \"physical\", \"picture\", \"picture\", \"picture\", \"picture\", \"picture\", \"picture\", \"play\", \"play\", \"play\", \"play\", \"pocket\", \"pocket\", \"pocket\", \"pocket\", \"pocket\", \"point\", \"point\", \"point\", \"point\", \"politely\", \"politely\", \"pool\", \"pool\", \"promise\", \"promise\", \"promise\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"question\", \"question\", \"question\", \"question\", \"question\", \"rain\", \"rain\", \"rain\", \"rain\", \"rain\", \"rain\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"read\", \"read\", \"read\", \"read\", \"read\", \"read\", \"read\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"receiver\", \"request\", \"request\", \"request\", \"request\", \"rich\", \"rich\", \"ride\", \"ride\", \"ride\", \"ride\", \"romantic\", \"romantic\", \"romantic\", \"romantic\", \"room\", \"room\", \"room\", \"room\", \"room\", \"room\", \"room\", \"room\", \"room\", \"room\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"sad\", \"sad\", \"sad\", \"sad\", \"scarcely\", \"scarcely\", \"scarcely\", \"scarcely\", \"scene\", \"scene\", \"scene\", \"scene\", \"second\", \"second\", \"second\", \"second\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"servant\", \"servant\", \"servant\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"shadow\", \"shadow\", \"shaking\", \"shaking\", \"sharply\", \"sharply\", \"sharply\", \"shining\", \"shining\", \"shining\", \"shining\", \"shock\", \"shock\", \"shock\", \"shock\", \"shoe\", \"shoe\", \"shoe\", \"shoe\", \"shoe\", \"shoe\", \"should\", \"should\", \"should\", \"should\", \"should\", \"show\", \"show\", \"sick\", \"sick\", \"sick\", \"silence\", \"silence\", \"silence\", \"silent\", \"silent\", \"silent\", \"silver\", \"silver\", \"silver\", \"silver\", \"singing\", \"singing\", \"sister\", \"sister\", \"sister\", \"sister\", \"sister\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"smile\", \"smile\", \"smile\", \"smile\", \"smile\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"so\", \"solemn\", \"solemn\", \"solemn\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"some\", \"somebody\", \"somebody\", \"somebody\", \"somebody\", \"somewhere\", \"somewhere\", \"somewhere\", \"somewhere\", \"somewhere\", \"song\", \"song\", \"soon\", \"soon\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"spot\", \"spot\", \"spot\", \"stable\", \"stable\", \"stable\", \"stand\", \"stand\", \"stand\", \"star\", \"star\", \"star\", \"stared\", \"stared\", \"stared\", \"startled\", \"startled\", \"station\", \"station\", \"station\", \"station\", \"stay\", \"stay\", \"stiff\", \"stiff\", \"store\", \"store\", \"store\", \"store\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"strange\", \"strange\", \"strange\", \"stranger\", \"stranger\", \"stranger\", \"straw\", \"straw\", \"straw\", \"straw\", \"stretched\", \"stretched\", \"subject\", \"subject\", \"suggestion\", \"suggestion\", \"suggestion\", \"suggestion\", \"summer\", \"summer\", \"summer\", \"summer\", \"summer\", \"sunlight\", \"sunlight\", \"sunlight\", \"sunlight\", \"taxi\", \"taxi\", \"taxi\", \"taxi\", \"taxi\", \"taxi\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"ten\", \"ten\", \"ten\", \"ten\", \"terrible\", \"terrible\", \"terrible\", \"terrible\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"their\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"them\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"then\", \"there_wa\", \"there_wa\", \"there_wa\", \"there_wa\", \"there_wa\", \"there_wa\", \"there_wa\", \"there_wa\", \"there_wa\", \"there_wa\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"they\", \"thick\", \"thick\", \"thick\", \"thin\", \"thin\", \"thin\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"this\", \"this\", \"this\", \"this\", \"this\", \"this\", \"this\", \"this\", \"this\", \"this\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"those\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"thrilling\", \"thrilling\", \"thrilling\", \"through\", \"through\", \"through\", \"through\", \"through\", \"through\", \"through\", \"through\", \"through\", \"through\", \"throwing\", \"throwing\", \"throwing\", \"throwing\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tom\", \"tom\", \"tom\", \"tom\", \"tom\", \"tom\", \"tom\", \"tom\", \"tom\", \"tom\", \"too\", \"too\", \"too\", \"too\", \"too\", \"too\", \"too\", \"too\", \"too\", \"too\", \"took\", \"took\", \"took\", \"took\", \"took\", \"took\", \"took\", \"took\", \"took\", \"took\", \"touched\", \"touched\", \"touched\", \"toward\", \"toward\", \"toward\", \"toward\", \"toward\", \"toward\", \"toward\", \"toward\", \"toward\", \"toward\", \"train\", \"train\", \"train\", \"train\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"trust\", \"trust\", \"trust\", \"try\", \"try\", \"try\", \"try\", \"turn\", \"turn\", \"turn\", \"turn\", \"twelve\", \"twelve\", \"twelve\", \"twelve\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"u\", \"understood\", \"understood\", \"understood\", \"understood\", \"until\", \"until\", \"until\", \"until\", \"until\", \"until\", \"until\", \"until\", \"until\", \"until\", \"urged\", \"urged\", \"urged\", \"use\", \"use\", \"use\", \"use\", \"vanished\", \"vanished\", \"vanished\", \"vanished\", \"various\", \"various\", \"various\", \"veranda\", \"veranda\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"very\", \"voice\", \"voice\", \"voice\", \"voice\", \"voice\", \"voice\", \"voice\", \"voice\", \"voice\", \"voice\", \"waiting\", \"waiting\", \"waiting\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want_to\", \"want_to\", \"want_to\", \"want_to\", \"want_to\", \"want_to\", \"want_to\", \"want_to\", \"want_to\", \"want_to\", \"watch\", \"watch\", \"watch\", \"watched\", \"watched\", \"watched\", \"water\", \"water\", \"water\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"we\", \"we\", \"we\", \"we\", \"we\", \"we\", \"we\", \"we\", \"we\", \"we\", \"weather\", \"weather\", \"weather\", \"went\", \"went\", \"went\", \"went\", \"went\", \"went\", \"went\", \"went\", \"went\", \"went\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"were\", \"west\", \"west\", \"west\", \"west\", \"west\", \"west\", \"west\", \"west\", \"west\", \"west\", \"wet\", \"wet\", \"wet\", \"wet\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"what\", \"wheel\", \"wheel\", \"wheel\", \"wheel\", \"wheel\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"when\", \"whenever\", \"whenever\", \"whenever\", \"whenever\", \"whenever\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"while\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"who\", \"who\", \"who\", \"who\", \"who\", \"who\", \"who\", \"who\", \"who\", \"who\", \"whole\", \"whole\", \"whole\", \"whole\", \"whose\", \"whose\", \"whose\", \"whose\", \"whose\", \"whose\", \"whose\", \"why\", \"why\", \"why\", \"why\", \"why\", \"why\", \"why\", \"why\", \"why\", \"why\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"will\", \"wilson\", \"wilson\", \"wilson\", \"wilson\", \"wilson\", \"wilson\", \"wilson\", \"wilson\", \"wilson\", \"wilson\", \"wind\", \"wind\", \"wind\", \"window\", \"window\", \"window\", \"window\", \"window\", \"window\", \"window\", \"window\", \"window\", \"window\", \"wire\", \"wire\", \"wire\", \"wire\", \"wire\", \"wire\", \"wire\", \"wolfshiem\", \"wolfshiem\", \"wolfshiem\", \"wolfshiem\", \"wolfshiem\", \"wolfshiem\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"wonder\", \"wonder\", \"wonder\", \"wonder\", \"wonder\", \"wonder\", \"worried\", \"worried\", \"worried\", \"worried\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"wrong\", \"wrong\", \"yard\", \"yard\", \"yard\", \"yard\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"you_are\", \"you_are\", \"you_are\", \"you_are\", \"you_are\", \"you_are\", \"you_are\", \"you_are\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"your\", \"your\", \"your\", \"your\", \"your\", \"your\", \"your\", \"your\", \"your\", \"your\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 4, 10, 9, 5, 6, 2, 7, 8, 3]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el681396381909907201207654260\", ldavis_el681396381909907201207654260_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el681396381909907201207654260\", ldavis_el681396381909907201207654260_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el681396381909907201207654260\", ldavis_el681396381909907201207654260_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aam57p7mgft-"
      },
      "source": [
        "## Things to experiment with\n",
        "\n",
        "* ``no_above`` and ``no_below`` parameters in ``filter_extremes`` method.\n",
        "* Adding trigrams or even higher order n-grams.\n",
        "* Consider whether using a hold-out set or cross-validation is the way to go for you.\n",
        "* Try other datasets.\n",
        "\n",
        "## Where to go from here\n",
        "\n",
        "* Check out a RaRe blog post on the AKSW topic coherence measure (http://rare-technologies.com/what-is-topic-coherence/).\n",
        "* pyLDAvis (https://pyldavis.readthedocs.io/en/latest/index.html).\n",
        "* Read some more Gensim tutorials (https://github.com/RaRe-Technologies/gensim/blob/develop/tutorials.md#tutorials).\n",
        "* If you haven't already, read [1] and [2] (see references).\n",
        "\n",
        "## References\n",
        "\n",
        "1. \"Latent Dirichlet Allocation\", Blei et al. 2003.\n",
        "2. \"Online Learning for Latent Dirichlet Allocation\", Hoffman et al. 2010.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "0APjvgrZm0hC",
        "JH33oN6vmx4E",
        "Yq08gxuCm45Q",
        "od5nsgvem8E_",
        "Aam57p7mgft-"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}